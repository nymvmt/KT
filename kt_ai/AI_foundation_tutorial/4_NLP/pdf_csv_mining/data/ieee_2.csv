"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Experiences with OpenMP, PGI, HMPP and OpenACC Directives on ISO/TTI Kernels","S. Ghosh; T. Liao; H. Calandra; B. M. Chapman","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; TOTAL E&P R&T USA, LLC, Houston, TX, USA; TOTAL E&P, Pau, France; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","11 Apr 2013","2012","","","691","700","GPUs are slowly becoming ubiquitous devices in High Performance Computing, as their capabilities to enhance the performance per watt of compute intensive algorithms as compared to multicore CPUs have been identified. The primary shortcoming of a GPU is usability, since vendor specific APIs are quite different from existing programming languages, and it requires a substantial knowledge of the device and programming interface to optimize applications. Hence, lately a growing number of higher level programming models are targeting GPUs to alleviate this problem. The ultimate goal for a high-level model is to expose an easy-to-use interface for the user to offload compute intensive portions of code (kernels) to the GPU, and tune the code according to the target accelerator to maximize overall performance with a reduced development effort. In this paper, we share our experiences of three of the notable high-level directive based GPU programming models - PGI, CAPS and OpenACC (from CAPS and PGI) on an Nvidia M2090 GPU. We analyze their performance and programmability against Isotropic (ISO)/Tilted Transversely Isotropic (TTI) finite difference kernels, which are primary components in the Reverse Time Migration (RTM) application used by oil and gas exploration for seismic imaging of the sub-surface. When ported to a single GPU using the mentioned directives, we observe an average 1.5-1.8x improvement in performance for both ISO and TTI kernels, when compared with optimized multi-threaded CPU implementations using OpenMP.","","978-0-7695-4956-9","10.1109/SC.Companion.2012.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495877","GPGPU;OpenMP;OpenACC;PGI;CAPS;HMPP;ISO;TTI;Finite Difference Stencils;RTM","","finite difference methods;graphics processing units;multiprocessing systems;parallel processing;user interfaces","OpenMP directive;PGI directive;HMPP directive;OpenACC directive;GPU;graphics processing unit;high performance computing;compute intensive algorithm;multicore CPU;vendor specific API;application program interface;programming language;programming interface;easy-to-use user interface;Nvidia M2090 GPU;GPU programming model;isotropic kernel;tilted transversely isotropic kernel;ISO-TTI finite difference kernel;reverse time migration;RTM application","","6","","11","","11 Apr 2013","","","IEEE","IEEE Conferences"
"Evaluating Multi-core and Many-Core Architectures through Parallelizing a High-Order WENO Solver","L. Deng; H. Bai; D. Zhao; F. Wang","China Aerodynamics R&D Center, Comput. Aerodynamics Inst., Mianyang, China; China Aerodynamics R&D Center, Comput. Aerodynamics Inst., Mianyang, China; China Aerodynamics R&D Center, Comput. Aerodynamics Inst., Mianyang, China; Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China","2016 IEEE Trustcom/BigDataSE/ISPA","9 Feb 2017","2016","","","2167","2174","This paper studies the implementation and optimization of a high-order weighted essentially non-oscillatory (WENO) solver to the solution of the Euler equations on the multi-core and many-core architectures (Intel Ivy Bridge CPU, Intel Xeon Phi 7110P coprocessor and NVIDIA Kepler K20c GPU). The implementation of up to ninth-order accurate WENO schemes is used in the solver. For the GPU platform, both the OpenACC-based and CUDA-based versions of different WENO schemes are developed. To achieve high performance, various optimizatin techniques are used. For Ivy Bridge CPU and MIC, we focus on three categories of optimization techniques: thread parallelism for multi-/many-core scaling, data parallelism to exploit the SIMD mechanism and improving on-chip data reuse, to maximize the performance. Also, we provide an in-depth analysis on the performance differences between Ivy Bridge and MIC. The numerical experiments show that the OpenACC performance can reach up to 84% in contrast to CUDA performance with careful manual optimizations, and the proposed CUDA-based version can achieve a speedup of 9.0 on a Kepler GPU in comparison with the sequential run. We also notice that the speedups of different WENO schemes roughly reach 15.9 and 192.2 on the two Ivy Bridge CPUs and the MIC, respectively. Besides, we conduct a systematic comparison of the three platforms in three aspects: performance, programmability, and power efficiency. Our insights facilitate the programmers to select the right platform with a suitable programming model according to their target applications.","2324-9013","978-1-5090-3205-1","10.1109/TrustCom.2016.0333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847216","high-order;WENO solver;Euler equations;performance;programmability;optimization techniques;power efficiency;Ivy Bridge;MIC;GPU;CUDA;OpenACC","Graphics processing units;Mathematical model;Optimization;Programming;Bridges;Microwave integrated circuits;Computer architecture","differential equations;graphics processing units;multiprocessing systems;optimisation;parallel architectures","multicore architecture evaluation;many-core architecture evaluation;high-order WENO solver;high-order weighted essentially nonscillatory solver;Euler equations;Intel Ivy Bridge CPU;Intel Xeon Phi 7110P coprocessor;NVIDIA Kepler K20c GPU;OpenACC-based version;CUDA-based version;optimization techniques;thread parallelism;data parallelism;multicore scaling;many-core scaling;SIMD mechanism;on-chip data reuse improvement;programming model","","1","","22","","9 Feb 2017","","","IEEE","IEEE Conferences"
"A customizable MapReduce framework for complex data-intensive workflows on GPUs","Z. Qiao; Shuwen Liang; H. Jiang; Song Fu","Department of Computer Science and Engineering, University of North Texas, United States of America; Department of Computer Science and Engineering, University of North Texas, United States of America; Department of Computer Science, Arkansas State University, United States of America; Department of Computer Science and Engineering, University of North Texas, United States of America","2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC)","18 Feb 2016","2015","","","1","8","The MapReduce programming model has been widely used in big data and cloud applications. Criticism on its inflexibility when being applied to complicated scientific applications recently emerges. Several techniques have been proposed to enhance its flexibility. However, some of them exert special requirements on applications, while others fail to support the increasingly popular coprocessors, such as Graphics Processing Unit (GPU). In this paper, we propose MR-Graph, a customizable and unified framework for GPU-based MapReduce, which aims to improve the flexibility and performance of MapReduce. MR-Graph addresses the limitations and restrictions of the traditional MapReduce execution paradigm. The three execution modes integrated in MR-Graph facilitates users to write their applications in a more flexible fashion by defining a Map and Reduce function call graph. MR-Graph efficiently explores the memory hierarchy in GPUs to reduce the data transfer overhead between execution stages and accommodate big data applications.We have implemented a prototype of MR-Graph and experimental results show the effectiveness of using MR-Graph for flexible and scalable GPU-based MapReduce computing.","2374-9628","978-1-4673-8590-9","10.1109/PCCC.2015.7410298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410298","MapReduce;GPU;Customizable;Data Intensive;Iterative;Recursive","Graphics processing units;Programming;Computational modeling;Data models;Parallel processing;Big data;Computer architecture","Big Data;cloud computing;graph theory;graphics processing units;parallel processing","customizable MapReduce framework;complex data-intensive workflows;MapReduce programming model;big data applications;cloud applications;graphics processing unit;MR-Graph;MapReduce execution paradigm;map and reduce function call graph;memory hierarchy;data transfer overhead;big data applications;GPU-based MapReduce computing","","2","","18","","18 Feb 2016","","","IEEE","IEEE Conferences"
"Human Skin Colour Detection Algorithm Optimization with CUDA","D. Ghorpade; A. D. Thakare","Dept. of Computer Engineering Pimpri Chinchwad College of Engineering, Pune, India; Dept. of Computer Engineering Pimpri Chinchwad College of Engineering, Pune, India","2017 International Conference on Computing, Communication, Control and Automation (ICCUBEA)","13 Sep 2018","2017","","","1","6","This Human Skin Colour detection is analyzed to be a most suitable and strong cue for face detection and face recognition which finds various application in domain like surveillance, face biometrics, gesture recognition, interactive game application and many other. These applications require fast image processing in real time and demands enormously high performance with respect to time and processing speed. Earlier work done on sequential architecture does not provide required capability and can be achieved by parallel programming. As Image processing applications shows high degree of parallelism, they prove excellent source for multi-core platform. We can use GPU multithreaded parallel computing techniques to improve the speed of image processing. The paper proposes a data parallelism programming model for Human Skin Colour Detection algorithm. The objective is to increase the computational speed of the algorithm through data parallelism using CUDA framework. The framework is supported by OpenCV libraries and implemented by GPU (Graphics Processing Unit). The evaluation is done on basis of comparative analysis of serial and parallel programming computing. The pixel based Explicitly Defined Region skin classifier is used and RGB colour space is chosen due to its wider use in storing and processing of digital image data. To gain substantial acceleration in image computations, skin classifier code is off-loaded to GPU having compute capacity of 3.5 and rest of code is executed on CPU. We have tested module on static images, as well as on live camera captured images. SFA standard dataset is utilized to evaluate performance. Speedup of 23.45% was achieved using parallel programming.","","978-1-5386-4008-1","10.1109/ICCUBEA.2017.8464010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8464010","Human skin colour;GPU;Image processing;CUDA;Parallel processing;Colour space;RGB colour space;Skin classifier","Graphics processing units;Image color analysis;Skin;Instruction sets;Parallel processing;Computational modeling;Kernel","face recognition;graphics processing units;image classification;image colour analysis;multiprocessing systems;multi-threading;object detection","image processing applications;graphics processing unit;OpenCV libraries;pixel based explicitly defined region skin classifier;digital image data storage;SFA standard dataset;live camera captured images;static images;skin classifier code;image computations;RGB colour space;CUDA framework;computational speed;data parallelism programming model;GPU multithreaded parallel computing techniques;multicore platform;parallel programming;processing speed;fast image processing;interactive game application;gesture recognition;face biometrics;face recognition;face detection","","","","20","","13 Sep 2018","","","IEEE","IEEE Conferences"
"A way for accelerating DNA sequences reconstruction problem based on CUDA","Y. Zhong; J. Lin; B. Wang; C. Tao; X. Wen; C. Nian","Computer Science and Engineering Department, Sichuan University Jinjiang College, Penshan 620860, China; Computer Science and Engineering Department, Sichuan University Jinjiang College, Penshan 620860, China; Computer Science and Engineering Department, Sichuan University Jinjiang College, Penshan 620860, China; Computer Science and Engineering Department, Sichuan University Jinjiang College, Penshan 620860, China; Computer Science and Engineering Department, Sichuan University Jinjiang College, Penshan 620860, China; Computer Science and Engineering Department, Sichuan University Jinjiang College, Penshan 620860, China","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","1 Dec 2014","2014","","","151","157","Traditionally, shotgun for DNA sequences alignment is one of the main method of bioinformatics. It is used to break a long DNA sequence into small fragments. This paper introduces a new method to improve the efficiency of DNA sequence reconstruction after shotgun method using construction suffix array based on CUDA programming model. The experimental results show the construction of suffix array using GPU is an more efficient approach on Intel(R) Core(TM) i3-3110K quad-core and NVIDIA GeForce 610M GPU. Consequently, The experiment presents the efficiency of GPU performance compared with CPU performance, and study shows the method is more than 20 times speedup than that of CPU serial implementation.","","978-1-4799-3080-7","10.1109/ICACCI.2014.6968196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968196","CUDA;GPU;shotgun method;superstring;DNA sequence reassemble;suffix array;radix sort","DNA;Graphics processing units;Arrays","bioinformatics;DNA;graphics processing units;parallel architectures","DNA sequences reconstruction problem;DNA sequences alignment;bioinformatics;long DNA sequence;DNA sequence reconstruction;shotgun method;construction suffix array;CUDA programming model;Intel Core i3-3110K quadcore;NVIDIA GeForce 610M GPU","","","","13","","1 Dec 2014","","","IEEE","IEEE Conferences"
"Accelerating data clustering on GPU-based clusters under shared memory abstraction","K. I. Karantasis; E. D. Polychronopoulos; G. N. Dimitrakopoulos","High Performance Information Systems Lab, School of Computer Engineering and Informatics, University of Patras, Rio, Greece 26500; High Performance Information Systems Lab, School of Computer Engineering and Informatics, University of Patras, Rio, Greece 26500; High Performance Information Systems Lab, School of Computer Engineering and Informatics, University of Patras, Rio, Greece 26500","2010 IEEE International Conference On Cluster Computing Workshops and Posters (CLUSTER WORKSHOPS)","28 Oct 2010","2010","","","1","5","Many-core graphics processors are playing today an important role in the advancements of modern highly concurrent processors. Their ability to accelerate computation is being explored under several scientific fields. In the current paper we present the acceleration of a widely used data clustering algorithm, K-means, in the context of high performance GPU clusters. As opposed to most related implementation efforts that use MPI to port their target applications on a GPU cluster, our implementation follows the Software Distributed Shared Memory (SDSM) paradigm in order to distribute information and computation across the accelerator cluster. In order to investigate the efficiency of a programming model that offers shared memory abstraction on GPU clusters we present two implementations, one that is based on a SDSM implementation of OpenMP and another that utilizes the Pleiad cluster middleware on top of the Java platform. The first results show that such an implementation is feasible in order to accelerate a broad category of large scale, data intensive applications, among which K-means is a characteristic case.","","978-1-4244-8396-9","10.1109/CLUSTERWKSP.2010.5613079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613079","","Graphics processing unit;Clustering algorithms;Programming;Middleware;Acceleration;Graphics","data handling;distributed shared memory systems;message passing;multiprocessing programs;pattern clustering","data clustering;GPU-based clusters;shared memory abstraction;many-core graphics processors;concurrent processors;K-means algorithm;MPI;software distributed shared memory;OpenMP","","6","","24","","28 Oct 2010","","","IEEE","IEEE Conferences"
"Design and evaluation of a parallel k-nearest neighbor algorithm on CUDA-enabled GPU","S. Liang; Y. Liu; C. Wang; L. Jian","Graduate University of Chinese Academy of Sciences, Beijing, 100190, China; Graduate University of Chinese Academy of Sciences, Beijing, 100190, China; Agilent Technologies, Beijing, 100102, China; Graduate University of Chinese Academy of Sciences, Beijing, 100190, China","2010 IEEE 2nd Symposium on Web Society","21 Oct 2010","2010","","","53","60","Recent developments in Graphics Processing Units (GPUs) have enabled inexpensive high performance computing for general-purpose applications. Due to GPU's tremendous computing capability, it has emerged as the co-processor of CPU to achieve a high overall throughput. CUDA programming model provides the programmers adequate C language like APIs to better exploit the parallel power of the GPU. K-nearest neighbor (KNN) is a widely used classification technique and has significant applications in various domains, especially in text classification. The computational-intensive nature of KNN requires a high performance implementation. In this paper, we propose CUKNN, a CUDA-based parallel implementation of KNN. It launches two CUDA kernels, distance calculation kernel and selecting kernel. In the distance calculation kernel, a great number of concurrent CUDA threads are issued, where each thread performs the calculation between the query object and a reference object; in the selecting kernel, threads in a block find the local-k nearest neighbors of the query object concurrently, and then a thread is invoked to find the global-k nearest neighbors out of the queues of local-k neighbors. Various CUDA optimization techniques are applied to maximize the utilization of GPU. We evaluate our implementation by using synthetic dataseis and a real physical simulation dataset. The experimental results demonstrate that CUKNN outperforms the serial KNN on an HP xw8600 workstation significantly, achieving up to 46.7IX speedup on the synthetic dataseis and 42.49X on the physical simulation dataset including I/O cost. It also shows good scalability when varying the number of dimensions of the reference dataset, the number of objects in the reference dataset, and the number of objects in the query dataset.","2158-6993","978-1-4244-6359-6","10.1109/SWS.2010.5607480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5607480","","Graphics processing unit;Instruction sets;Kernel;Nearest neighbor searches;Computational modeling;Programming;Classification algorithms","C language;computer graphic equipment;coprocessors;optimisation;pattern classification","parallel k-nearest neighbor algorithm;CUDA-enabled GPU;graphics processing units;coprocessor;C language;KNN;classification technique;text classification;parallel implementation;CUDA optimization techniques;query dataset","","12","1","18","","21 Oct 2010","","","IEEE","IEEE Conferences"
"Iterative tensor tracking using GPU for textile fabric defect detection","K. L. Mak; X. W. Tian","Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, China; Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, China","The 2010 International Conference on Green Circuits and Systems","9 Aug 2010","2010","","","375","380","This paper presents an efficient real-time implementation of an unsupervised textile fabric defect detection algorithm called ITT using the concept of iterative tensor tracking on graphics processing unit (GPU). The algorithm adopts a new local image descriptor, Spatial Histograms of Oriented Gradients (S-HOG), which is shift-invariant, light insensitive and space scalable. For a given textile fabric image, ITT iteratively updates and then analyzes S-HOG using tensor operations, in particular tensor decomposition to detect textile defects. To speedup the calculation required, ITT is implemented on the GPU using the Compute Unified Device Architecture (CUDA) programming model. The respective computational efficiencies of implementing ITT on the GPU and on the CPU are compared by using experiments. The results demonstrate that the computation speed of the former is on average thirty times and ten times faster than that of the later for updating the S-HOG and for detecting defects respectively because of its parallel processing nature.","","978-1-4244-6878-2","10.1109/ICGCS.2010.5543036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5543036","","Tensile stress;Textiles;Fabrics;Detection algorithms;Graphics;Iterative algorithms;Histograms;Image analysis;Computer architecture;Computational efficiency","computer graphic equipment;coprocessors;fabrics;iterative methods;parallel architectures;production engineering computing;tensors;textiles;tracking","iterative tensor tracking;GPU;textile fabric defect detection;real-time implementation;graphics processing unit;local image descriptor;spatial histograms of oriented gradients;textile fabric image;tensor operations;tensor decomposition;compute unified device architecture programming;computation speed;parallel processing","","1","","9","","9 Aug 2010","","","IEEE","IEEE Conferences"
"An Evaluation of Unified Memory Technology on NVIDIA GPUs","W. Li; G. Jin; X. Cui; S. See","Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China; Tokyo Inst. of Technol., Tokyo, Japan; Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China; Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","9 Jul 2015","2015","","","1092","1098","Unified Memory is an emerging technology which is supported by CUDA 6.X. Before CUDA 6.X, the existing CUDA programming model relies on programmers to explicitly manage data between CPU and GPU and hence increases programming complexity. CUDA 6.X provides a new technology which is called as Unified Memory to provide a new programming model that defines CPU and GPU memory space as a single coherent memory (imaging as a same common address space). The system manages data access between CPU and GPU without explicit memory copy functions. This paper is to evaluate the Unified Memory technology through different applications on different GPUs to show the users how to use the Unified Memory technology of CUDA 6.X efficiently. The applications include Diffusion3D Benchmark, Parboil Benchmark Suite, and Matrix Multiplication from the CUDA SDK Samples. We changed those applications to corresponding Unified Memory versions and compare those with the original ones. We selected the NVIDIA Keller K40 and the Jetson TK1, which can represent the latest GPUs with Keller architecture and the first mobile platform of NVIDIA series with Keller GPU. This paper shows that Unified Memory versions cause 10% performance loss on average. Furthermore, we used the NVIDIA Visual Profiler to dig the reason of the performance loss by the Unified Memory technology.","","978-1-4799-8006-2","10.1109/CCGrid.2015.105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152596","Unified Memory;Heterogeneous Computing;CUDA programming model","Graphics processing units;Benchmark testing;Kernel;Programming;Computational modeling;Memory management;Random access memory","graphics processing units;parallel architectures;storage management","unified memory technology;NVIDIA GPUs;CUDA 6.X;CUDA programming model;data management;CPU;programming complexity;single coherent memory;Diffusion3D benchmark;parboil benchmark suite;matrix multiplication;CUDA SDK samples;NVIDIA Keller K40;Jetson TK1;Keller architecture;mobile platform;NVIDIA visual profiler","","27","","21","","9 Jul 2015","","","IEEE","IEEE Conferences"
"Improving Application Performance by Efficiently Utilizing Heterogeneous Many-core Platforms","J. Shen; A. L. Varbanescu; H. Sips","Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands; Inf. Inst., Univ. of Amsterdam, Amsterdam, Netherlands; Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","9 Jul 2015","2015","","","709","712","Heterogeneous platforms integrating different types of processing units (such as multi-core CPUs and GPUs) are in high demand in high performance computing. Existing studies have shown that using heterogeneous platforms can improve application performance and hardware utilization. However, systematic methods to design, implement, and map applications to efficiently use heterogeneous computing resources are only very few. The goal of my PhD research is therefore to study such heterogeneous systems and propose systematic methods to allow many (classes of) applications to efficiently use them. After 3.5 years of PhD study, my contributions are (1) a thorough evaluation of a suitable programming model for heterogeneous computing, (2) a workload partitioning framework to accelerate parallel applications on heterogeneous platforms, (3) a modelling-based prediction method to determine the optimal workload partitioning, (4) a systematic approach to decide the best mapping between the application and the platform by choosing the best performing hardware configuration (Only-CPU, Only-GPU, or CPU+GPU with the workload partitioning). In the near future, I plan to apply my approach to large-scale applications and platforms to expand its usability and applicability.","","978-1-4799-8006-2","10.1109/CCGrid.2015.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152538","Heterogeneous platforms;Workload partitioning;Hardware configuration;Multi-core CPUs;GPUs;Accelerators","Hardware;Systematics;Computational modeling;Kernel;Graphics processing units;Programming;Predictive models","graphics processing units;multiprocessing systems;performance evaluation","application performance improvement;heterogeneous many-core platforms;multicore CPU;multicore GPU;high-performance computing;hardware utilization improvement;heterogeneous computing resource usage;programming model;parallel applications;modeling-based prediction method;optimal workload partitioning;only-CPU hardware configuration;only-GPU hardware configuration;CPU-plus-GPU hardware configuration;large-scale applications","","1","3","10","","9 Jul 2015","","","IEEE","IEEE Conferences"
"Singular value decomposition on GPU using CUDA","S. Lahabar; P. J. Narayanan","Center for Visual Information Technology, International Institute of Information Technology, Hyderabad, India; Center for Visual Information Technology, International Institute of Information Technology, Hyderabad, India","2009 IEEE International Symposium on Parallel & Distributed Processing","10 Jul 2009","2009","","","1","10","Linear algebra algorithms are fundamental to many computing applications. Modern GPUs are suited for many general purpose processing tasks and have emerged as inexpensive high performance co-processors due to their tremendous computing power. In this paper, we present the implementation of singular value decomposition (SVD) of a dense matrix on GPU using the CUDA programming model. SVD is implemented using the twin steps of bidiagonalization followed by diagonalization. It has not been implemented on the GPU before. Bidiagonalization is implemented using a series of householder transformations which map well to BLAS operations. Diagonalization is performed by applying the implicitly shifted QR algorithm. Our complete SVD implementation outperforms the Matlab and Intel regMath kernel library (MKL) LAPACK implementation significantly on the CPU. We show a speedup of upto 60 over the MATLAB implementation and upto 8 over the Intel MKL implementation on a Intel Dual Core 2.66 GHz PC on NVIDIA GTX 280 for large matrices. We also give results for very large matrices on NVIDIA Tesla S1070.","1530-2075","978-1-4244-3751-1","10.1109/IPDPS.2009.5161058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5161058","","Singular value decomposition;Linear algebra;Computer applications;Coprocessors;High performance computing;Matrix decomposition;Mathematical model;Kernel;Libraries;MATLAB","computer graphic equipment;linear algebra;matrix algebra;parallel processing;programming languages;singular value decomposition","singular value decomposition;graphics processing unit;high performance coprocessor;CUDA programming model;householder transformation;Intel Math kernel library;Matlab;Intel Dual Core PC;linear algebra algorithm;parallel coprocessor;frequency 2.66 GHz","","61","","25","","10 Jul 2009","","","IEEE","IEEE Conferences"
"CUDA-enabled Hadoop cluster for Sparse Matrix Vector Multiplication","M. Reza; A. Sinha; R. Nag; P. Mohanty","High Performance Computing Lab, School of Computer Science and Engineering, National Institute of Science & Technology, Berhampur-761008, India; High Performance Computing Lab, School of Computer Science and Engineering, National Institute of Science & Technology, Berhampur-761008, India; High Performance Computing Lab, School of Computer Science and Engineering, National Institute of Science & Technology, Berhampur-761008, India; High Performance Computing Lab, School of Computer Science and Engineering, National Institute of Science & Technology, Berhampur-761008, India","2015 IEEE 2nd International Conference on Recent Trends in Information Systems (ReTIS)","3 Sep 2015","2015","","","169","172","Compute Unified Device Architecture (CUDA) is an architecture and programming model that allows leveraging the high compute-intensive processing power of the Graphical Processing Units (GPUs) to perform general, non-graphical tasks in a massively parallel manner. Hadoop is an open-source software framework that has its own file system, the Hadoop Distributed File System (HDFS), and its own programming model, the Map Reduce, in order to accomplish the tasks of storage of very large amount of data and their fast processing in a distributed manner in a cluster of inexpensive hardware. This paper presents a model and implementation of a Hadoop-CUDA Hybrid approach to perform Sparse Matrix Vector Multiplication (SpMV) of very large matrices in a very high performing manner. Hadoop is used for splitting the input matrix into smaller sub-matrices, storing them on individual data nodes and then invoking the required CUDA kernels on the individual GPU-possessing cluster nodes. The original SpMV is done using CUDA. Such an implementation has been seen to improve the performance of the SpMV operation over very large matrices by speedup of around 1.4 in comparison to non-Hadoop, single-GPU CUDA implementation.","","978-1-4799-8349-0","10.1109/ReTIS.2015.7232872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7232872","Hadoop;MapReduce;SpMV;CUDA;SCOO;GPGPU","Graphics processing units;Sparse matrices;Kernel;Instruction sets;Java;Programming;File systems","data handling;graphics processing units;parallel architectures","CUDA-enabled Hadoop cluster;sparse matrix vector multiplication;compute unified device architecture;programming model;high compute-intensive processing power;graphical processing units;massively parallel manner;open-source software framework;Hadoop distributed file system;Map Reduce;distributed manner;Hadoop-CUDA hybrid approach;input matrix;smaller sub-matrices;data nodes;GPU-possessing cluster nodes","","2","","11","","3 Sep 2015","","","IEEE","IEEE Conferences"
"GREEN Cache: Exploiting the Disciplined Memory Model of OpenCL on GPUs","J. Lee; D. H. Woo; H. Kim; M. Azimi","Intel Corporation, Hillsboro, OR; Google, Inc., Mountain View, CA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA; Platform Architecture Research team in the Microprocessor and Programming Research group","IEEE Transactions on Computers","6 Oct 2015","2015","64","11","3167","3180","As various graphics processing unit architectures are deployed across broad computing spectrum from a hand-held or embedded device to a high-performance computing server, OpenCL becomes the de facto standard programming environment for general-purpose computing on graphics processing units. Unlike its CPU counterpart, OpenCL has several distinct features such as its disciplined memory model, which is partially inherited from conventional 3D graphics programming models. On the other hand, due to ever increasing memory bandwidth pressure and low power requirement, the capacity of on-chip caches in GPUs keeps increasing overtime. Given such trends, we believe that we have interesting programming model/architecture co-optimization opportunities, in particular, how to energy-efficiently utilize large on-chip caches for GPUs. In this paper, as a showcase, we study the characteristics of the OpenCL memory model and propose a technique called GPU Region-aware energy-efficient non-inclusive cache hierarchy, or GREEN cache hierarchy. With the GREEN cache, our simulation results show that we can save 56 percent of dynamic energy in the L1 cache, 39 percent of dynamic energy in the L2 cache, and 50 percent of leakage energy in the L2 cache with practically no performance degradation and off-chip access increases.","1557-9956","","10.1109/TC.2015.2395435","US National Science Foundation(grant numbers:1054830); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018047","OpenCL;GPU;Cache;OpenCL;GPU;cache","Graphics processing units;Kernel;Computational modeling;Programming;Memory management;Hardware;Training","cache storage;energy conservation;graphics processing units;low-power electronics;power aware computing","disciplined memory model;graphics processing unit architectures;hand-held device;embedded device;high-performance computing server;de facto standard programming environment;general-purpose computing;3D graphics programming models;memory bandwidth;low power requirement;programming architecture;energy efficiency;large on-chip caches;GPU;OpenCL memory model;region-aware energy-efficient noninclusive cache hierarchy;GREEN cache hierarchy;dynamic energy;L1 cache;L2 cache;leakage energy;off-chip access","","7","","40","IEEE","22 Jan 2015","","","IEEE","IEEE Journals"
"Fast CUDA-based codec for height fields","Đ. M. Đurđević; I. I. Tartalja","School of Electrical Engineering, University of Belgrade, Bul. kralja Aleksandra 73, 11120, Serbia; School of Electrical Engineering, University of Belgrade, Bul. kralja Aleksandra 73, 11120, Serbia","2013 21st Telecommunications Forum Telfor (TELFOR)","20 Jan 2014","2013","","","947","954","Following the advances in remote sensing technology in the last decade, the horizontal and vertical scan resolutions for digital terrains have reached the order of a meter and decimeter, respectively. At these resolutions, descriptions of real terrains require very large storage spaces. Efficient storage, transfer, retrieval, and manipulation of such large amounts of data require an efficient compression method. This paper presents a method for fast lossy and lossless compression of regular height fields, which are a commonly used solution for representing surfaces scanned at regular intervals along two axes. The method is suitable for SIMD parallel implementation and thus inherently suitable for modern GPU architectures, which significantly outperform modern CPUs in computation speed, and are already present in home computers. The method allows independent decompression of individual data points, as well as progressive decompression. Even in the case of lossy decompression, the decompressed surface is inherently seamless. The method's efficiency was confirmed through a CUDA implementation of compression and decompression algorithms, and application in a terrain visualization system.","","978-1-4799-1420-3","10.1109/TELFOR.2013.6716388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716388","height field;lossy and lossless compression;progressive decompression;SIMD parallelism;GPU friendly algorithm;CUDA programming model;terrain visualization","Graphics processing units;Image coding;Approximation methods;Parallel processing;Data visualization;Vegetation;Educational institutions","codecs;data compression;geophysical image processing;graphics processing units;parallel architectures;remote sensing","CUDA based codec;height fields;horizontal scan resolution;vertical scan resolution;digital terrains;regular height field lossy compression;regular height field lossless compression;SIMD parallel implementation;GPU architecture;terrain visualization system","","","","31","","20 Jan 2014","","","IEEE","IEEE Conferences"
"Unified Cross-Platform Profiling of Parallel C++ Applications","V. Kucher; F. Fey; S. Gorlatch","University of Muenster, Einsteinstr. 62, Muenster, Germany; University of Muenster, Einsteinstr. 62, Muenster, Germany; University of Muenster, Einsteinstr. 62, Muenster, Germany","2018 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)","14 Feb 2019","2018","","","57","62","To address the great variety of available parallel hardware architectures (CPUs, GPUs, etc.), high-performance applications increasingly demand cross-platform portability. While unified programming models like OpenCL or SYCL provide the ultimate portability of code, the profiling of applications in the development process is still done by using different platform-specific tools of the corresponding hardware vendors. We design and implement a unified, cross-platform profiling interface by extending the PACXX framework for unified programming in C++. With our profiling interface, a single tool is used to profile parallel C++ applications across different target platforms. We illustrate and evaluate our uniform profiler using an example application of matrix multiplication for CPU and GPU architectures.","","978-1-7281-0182-8","10.1109/PMBS.2018.8641652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641652","C++;parallelism;many-cores;GPU programming;cross-platform;unified programming model;profiling","Kernel;Computer architecture;Hardware;Graphics processing units;Measurement;Tools;C++ languages","C++ language;graphics processing units;matrix multiplication;multiprocessing systems;parallel architectures;parallel programming","unified cross-platform profiling;parallel C++ applications;high-performance applications;parallel hardware architectures;hardware vendors;OpenCL;SYCL;platform-specific tools;PACXX;matrix multiplication;GPU architectures;CPU architectures","","","","16","","14 Feb 2019","","","IEEE","IEEE Conferences"
"Exploiting GPUs to Simulate Complex Systems","F. Messina; G. Pappalardo; C. Santoro","Dipt. di Mat. e Inf., Univ. di Catania, Catania, Italy; Dipt. di Mat. e Inf., Univ. di Catania, Catania, Italy; Dipt. di Mat. e Inf., Univ. di Catania, Catania, Italy","2013 Seventh International Conference on Complex, Intelligent, and Software Intensive Systems","19 Sep 2013","2013","","","535","540","This paper describes GCS (GPU-aware ComplexSim), a simulation framework for complex systems which is capable of executing on GPUs exploiting the CUDA programming model. GCS is based on an architecture similar to a previous work, ComplexSim, which provides simulation functionalities on symmetric multiprocessing (SMP) systems. With the current architecture of GCS, the simulation of the complex system run on GPUs, while tasks related to graph analysis still run on the host, by exploiting the embedded multi-thread engine of ComplexSim. The user code can be provided as a behaviour function, without taking into account issues related to task parallelisation, which are managed by the engine of GCS. In GCS, network data -"" as links and mailboxes -"" are organised as SoA (Structure of Array) to deal with the constraints and optimisation issues related to GPU architecture and the CUDA programming model. Moreover nodes attributes are defined by the user as in the case of ComplexSim, but GCS automatically organises them into SoA. GCS exhibits a significant improvement, in terms of simulation times, if compared to ComplexSim running on a SMP system.","","978-0-7695-4992-7","10.1109/CISIS.2013.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603946","","Graphics processing units;Message systems;Arrays;Instruction sets;Engines;Data models","digital simulation;embedded systems;graphics processing units;large-scale systems;multiprocessing systems;multi-threading;parallel architectures","complex systems;GCS;GPU-aware ComplexSim;simulation framework;CUDA programming model;symmetric multiprocessing;SMP systems;graph analysis;embedded multithread engine;user code;network data;SoA;structure of array;GPU architecture","","12","","29","","19 Sep 2013","","","IEEE","IEEE Conferences"
"Parallelizing the cellular potts model on GPU and multi-core CPU: An OpenCL cross-platform study","C. Yu; B. Yang","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","2014 11th International Joint Conference on Computer Science and Software Engineering (JCSSE)","26 Jun 2014","2014","","","117","122","In this paper, we present the analysis and development of a cross-platform OpenCL parallelization of the Cellular Potts Model (CPM). In general, the evolution of the CPM is time-consuming. Using data-parallel programming model such as CUDA can accelerate the process, but it is highly dependent on the hardware type and manufacturer. Recently, OpenCL has attracted a lot of attention and been widely used by researchers. OpenCL provides a flexible solution, which allows us to come up with an implementation that can execute on both GPUs and multi-core CPUs regardless of the hardware type and manufacturer. Some optimizations are also made for both GPU and multi-core CPU implementations of the CPM, and we also propose a resource management method, MLBBRM. Experimental results show that the developed optimized algorithms for both GPU and multi-core CPU have an average speedup of about 30× and 8× respectively compared with the single threaded CPU implementation.","","978-1-4799-5822-1","10.1109/JCSSE.2014.6841853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841853","Cellular Potts Model;OpenCL;Parallel computing;Cross-platform","Computer science;Software engineering","biology computing;cellular biophysics;graphics processing units;multiprocessing systems;parallel processing;resource allocation","cross-platform OpenCL parallelization;cellular Potts model;CPM;GPU;optimizations;multicore CPU implementations;resource management method;MLBBRM","","3","","17","","26 Jun 2014","","","IEEE","IEEE Conferences"
"Tradeoffs and Considerations in the Design of Accelerators for Database Applications","R. Moussalli","T.J. Watson Res. Center, Accel. Platforms Group, IBM, Yorktown Heights, NY, USA","2017 IEEE 33rd International Conference on Data Engineering (ICDE)","18 May 2017","2017","","","1615","1615","General purpose processors have traditionally been favored over application-specific architectures due to the provided flexibility, standardized and simpler programming model, as well as significant reduction in development time. Fueled by the steady advances in transistor scaling, general purpose CPUs satisfied the performance needs of most applications. While CPUs were becoming ubiquitous, advances in digital storage technologies and sensing devices (cameras, microphones, etc) led to massive and sky-rocketing amounts of data being generated by devices of all scales. Extracting insights out of this Big Data introduces significant opportunities for business intelligence, though the growth of data volumes and complexity of query patterns has been increasing at a startling rate. With Moore's law ending, transistors' shrinking coming to a halt and CPU performance saturating, accelerator technologies are increasingly embraced to augment general purpose CPUs and to address performance concerns. Accelerators diverge from traditional CPU architectures in the way they utilize the available silicon resources. In particular, accelerators maximize the resources available for raw computing (ALUs, Floating Point Units) and push back the burden of correct program semantics and control to higher levels of the stack including the compiler and programming models, while focusing on a selected subset of applications. Accelerators include Application Specific Integrated Circuits (ASICs), Field Programmable Gate Arrays (FPGAs) and General Purpose Graphics Processing Units (GPGPUs), each with their programming model, advantages and challenges. FPGAs enable the deployment of deep custom pipelines, whereas GPGPUs provide hundreds of small processors executing in a massively parallel fashion. Compared to CPUs, accelerators attain higher performance out of the available transistors for a wide range of applications. This talk covers tradeoffs of accelerators (FPGA and GPGPU) specific to a set of database applications, namely XML filtering, spatiotemporal analytics in the context of the Internet of Things, and relational database querying. Tradeoff metrics include programmability, performance, accuracy and energy consumption. While accelerators achieve high speedups for ""hot"" code paths, the attach point of accelerators in a system significantly impacts the end-to-end application performance. As such, system and deployment-level considerations must be made. To this end, I will go over IBM's efforts to facilitate the inclusion and increase the adoption of accelerators. These include (1) the Coherent Accelerator Processor Interface (CAPI), reducing software refactoring from the CPU side as well as CPU-accelerator latency, (2) the ConTutto research platform for acceleration innovation in the memory subsystem, providing very high bandwidth to accelerators, and (3) NVLink<sup>®</sup>-enabled IBM POWER<sup>®</sup> processors.","2375-026X","978-1-5090-6543-1","10.1109/ICDE.2017.238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930135","accelerator;acceleration;hardware;FPGA;GPU;database;ConTutto;CAPI;NVLink","Program processors;Field programmable gate arrays;Integrated circuit modeling;Trademarks;Databases;Programming;Transistors","Internet of Things;query processing;relational databases;software maintenance;XML","database applications;programming models;application specific integrated circuits;ASIC;field programmable gate arrays;FPGA;general purpose graphics processing units;GPGPU;deep custom pipelines;XML filtering;spatiotemporal analytics;Internet of Things;relational database querying;programmability;energy consumption;deployment-level considerations;coherent accelerator processor interface;CAPI;software refactoring;CPU-accelerator latency;ConTutto research platform;acceleration innovation;memory subsystem;NVLink-enabled IBM POWER processors;compiler;program semantics;floating point units;ALU;raw computing;accelerators;silicon resources;CPU architectures;accelerator technologies;CPU performance saturating;Moore law ending;query patterns;Big Data;sky-rocketing amounts;sensing devices;digital storage technologies;general purpose CPU;transistor scaling;development time;simpler programming model;standardized;flexibility;application-specific architectures;general purpose processors","","2","","","","18 May 2017","","","IEEE","IEEE Conferences"
"Achieving Portability and Performance through OpenACC","J. A. Herdman; W. P. Gaudin; O. Perks; D. A. Beckingsale; A. C. Mallinson; S. A. Jarvis","High Performance Comput., AWE plc, Aldermaston, UK; High Performance Comput., AWE plc, Aldermaston, UK; High Performance Comput., AWE plc, Aldermaston, UK; Dept. of Comput. Sci., Univ. of Warwick, Warwick, UK; Dept. of Comput. Sci., Univ. of Warwick, Warwick, UK; Dept. of Comput. Sci., Univ. of Warwick, Warwick, UK","2014 First Workshop on Accelerator Programming using Directives","9 Apr 2015","2014","","","19","26","OpenACC is a directive-based programming model designed to allow easy access to emerging advanced architecture systems for existing production codes based on Fortran, C and C++. It also provides an approach to coding contemporary technologies without the need to learn complex vendor-specific languages, or understand the hardware at the deepest level. Portability and performance are the key features of this programming model, which are essential to productivity in real scientific applications. OpenACC support is provided by a number of vendors and is defined by an open standard. However the standard is relatively new, and the implementations are relatively immature. This paper experimentally evaluates the currently available compilers by assessing two approaches to the OpenACC programming model: the ""parallel"" and ""kernels"" constructs. The implementation of both of these construct is compared, for each vendor, showing performance differences of up to 84%. Additionally, we observe performance differences of up to 13% between the best vendor implementations. OpenACC features which appear to cause performance issues in certain compilers are identified and linked to differing default vector length clauses between vendors. These studies are carried out over a range of hardware including GPU, APU, Xeon and Xeon Phi based architectures. Finally, OpenACC performance, and productivity, are compared against the alternative native programming approaches on each targeted platform, including CUDA, OpenCL, OpenMP 4.0 and Intel Offload, in addition to MPI and OpenMP.","","978-1-4673-6753-0","10.1109/WACCPD.2014.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081674","","Kernel;Graphics processing units;Computer architecture;Programming;Acceleration;Hardware;Microprocessors","application program interfaces;software performance evaluation;software portability;software standards","OpenMP;Intel Offload;MPI;OpenCL;CUDA;native programming approaches;Xeon Phi based architectures;APU;GPU;OpenACC features;kernels constructs;parallel constructs;OpenACC programming model;open standard;real scientific applications;C++ language;C language;Fortran;production codes;advanced architecture systems;directive-based programming model;portability","","8","","20","","9 Apr 2015","","","IEEE","IEEE Conferences"
"Fast parallel interpolation algorithm using CUDA","Y. Zhao; Q. Qiu; J. Fang; L. Li","Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China","2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS","27 Jan 2014","2013","","","3662","3665","Interpolation is one time consuming and serial operation in the fields of spatial information processing. While fast processing speed is often required in some real-time interactive scenarios. With the development of Purpose computing on Graphics Processing Units (GPGPU), it provides an opportunity to accelerate some traditional inefficient algorithms with low-cost and low-power compared to clusters. In this paper, we mapped the Inverse distance weighted (IDW) interpolation method to Compute Unified Device Architecture (CUDA) parallel programming model. Taking the advantage of Graphics Processing Unit (GPU) parallel computing, we build two-level indexes on GPU, then clever blocking schemes are used to assign computing task among different threads. After illustrate the parallel interpolation process, we conduct several experiments, the result shows the correctness and high efficiency of our optimized implementation. With larger influence radius and massive data, the performance can obtain dozens of times speedups over a very similar single-threaded CPU implementation.","2153-7003","978-1-4799-1114-1","10.1109/IGARSS.2013.6723624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6723624","parallel interpolation;GPGPU;CUDA;Inverse distance weighted interpolation","Graphics processing units;Interpolation;Indexes;Tiles;Instruction sets;Binary codes;Real-time systems","graphics processing units;interpolation;mathematics computing;multi-threading;parallel architectures","parallel interpolation algorithm;spatial information processing;processing speed;real-time interactive scenarios;GPGPU;inverse distance weighted interpolation method;IDW interpolation method;compute unified device architecture;CUDA parallel programming model;graphics processing unit parallel computing;GPU parallel computing;two-level indexes;blocking schemes;computing task assignment;multithreading;performance improvement","","","1","9","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Evaluating cache coherent shared virtual memory for heterogeneous multicore chips","B. A. Hechtman; D. J. Sorin","Department of Electrical and Computer Engineering, Duke University, USA; Department of Electrical and Computer Engineering, Duke University, USA","2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","15 Jul 2013","2013","","","118","119","Although current homogeneous chips tightly couple the cores with cache-coherent shared virtual memory (CCSVM), this is not the communication paradigm used by any current heterogeneous chip. In this paper, we present a CCSVM design for a CPU/GPU chip, as well as an extension of the pthreads programming model for programming this HMC. We experimentally compare CCSVM/xthreads to a state-of-the-art CPU/GPU chip from AMD that runs OpenCL software. CCSVM's more efficient communication enables far better performance and far fewer DRAM accesses.","","978-1-4673-5779-1","10.1109/ISPASS.2013.6557152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557152","","Graphics processing units;Multicore processing;Instruction sets;Programming;Random access memory;Couplings","graphics processing units;integrated circuit design;multi-threading;virtual storage","cache coherent shared virtual memory evaluation;CCSVM design;heterogeneous multicore chips;homogeneous chips;GPU chip;CPU chip;pthreads programming model;HMC;AMD;OpenCL software;DRAM accesses;xthreads","","9","","6","","15 Jul 2013","","","IEEE","IEEE Conferences"
"Fine-Granular Parallel EBCOT and Optimization with CUDA for Digital Cinema Image Compression","F. Wei; Q. Cui; Y. Li","Sch. of Inf. & Commun. Eng., Beijing Univ. of Posts & Telecommun., Beijing, China; Sch. of Inf. & Commun. Eng., Beijing Univ. of Posts & Telecommun., Beijing, China; Sch. of Inf. & Commun. Eng., Beijing Univ. of Posts & Telecommun., Beijing, China","2012 IEEE International Conference on Multimedia and Expo","13 Sep 2012","2012","","","1051","1054","JPEG2000 has been accepted by The Society of Motion Picture and Television Engineers (SMPTE) as the image compression standard for the digital distribution of motion pictures. In JPEG2000, the biggest contribution to the coding performance comes from the Embedded Block Coding with Optimized Truncation (EBCOT), which is also the most time-consuming module by occupying almost 37% of the encoding time. There have been many research activities in the optimization of EBCOT on platforms like FPGA and VLSI, but on Graphics Processing Unit (GPU), a currently popular parallel computing platform in post-production of motion pictures, still few works have been done. This paper proposes a fine-granular parallel EBCOT by re-designing the highly serialized bit-plane coding to a parallel structure where the coding of all bits in a bit-plane could be performed in parallel, then the bit coding tasks can be distributed to the stream processors in GPU by taking advantage of the programming and memory model of CUDA. Experimental results show that our algorithms reveal 3 to 4 times computational speed improvement on an ordinary GPU compared to that on CPU.","1945-788X","978-1-4673-1659-0","10.1109/ICME.2012.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298542","EBCOT;GPU;CUDA;JPEG2000;DCI","Graphics processing unit;Encoding;Image coding;Transform coding;Motion pictures;Instruction sets;Strips","block codes;cinematography;graphics processing units;image coding;image motion analysis;parallel architectures","fine-granular parallel EBCOT;digital cinema image compression;JPEG2000;Society of Motion Picture and Television Engineers;image compression standard;digital distribution;coding performance;embedded block coding with optimized truncation;EBCOT optimization;graphics processing unit;GPU;parallel computing platform;motion picture post-production;bit-plane coding;parallel structure;stream processors;CUDA programming model;CUDA memory model;computational speed improvement","","6","","12","","13 Sep 2012","","","IEEE","IEEE Conferences"
"A real time Breast Microwave Radar imaging reconstruction technique using simt based interpolation","D. Flores-Tapia; S. Pistorius","Division of Medical Physics, CancerCare Manitoba, USA; Division of Medical Physics, CancerCare Manitoba, USA","2010 IEEE International Conference on Image Processing","3 Dec 2010","2010","","","1389","1392","Breast Microwave Radar(BMR) is a novel imaging modality that is capable of producing high contrast images and can detect tumors of at least 4mm. To properly visualize the responses from the breast structures, BMR data sets must be reconstructed. In this paper, a real time BMR image formation technique is proposed. This approach is based on the use of a Single Instruction Multiple Thread(SIMT) interpolation method. By using this programming model, the proposed approach can be implemented on General Purpose Graphic Processing Unit (GPGPU) platform to speed up the reconstruction process. The proposed method yielded promising results when applied to simulated data sets obtained using anatomically accurate numeric phantoms. In average, the proposed approach yielded speed increases of one order of magnitude compared to its CPU counterpart, and two orders of magnitude with respect to current BMR reconstruction techniques.","2381-8549","978-1-4244-7994-8","10.1109/ICIP.2010.5652126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652126","Breast microwave radar;GPU;real time image processing","Image reconstruction;Interpolation;Breast;Microwave imaging;Radar imaging;Microwave theory and techniques;Computational modeling","biological organs;computer graphic equipment;coprocessors;image reconstruction;interpolation;medical image processing;microwave imaging;multi-threading;radar imaging;tumours","real time breast microwave radar imaging reconstruction technique;high contrast images;tumors;BMR data sets;BMR image formation technique;single instruction multiple thread interpolation method;programming model;general purpose graphic processing unit platform;simulated data sets;numeric phantoms;CPU;BMR reconstruction techniques;SIMT","","2","","10","","3 Dec 2010","","","IEEE","IEEE Conferences"
"Leveraging Data-Flow Task Parallelism for Locality-Aware Dynamic Scheduling on Heterogeneous Platforms","O. S. Simsek; A. Drebes; A. Pop","Sch. of Comput. Sci., Univ. of Manchester, Manchester, UK; Sch. of Comput. Sci., Univ. of Manchester, Manchester, UK; Sch. of Comput. Sci., Univ. of Manchester, Manchester, UK","2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","6 Aug 2018","2018","","","540","549","Writing programs for heterogeneous platforms is challenging, since programmers must deal with multiple programming models, partition work for CPUs and accelerators with different compute capabilities, and manage memory in multiple distinct address spaces. We show that using a task-parallel data-flow programming model, in which parallelism is specified in a platform-neutral description that abstracts in particular from the heterogeneity of the hardware, efficient execution can be carried out by a run-time system at execution time using an appropriate task scheduling and memory allocation scheme. This is achieved through dynamic scheduling of tasks by reducing the dependence exchanges between devices, interleaved execution of tasks and transfer between host and device memory, and load balancing across CPUs and GPUs. Our results show our technique increases the number of tasks offloaded to the GPU and improves data locality of GPU tasks leading to a significant reduction of GPU idle time and thus to substantial improvements of performance.","","978-1-5386-5555-9","10.1109/IPDPSW.2018.00093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8425460","Task-parallelism;heterogeneous systems;scheduling;memory allocation","Task analysis;Graphics processing units;Dynamic scheduling;Memory management;Processor scheduling;Data transfer","graphics processing units;multiprocessing systems;parallel processing;parallel programming;processor scheduling;resource allocation","locality-aware dynamic scheduling;heterogeneous platforms;multiple programming models;partition work;CPUs;multiple distinct address;task-parallel data-flow programming model;platform-neutral description;run-time system;execution time;appropriate task scheduling;device memory;data locality;GPU tasks;GPU idle time;compute capabilities;data-flow task parallelism","","1","","20","","6 Aug 2018","","","IEEE","IEEE Conferences"
"An Enhanced Profiling Framework for the Analysis and Development of Parallel Primitives for GPUs","N. Bombieri; F. Busato; F. Fummi","Dept. of Comput. Sci., Univ. of Verona, Verona, Italy; Dept. of Comput. Sci., Univ. of Verona, Verona, Italy; Dept. of Comput. Sci., Univ. of Verona, Verona, Italy","2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip","12 Nov 2015","2015","","","1","8","Parallelizing software applications through the use of existing optimized % target-oriented primitives is a common trend that mediates the complexity of manual parallelization and the use of less efficient directive-based programming models. Parallel primitive libraries allow software engineers to map any sequential code to a target many-core architecture by identifying the most computational intensive code sections and mapping them into one ore more existing primitives. On the other hand, the spreading of such a primitive-based programming model and the different GPU architectures have led to a large and increasing number of third-party libraries, which often provide different implementations of the same primitive, each one optimized for a specific architecture. From the developer point of view, this moves the actual problem of parallelizing the software application to selecting, among the several implementations, the most efficient primitives for the target platform. This paper presents a profiling framework for GPU primitives, which allows measuring the implementation quality of a given primitive by considering the target architecture characteristics. The framework collects the information provided by a standard GPU profiler and combines them into optimization criteria. The criteria evaluations are weighed to distinguish the impact of each optimization on the overall quality of the primitive implementation. The paper shows how the tuning of the different weights has been conducted through the analysis of five of the most widespread existing primitive libraries and how the framework has been eventually applied to improve the implementation performance of a standard primitive.","","978-1-4799-8670-5","10.1109/MCSoC.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328180","","Graphics processing units;Kernel;Instruction sets;Optimization;Libraries;Synchronization","graphics processing units;parallel programming","profiling framework;software applications parallelization;parallel primitives;GPU;graphics processing unit;directive-based programming models;many-core architecture;primitive-based programming model;GPU profiler","","1","","18","","12 Nov 2015","","","IEEE","IEEE Conferences"
"ST-Accel: A High-Level Programming Platform for Streaming Applications on FPGA","Z. Ruan; T. He; B. Li; P. Zhou; J. Cong","Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA; Microsoft Res. Univ. of Sci. & Technol. of China, China; Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA","2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)","11 Sep 2018","2018","","","9","16","In recent years we have witnessed the emergence of the FPGA in many high-performance systems. This is due to FPGA's high reconfigurability and improved user-friendly programming environment. OpenCL, supported by major FPGA vendors, is a high-level programming platform that liberates hardware developers from having to deal with the complex and error-prone HDL development. While OpenCL exposes a GPU-like programming model, which is well-suited for compute-intensive tasks, in many state-of-art systems that deploy FPGA, we observe that the workloads are streaming-like, which is communication-intensive. This mismatch leads to low throughput and high end-to-end latency. In this paper, we propose ST-Accel, a new high-level programming platform for streaming applications on FPGA. It has the following advantages: (i) ST-Accel adopts the multiprocessing programming model to capture the inherent pipeline-level parallelism of streaming applications while reducing the end-to-end latency. (ii) A message-passing-based host/FPGA communication model is used to avoid the coherency issue of shared memory, thus enabling host/FPGA communication during kernel execution. (iii) ST-Accel provides a high-level abstraction for I/O devices to support direct I/O device access that eliminates the overhead of host CPU and reduces the I/O latency. (iv) ST-Accel enables the decoupled access/execute architecture to maximize the utilization of I/O devices. (v) The host/FPGA communication interface is redesigned to cater to the demands of both latency-critical and throughput-critical scenarios. The experimental results on the Amazon AWS cloud and local machine show that ST-Accel can achieve 1.6X-166X throughput and 1/3 latency for typical streaming workloads when compared to OpenCL.","2576-2621","978-1-5386-5522-1","10.1109/FCCM.2018.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457626","FPGA;OpenCL;Programming Platform;Streaming","Field programmable gate arrays;Kernel;Programming;Random access memory;Hardware design languages;Acceleration;Throughput","field programmable gate arrays;message passing;multiprocessing programs;parallel programming","high-performance systems;improved user-friendly programming environment;OpenCL;high-level programming platform;GPU-like programming model;ST-Accel;multiprocessing programming model;high-level abstraction;pipeline-level parallelism;message-passing-based host-FPGA communication model;host-FPGA communication interface;error-prone HDL development;streaming applications;end-to-end latency reduction;kernel execution;direct I/O device access;decoupled access-execute architecture;throughput-critical scenarios;Amazon AWS cloud;I/O latency reduction","","11","","28","","11 Sep 2018","","","IEEE","IEEE Conferences"
"Distributed-memory multi-GPU block-sparse tensor contraction for electronic structure","T. Herault; Y. Robert; G. Bosilca; R. J. Harrison; C. A. Lewis; E. F. Valeev; J. J. Dongarra","University of Tennessee,ICL,TN,USA; University of Tennessee,ICL,TN,USA; Stony Brook University,IACS,NY,USA; Stony Brook University,IACS,NY,USA; Sandia Ntl. Lab.,CA,USA; Virignia Tech,Dept. of Chemistry,VA,USA; University of Tennessee,ICL,TN,USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021","2021","","","537","546","Many domains of scientific simulation (chemistry, condensed matter physics, data science) increasingly eschew dense tensors for block-sparse tensors, sometimes with additional structure (recursive hierarchy, rank sparsity, etc.). Distributed-memory parallel computation with block-sparse tensorial data is paramount to minimize the time-to-solution (e.g., to study dynamical problems or for real-time analysis) and to accommodate problems of realistic size that are too large to fit into the host/device memory of a single node equipped with accelerators. Unfortunately, computation with such irregular data structures is a poor match to the dominant imperative, bulk-synchronous parallel programming model. In this paper, we focus on the critical element of block-sparse tensor algebra, namely binary tensor contraction, and report on an efficient and scalable implementation using the task-focused PaRSEC runtime. High performance of the block-sparse tensor contraction on the Summit supercomputer is demonstrated for synthetic data as well as for real data involved in electronic structure simulations of unprecedented size.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460455","electronic structure;tensor contraction;block-sparse matrix multiplication;distributed memory;multi-GPU nodes;PaRSEC","Tensors;Runtime;Computational modeling;Tools;Data models;Supercomputers;Real-time systems","distributed memory systems;graphics processing units;parallel programming;tensors","dense tensors;distributed-memory parallel computation;block-sparse tensorial data;irregular data structures;block-sparse tensor algebra;binary tensor contraction;electronic structure simulations;distributed-memory multiGPU block-sparse tensor contraction","","","","32","","28 Jun 2021","","","IEEE","IEEE Conferences"
"Hybrid CPU/GPU tasks optimized for concurrency in OpenMP","A. E. Eichenberger; G. . -T. Bercea; A. Bataev; L. Grinberg; J. K. O'Brien",NA; NA; NA; NA; NA,"IBM Journal of Research and Development","13 May 2020","2020","64","3/4","13:1","13:14","Sierra and Summit supercomputers exhibit a significant amount of intranode parallelism between the host POWER9 CPUs and their attached GPU devices. In this article, we show that exploiting device-level parallelism is key to achieving high performance by reducing overheads typically associated with CPU and GPU task execution. Moreover, manually exploiting this type of parallelism in large-scale applications is nontrivial and error-prone. We hide the complexity of exploiting this hybrid intranode parallelism using the OpenMP programming model abstraction. The implementation leverages the semantics of OpenMP tasks to express asynchronous task computations and their associated dependences. Launching tasks on the CPU threads requires a careful design of work-stealing algorithms to provide efficient load balancing among CPU threads. We propose a novel algorithm that removes locks from all task queueing operations that are on the critical path. Tasks assigned to GPU devices require additional steps such as copying input data to GPU devices, launching the computation kernels, and copying data back to the host CPU memory. We perform key optimizations to reduce the cost of these additional steps by tightly integrating data transfers and GPU computations into streams of asynchronous GPU operations. We further map high-level dependences between GPU tasks to the same asynchronous GPU streams to further avoid unnecessary synchronization. Results validate our approach.","0018-8646","","10.1147/JRD.2019.2960245","U.S. Department of Energy(grant numbers:B604142); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935508","","Task analysis;Graphics processing units;Instruction sets;Parallel processing;Runtime;Scheduling;Delays","","","","","","14","IBM","17 Dec 2019","","","IBM","IBM Journals"
"GPU Accelerated Lanczos Algorithm with Applications","K. K. Matam; K. Kothapalli","Int. Inst. of Inf. Technol., Hyderabad Gachibowli, Hyderabad, India; Int. Inst. of Inf. Technol., Hyderabad Gachibowli, Hyderabad, India","2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications","5 May 2011","2011","","","71","76","Graphics Processing Units provide a large computational power at a very low price which position them as an ubiquitous accelerator. GPGPU is accelerating general purpose computations using GPU's. GPU's have been used to accelerate many Linear Algebra routines and Numerical Methods. Lanczos is an iterative method well suited for finding the extreme eigenvalues and the corresponding eigenvectors of large sparse symmetric matrices. In this paper, we present an implementation of Lanczos Algorithm on GPU using the CUDA programming model and apply it to two important problems : graph bisection using spectral methods, and image segmentation. Our GPU implementation of spectral bisection performs better when compared to both an Intel Math Kernel Library implementation and a Matlab implementation. Our GPU implementation shows a speedup up to 97.3 times over Matlab Implementation and 2.89 times over the Intel Math Kernel Library implementation on a Intel Core i7 920 Processor, which is a quad-core CPU. Similarly, our image segmentation implementation achieves a speed up of 3.27 compared to a multicore CPU based implementation using Intel Math Kernel Library and OpenMP. Through this work, we therefore wish to establish that the GPU may still be a better platform for also highly irregular and computationally intensive applications.","","978-1-61284-829-7","10.1109/WAINA.2011.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763440","GPGPU;Lanczos;graph partitioning;image segmentation;spectral methods","Graphics processing unit;Eigenvalues and eigenfunctions;Image segmentation;Sparse matrices;Kernel;Instruction sets;Symmetric matrices","coprocessors;eigenvalues and eigenfunctions;graph theory;image segmentation;iterative methods;ubiquitous computing","accelerated Lanczos algorithm;graphics processing units;ubiquitous accelerator;GPGPU;general purpose computations;linear algebra;iterative method;eigenvalues;eigenvectors;CUDA programming;graph bisection;image segmentation;multicore CPU;Intel Math Kernel Library;OpenMP","","3","","18","","5 May 2011","","","IEEE","IEEE Conferences"
"Improved Bilinear Interpolation Method for Image Fast Processing","Y. Sa","Guangdong Univ. of Educ., Guangzhou, China","2014 7th International Conference on Intelligent Computation Technology and Automation","8 Jan 2015","2014","","","308","311","Bilinear interpolation algorithm is broadly applied in digital image processing but its calculation speed is very slow. In order to improve its performance in calculation, this paper proposes a graphic processing unit acceleration-based bilinear interpolation parallel It mainly utilizes Wallis transforming independence among various blocks in bilinear interpolation, which is adaptable to characteristics of GPU parallel processing structure. It maps traditional serial bilinear interpolation algorithm to CUDA parallel programming model and optimize thread allocation, memory usage, hardware resources division, etc, to make full use of huge calculation ability. The experiment results show bilinear interpolation parallel algorithm can greatly improve calculation speed with increasing image resolution.","","978-1-4799-6636-3","10.1109/ICICTA.2014.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003545","bilinear interpolation;image processing;GPU;CUDA","Graphics processing units;Interpolation;Instruction sets;Parallel algorithms;Acceleration;Computers","graphics processing units;image resolution;interpolation;parallel algorithms;parallel architectures;parallel programming","digital image processing;performance improvement;graphic processing unit acceleration;Wallis transforming independence;GPU parallel processing structure;serial bilinear interpolation algorithm;CUDA parallel programming model;thread allocation optimization;memory usage optimization;hardware resource division optimization;calculation ability;bilinear interpolation parallel algorithm;calculation speed improvement;image resolution improvement","","7","","11","","8 Jan 2015","","","IEEE","IEEE Conferences"
"Generalizing the Utility of GPUs in Large-Scale Heterogeneous Computing Systems","S. Xiao; W. Feng","Dept. of Electr. & Comput. Eng., Virginia Tech, Blacksburg, VA, USA; Dept. of Electr. & Comput. Eng., Virginia Tech, Blacksburg, VA, USA","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","20 Aug 2012","2012","","","2554","2557","Graphics Processing Units (GPUs) have been widely used as accelerators in large-scale heterogeneous computing systems. However, current programming models can only support the utilization of local GPUs. When using non-local GPUs, programmers need to explicitly call API functions for data communication across computing nodes. As such, programming GPUs in large-scale computing systems is more challenging than local GPUs since local and remote GPUs have to be dealt with separately. In this work, we propose a virtual OpenCL (VOCL) framework to support the transparent virtualization of GPUs. This framework, based on the OpenCL programming model, exposes physical GPUs as decoupled virtual resources that can be transparently managed independent of the application execution. To reduce the virtualization overhead, we optimize the GPU memory accesses and kernel launches. We also extend the VOCL framework to support live task migration across physical GPUs to achieve load balance and/or quick system maintenance. Our experiment results indicate that VOCL can greatly simplify the task of programming cluster-based GPUs at a reasonable virtualization cost.","","978-1-4673-0974-5","10.1109/IPDPSW.2012.325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270892","graphics processing unit (GPU);virtual OpenCL;task migration","Graphics processing unit;Kernel;Optimization;Libraries;Bandwidth;Programming","application program interfaces;graphics processing units","GPU utility;large scale heterogeneous computing systems;graphics processing units;current programming models;API functions;data communication;virtual OpenCL;VOCL;OpenCL programming model;decoupled virtual resources","","2","","14","","20 Aug 2012","","","IEEE","IEEE Conferences"
"VOCL: An optimized environment for transparent virtualization of graphics processing units","S. Xiao; P. Balaji; Q. Zhu; R. Thakur; S. Coghlan; H. Lin; G. Wen; J. Hong; W. -c. Feng","Dept. of Computer Science, Virginia Tech., USA; Math. and Comp. Sci. Div., Argonne National Lab., USA; Accenture Technology Labs, USA; Math. and Comp. Sci. Div., Argonne National Lab., USA; Leadership Comp. Facility, Argonne National Lab., USA; Dept. of Computer Science, Virginia Tech., USA; Shenzhen Inst. of Adv. Tech., Chinese Academy of Sciences, China; Shenzhen Inst. of Adv. Tech., Chinese Academy of Sciences, China; Dept. of Computer Science, Virginia Tech., USA","2012 Innovative Parallel Computing (InPar)","25 Oct 2012","2012","","","1","12","Graphics processing units (GPUs) have been widely used for general-purpose computation acceleration. However, current programming models such as CUDA and OpenCL can support GPUs only on the local computing node, where the application execution is tightly coupled to the physical GPU hardware. In this work, we propose a virtual OpenCL (VOCL) framework to support the transparent utilization of local or remote GPUs. This framework, based on the OpenCL programming model, exposes physical GPUs as decoupled virtual resources that can be transparently managed independent of the application execution. The proposed framework requires no source code modifications. We also propose various strategies for reducing the overhead caused by data communication and kernel launching and demonstrate about 85% of the data write bandwidth and 90% of the data read bandwidth compared to data write and read, respectively, in a native nonvirtualized environment. We evaluate the performance of VOCL using four real-world applications with various computation and memory access intensities and demonstrate that compute-intensive applications can execute with negligible overhead in the VOCL environment.","","978-1-4673-2633-9","10.1109/InPar.2012.6339609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339609","Graphics Processing Unit (GPU);Transparent Virtualization;OpenCL","Graphics processing unit;Kernel;Libraries;Programming;Computational modeling","graphics processing units;virtualisation","VOCL;transparent virtualization;graphics processing units;general-purpose computation acceleration;CUDA;local computing node;physical GPU hardware;virtual OpenCL framework;VOCL framework;OpenCL programming model;virtual resources;application execution;data communication;kernel launching;data write bandwidth;data read bandwidth;memory access intensity","","37","1","27","","25 Oct 2012","","","IEEE","IEEE Conferences"
"CUDA-Based Parallel Implementation of IBM Word Alignment Algorithm for Statistical Machine Translation","S. Jing; G. Yan; X. Chen; P. Jin; Z. Guo","Sch. of Comput. Sci., Leshan Normal Univ., Leshan, China; Sch. of Foreign Language, Leshan Normal Univ., Leshan, China; Sch. of Comput. Sci., Leshan Normal Univ., Leshan, China; Sch. of Comput. Sci., Leshan Normal Univ., Leshan, China; Sch. of Comput. Sci., Leshan Normal Univ., Leshan, China","2016 17th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)","8 Jun 2017","2016","","","189","194","Word alignment is a basic task in natural language processing and it usually serves as the starting point when building a modern statistical machine translation system. However, the state-of-art parallel algorithm for word alignment is still time-consuming. In this work, we explore a parallel implementation of word alignment algorithm on Graphics Processor Unit (GPU), which has been widely available in the field of high performance computing. We use the Compute Unified Device Architecture (CUDA) programming model to re-implement a state-of-the-art word alignment algorithm, called IBM Expectation-Maximization (EM) algorithm. A Tesla K40M card with 2880 cores is used for experiments and execution times obtained with the proposed algorithm are compared with a sequential algorithm and a multi-threads algorithm on an IBM X3850 server, which has two Intel Xeon E7 CPUs (2.0GHz * 10 cores). The best experimental results show a 16.8-fold speedup compared to the multi-threads algorithm and a 234.7-fold speedup compared to the sequential algorithm.","","978-1-5090-5081-9","10.1109/PDCAT.2016.050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7943355","Word Alignment;GPU;Parallel Computation;Expectation-Maximization Algorithm;CUDA","Graphics processing units;Algorithm design and analysis;Instruction sets;Hidden Markov models;Computational modeling;Kernel;Programming","graphics processing units;language translation;microprocessor chips;multiprocessing systems;natural language processing;optimisation;parallel algorithms;parallel architectures;statistical analysis;word processing","CUDA-based parallel implementation;IBM word alignment algorithm;statistical machine translation;natural language processing;parallel algorithm;graphics processor unit;GPU;high performance computing;compute unified device architecture;CUDA programming model;IBM expectation-maximization;EM algorithm;Tesla K40M card;2880 cores;sequential algorithm;multithreads algorithm;IBM X3850 server;Intel Xeon E7 CPUs","","1","","15","","8 Jun 2017","","","IEEE","IEEE Conferences"
"A new parallel video understanding and retrieval system","K. Liu; T. Zhang; L. Wang","HP Labs China; HP Labs, Palo Alto; HP Labs China","2010 IEEE International Conference on Multimedia and Expo","23 Sep 2010","2010","","","679","684","In this paper, a hybrid parallel computing framework is proposed for video understanding and retrieval. It is a unified computing architecture based on the Map-Reduce programming model, which supports multi-core and GPU architectures. A key task scheduler is designed for the parallelization of computation tasks. The SVM method is used to train models for video understanding purposes. To effectively shorten the training and processing time, the hybrid computing framework is used to train large scale SVM models. The TRECVID database is used as the basic experimental content for video understanding and retrieval. Experiments were conducted on two 8-core servers, each equipped with NVIDIA Quadro FX 4600 graphics card. Results proved that the proposed parallel computing framework works well for the video understanding and retrieval system by speeding up system development and providing better performances.","1945-788X","978-1-4244-7493-6","10.1109/ICME.2010.5583873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583873","Parallel computing;map-reduce;multi-core CPU;general purpose GPU;video understanding and retrieval","Support vector machines;Training;Graphics processing unit;Feature extraction;Classification algorithms;Computational modeling;Computer architecture","computer graphic equipment;coprocessors;parallel processing;support vector machines;video retrieval;video signal processing;visual databases","parallel video understanding;video retrieval system;hybrid parallel computing framework;Map-Reduce programming model;GPU architectures;key task scheduler;SVM method;TRECVID database;NVIDIA Quadro FX 4600 graphics card","","5","1","14","","23 Sep 2010","","","IEEE","IEEE Conferences"
"An Efficient Numerical Solution Technique for VLSI Interconnect Equations on Many-Core Processors","G. Doménech-Asensi; T. J. Kazmierski","Dpto. de Electrónica, Tec. de Computadoras y Proyectos, Universidad Politécnica de Cartagena, Cartagena, Spain; Dpt. of Electronics and Computers Science, University of Southampton, Southampton, UK","2019 IEEE International Symposium on Circuits and Systems (ISCAS)","1 May 2019","2019","","","1","5","This paper presents a technique to accelerate transient simulations of analog circuits using an explicit integration method parallelised on a many-core computer. Usual methods used by SPICE-type simulators are based on Newton-Raphson iterations, which are reliable and numerically stable, but require long CPU processing times. However, although the integration time step in explicit methods is smaller than that used in implicit methods, this technique avoids the calculation of time-consuming computations due to the Jacobian matrix inversion. The proposed method uses an explicit integration scheme based on the fourth order Adams-Bashforth formula. The algorithm has been parallelised on a NVIDIA general purpose GPU using the CUDA programming model. As a case study, the RC ladder model of a VLSI interconnect is simulated on a general purpose graphic processing unit and the achieved performance is then evaluated against that of a multiprocessor CPU. The results show that the proposed technique achieves a speedup of one order of magnitude in comparison with implicit integration techniques executed on a CPU.","2158-1525","978-1-7281-0397-6","10.1109/ISCAS.2019.8702085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8702085","simulation acceleration;state-space technique;GPU;VLSI interconnect","Instruction sets;Mathematical model;Graphics processing units;Computational modeling;Integrated circuit interconnections;Jacobian matrices;Transient analysis","coprocessors;graphics processing units;integration;multiprocessing systems;Newton-Raphson method;parallel architectures;SPICE;VLSI","implicit methods;time-consuming computations;Jacobian matrix inversion;explicit integration scheme;fourth order Adams-Bashforth formula;NVIDIA general purpose GPU;CUDA programming model;RC ladder model;multiprocessor CPU;implicit integration techniques;VLSI interconnect equations;many-core processors;transient simulations;many-core computer;SPICE-type simulators;Newton-Raphson iterations;integration time step;numerical solution technique;integration method;CPU processing times","","2","","17","","1 May 2019","","","IEEE","IEEE Conferences"
"Three-dimensional particle beam simulation using high performance graphics processing hardware","S. J. Cooke","Naval Research Laboratory, Washington DC 20375, USA","2009 IEEE International Conference on Plasma Science - Abstracts","28 Aug 2009","2009","","","1","1","In this paper, a new three- dimensional particle beam simulation using a high performance hardware is introduced. The graphics processing unit (GPU) hardware is highly parallel and has low-cost computational capabilities. The code uses NVIDIA CUDA programming model and a particle trajectory integration steps on the graphics hardware, in a standard 4th-order Runge-Kutta scheme. The model uses mesh less representations for electromagnetic fields, based on either analytic formulas or field expansion techniques, to achieve a high degree of parallelism in the calculations. The author describe potential applications of the code for 3D simulation of vacuum electronic devices, and present details of both the algorithms used and simulation performance results.","0730-9244","978-1-4244-2617-1","10.1109/PLASMA.2009.5227556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5227556","","Particle beams;Graphics;Hardware;Computational modeling;Concurrent computing;Code standards;Electromagnetic modeling;Electromagnetic fields;Electromagnetic analysis;Parallel processing","electromagnetic fields;microprocessor chips;parallel programming;particle beams;Runge-Kutta methods;solid modelling;three-dimensional displays","three-dimensional particle beam simulation;graphics processing unit hardware;GPU hardware;NVIDIA CUDA programming model;Runge-Kutta scheme;electromagnetic fields;vacuum electronic devices","","","","1","","28 Aug 2009","","","IEEE","IEEE Conferences"
"Evaluation of Medical Imaging Applications using SYCL","Z. Jin; H. Finkel","Argonne National Laboratory,Leadership Computing Facility,Lemont,IL,USA; Argonne National Laboratory,Leadership Computing Facility,Lemont,IL,USA","2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","6 Feb 2020","2019","","","2259","2264","As opposed to the Open Computing Language (OpenCL) programming model in which host and device codes are written in different languages, the SYCL programming model can combine host and device codes for an application in a type-safe way to improve development productivity. In this paper, we chose two medical imaging applications (Heart Wall and Particle Filter) in the Rodinia benchmark suite to study the performance and programming productivity of the SYCL programming model. More specifically, we introduced the SYCL programming model, shared our experience of implementing the applications using SYCL, and compared the performance and programming portability of the SYCL implementations with the OpenCL implementations on an Intel® Xeon® CPU and an Iris® Pro integrated GPU. The results are promising. For the Heart Wall application, the SYCL implementation is on average 15% faster than the OpenCL implementation on the GPU. For the Particle Filter application, the SYCL implementation is 3% slower than the OpenCL implementation on the GPU, but it is 75% faster on the CPU. Using lines of code as an indicator of programming productivity, the SYCL host program reduces the lines of code of the OpenCL host program by 52% and 38% for the Heart Wall and Particle Filter applications, respectively.","","978-1-7281-1867-3","10.1109/BIBM47256.2019.8982983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982983","","","biomedical imaging;graphics processing units;multiprocessing systems;parallel architectures;particle filtering (numerical methods);program compilers;software portability","OpenCL implementation;Heart Wall application;SYCL implementation;programming productivity;SYCL host program;OpenCL host program;medical imaging applications;device codes;SYCL programming model;programming portability;open computing language programming model;particle filter application","","3","","19","","6 Feb 2020","","","IEEE","IEEE Conferences"
"Parallel Algorithms for Approximate String Matching with k Mismatches on CUDA","Y. Liu; L. Guo; J. Li; M. Ren; K. Li","Sch. of Comput. Sci. & Technol., Heilongjiang Univ., Harbin, China; Sch. of Comput. Sci. & Technol., Heilongjiang Univ., Harbin, China; Sch. of Comput. Sci. & Technol., Heilongjiang Univ., Harbin, China; Sch. of Comput. Sci. & Technol., Heilongjiang Univ., Harbin, China; Dept. of Comput. Sci., State Univ. of New York, New Paltz, NY, USA","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","20 Aug 2012","2012","","","2414","2422","Approximate string matching using the k-mismatch technique has been widely applied to many fields such as virus detection and computational biology. The traditional parallel algorithms are all based on multiple processors, which have high costs of computing and communication. GPU has high parallel processing capability, low cost of computing, and less time of communication. To the best of our knowledge, there is no any parallel algorithm for approximate string matching with k mismatches on GPU. With a new parallel programming model based on CUDA, we present three parallel algorithms and their implementations on GPU, namely, the thread parallel algorithm, the block-thread parallel algorithm, and the OPT-block-thread parallel algorithm. The OPT-block thread parallel algorithm can take full advantage of the powerful parallel capability of GPU. Furthermore, it balances the load among the threads and optimizes the execution time with the memory model of GPU. Experimental results show that compared with the traditional sequential algorithm on CPU, our best parallel algorithm on GPU in this paper achieves speedup of 40-80.","","978-1-4673-0974-5","10.1109/IPDPSW.2012.298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270613","Approximate string matching;CUDA;GPU;Hamming distance;parallel algorithm","Instruction sets;Graphics processing unit;Parallel algorithms;Hamming distance;Kernel;Complexity theory","graphics processing units;multiprocessing systems;parallel architectures;string matching","approximate string matching;CUDA;k-mismatch technique;multiple processors;GPU;parallel processing;OPT-block-thread parallel algorithm","","13","","12","","20 Aug 2012","","","IEEE","IEEE Conferences"
"ValuePack: Value-based scheduling framework for CPU-GPU clusters","V. T. Ravi; M. Becchi; G. Agrawal; S. Chakradhar","Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Electr. & Comput. Eng., Univ. of Missouri, Columbia, MO, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; NEC Labs. America, Princeton, NJ, USA","SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","25 Feb 2013","2012","","","1","12","Heterogeneous computing nodes are becoming commonplace today, and recent trends strongly indicate that clusters, supercomputers, and cloud environments will increasingly host more heterogeneous resources, with some being massively parallel (e.g., GPU). With such heterogeneous environments becoming common, it is important to revisit scheduling problems for clusters and cloud environments. In this paper, we formulate and address the problem of value-driven scheduling of independent jobs on heterogeneous clusters, which captures both the urgency and relative priority of jobs. Our overall scheduling goal is to maximize the aggregate value or yield of all jobs. Exploiting the portability available from the underlying programming model, we propose four novel scheduling schemes that can automatically and dynamically map jobs onto heterogeneous resources. Additionally, to improve the utilization of massively parallel resources, we also propose heuristics to automatically decide when and which jobs can share a single resource.","2167-4337","978-1-4673-0806-9","10.1109/SC.2012.111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468477","","Graphics processing units;Processor scheduling;Delay;Multicore processing;Aggregates;Supercomputers;Torque","cloud computing;graphics processing units;processor scheduling","value-based scheduling framework;ValuePack;CPU-GPU clusters;heterogeneous computing nodes;supercomputers;cloud environments;value-driven scheduling;heterogeneous clusters;parallel resources","","5","","48","","25 Feb 2013","","","IEEE","IEEE Conferences"
"G-Storm: GPU-enabled high-throughput online data processing in Storm","Z. Chen; J. Xu; J. Tang; K. Kwiat; C. Kamhoua","Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, 13244; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, 13244; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, 13244; US Air Force Research Lab (AFRL), Rome, NY; US Air Force Research Lab (AFRL), Rome, NY","2015 IEEE International Conference on Big Data (Big Data)","28 Dec 2015","2015","","","307","312","The Single Instruction Multiple Data (SIMD) architecture of Graphic Processing Units (GPUs) makes them perfect for parallel processing of big data. In this paper, we present the design, implementation and evaluation of G-Storm, a GPU-enabled parallel system based on Storm, which harnesses the massively parallel computing power of GPUs for high-throughput online stream data processing. G-Storm has the following desirable features: 1) G-Storm is designed to be a general data processing platform as Storm, which can handle various applications and data types. 2) G-Storm exposes GPUs to Storm applications while preserving its easy-to-use programming model. 3) G-Storm achieves high-throughput and low-overhead data processing with GPUs. We implemented G-Storm based on Storm 0.9.2 and tested it using two different applications: continuous query and matrix multiplication. Extensive experimental results show that compared to Storm, G-Storm achieves over 7x improvement on throughput for continuous query, while maintaining reasonable average tuple processing time. It also leads to 2.3x throughput improvement for the matrix multiplication application.","","978-1-4799-9926-2","10.1109/BigData.2015.7363769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363769","","Graphics processing units;Storms;Kernel;Data processing;Programming;Fasteners;Indexes","Big Data;graphics processing units;matrix multiplication;parallel processing","matrix multiplication;continuous query;parallel processing;graphic processing units;SIMD architecture;single instruction multiple data;high-throughput online data processing;GPU;G-Storm","","18","","21","","28 Dec 2015","","","IEEE","IEEE Conferences"
"Iterative Solution on GPU of Linear Systems Arising from the A-V Edge-FEA of Time-Harmonic Electromagnetic Phenomena","A. F. P. Camargos; V. C. Silva; J. Guichon; G. Meunier","Inst. Fed. de Minas Gerais, Formiga, Brazil; Escola Politec., Univ. de Sao Paulo, São Paulo, Brazil; Grenoble Genie Electr. Lab., St. Martin d'Hères, France; Grenoble Genie Electr. Lab., St. Martin d'Hères, France","2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","14 Apr 2014","2014","","","365","371","We present a performance analysis of a parallel implementation to both preconditioned Conjugate Gradient and preconditioned Bi-conjugate Gradient solvers using graphic processing units with CUDA programming model. The solvers were optimized for the solution of sparse systems of equations arising from Finite Element Analysis of electromagnetic phenomena involved in the diffusion of underground currents under time-harmonic current excitation. We used a shifted Incomplete Cholesky factorization as preconditioner. Results show a significant speedup by using the GPU compared to a serial CPU implementation.","2377-5750","978-1-4799-2729-6","10.1109/PDP.2014.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787300","Finite Elements;Graphic Processing Unit;Preconditioner;Incomplete Factorization","Graphics processing units;Linear systems;Mathematical model;Equations;Iterative methods;Finite element analysis;Sparse matrices","computational electromagnetics;conjugate gradient methods;eddy currents;finite element analysis;graphics processing units;iterative methods;linear systems;parallel architectures","iterative solution;GPU;linear systems;A-V edge-FEA;time-harmonic electromagnetic phenomena;preconditioned biconjugate gradient solver;preconditioned conjugate gradient solver;finite element analysis;underground currents;time-harmonic current excitation;shifted incomplete Cholesky factorization;graphic processing unit;sparse systems;eddy currents","","5","","23","","14 Apr 2014","","","IEEE","IEEE Conferences"
"Toward a Portable Programming Environment for Distributed High Performance Accelerators","S. Hirasawa; H. Honda","Japan Sci. & Technol. Agency, Univ. of Electro-Commun., Chofu, Japan; Japan Sci. & Technol. Agency, Univ. of Electro-Commun., Chofu, Japan","2009 Software Technologies for Future Dependable Distributed Systems","13 Jan 2011","2009","","","189","194","Accelerators with little power consumption per computation performance are beginning to widely spread for High Performance Computing use, instead of general-purpose CPUs with much power consumption. They are GPUs, processors of Cell architecture, and FPGA accelerators. While these processors have much higher computation performance than general-purpose CPUs, they need specific programming environment respectively when using them as distributed memory accelerators. We discuss a portable programming environment which can be used in common with distributed memory accelerators in this paper.","","978-0-7695-3572-2","10.1109/STFSSD.2009.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804597","Heterogeneous Programming Model;Distributed Memory;SIMD Accelerator","Probability density function;Data mining;Software","coprocessors;distributed memory systems;field programmable gate arrays;programming environments","portable programming environment;distributed high performance accelerators;power consumption;computation performance;high performance computing;general-purpose CPU;GPU;cell architecture processors;FPGA accelerators;distributed memory accelerators","","1","1","12","","13 Jan 2011","","","IEEE","IEEE Conferences"
"Software technologies coping with memory hierarchy of GPGPU clusters for stencil computations","T. Endo; G. Jin","Global Science Information and Computing Center, Tokyo Institute of Technology/JST-CREST, Japan; Global Science Information and Computing Center, Tokyo Institute of Technology/JST-CREST, Japan","2014 IEEE International Conference on Cluster Computing (CLUSTER)","1 Dec 2014","2014","","","132","139","Stencil computations, which are important kernels for CFD simulations, have been highly successful on GPGPU clusters, due to high memory bandwidth and computation speed of GPU accelerators. However, sizes of the computed domains are limited by small capacity of GPU device memory. In order to support larger domain sizes, we utilize the memory hierarchy of GPGPU clusters; larger host memory is used for maintain large domains. However, it is challenging to achieve all of larger domain sizes, high performance and easiness of program development. Towards this goal, we combine two software technologies. From the aspect of algorithm, we adopt a locality improvement technique called temporal blocking. From the aspect of system software, we developed a MPI/CUDA wrapper library named HHRT, which supports memory swapping and finer grained programming model. With this combination, we demonstrate that our goal is achieved through evaluations on TSUBAME2.5, a petascale GPGPU supercomputer.","2168-9253","978-1-4799-5548-0","10.1109/CLUSTER.2014.6968747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968747","","Graphics processing units;Arrays;Performance evaluation;Libraries;Programming;Bandwidth;Supercomputers","application program interfaces;computational fluid dynamics;graphics processing units;message passing;parallel architectures","software technologies;memory hierarchy;GPGPU clusters;stencil computations;kernels;CFD simulations;memory bandwidth;computation speed;GPU accelerators;GPU device memory;program development;locality improvement technique;temporal blocking;MPI/CUDA wrapper library;HHRT;memory swapping;grained programming model;TSUBAME2.5;petascale GPGPU supercomputer","","12","","19","","1 Dec 2014","","","IEEE","IEEE Conferences"
"An Automatic Host and Device Memory Allocation Method for OpenMPC","H. Uchiyama; T. Tsumura; H. Matsuo","Nagoya Inst. of Technol., Nagoya, Japan; Nagoya Inst. of Technol., Nagoya, Japan; Nagoya Inst. of Technol., Nagoya, Japan","2012 Third International Conference on Networking and Computing","31 Jan 2013","2012","","","208","214","The CUDA programming model provides better abstraction for GPU programming. However, it is still hard to write programs with CUDA because both some specific techniques and knowledge about GPU architecture are required. Hence, many programming frameworks for CUDA have been developed. OpenMPC is one of them based on OpenMP. OpenMPC is an easy-to-write framework for programmers familiar with traditional OpenMP, but still requires programmers to use the special directives for utilizing fast device memories. To solve this problem, this paper proposes a method for allocating appropriate device memories automatically. This paper also proposes a method for automatically allocating page locked memory for the data which are transferred between host and device. The evaluation results with several programs show that proposed methods can reduce 52% execution time in maximum.","","978-1-4673-4624-5","10.1109/ICNC.2012.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424565","GPGPU;CUDA;OpenMPC;memory allocation","Graphics processing units;Kernel;Instruction sets;Programming;Resource management;Data transfer;Arrays","graphics processing units;paged storage;parallel architectures;parallel programming;program compilers;program diagnostics;storage allocation","OpenMPC;CUDA programming model;GPU programming abstraction;GPU architecture;programming frameworks;automatic device memory allocation method;automatic page locked memory allocation method;automatic host allocation method;graphics processing unit","","2","","10","","31 Jan 2013","","","IEEE","IEEE Conferences"
"Record Setting Software Implementation of DES Using CUDA","G. Agosta; A. Barenghi; F. De Santis; G. Pelosi","Dipt. di Elettron. e Inf., Politec. di Milano, Milan, Italy; Dipt. di Elettron. e Inf., Politec. di Milano, Milan, Italy; Politec. di Milano, Milan, Italy; Dipt. di Ing. dell'Inf. e Metodi Matematici, Univ. of Bergamo, Dalmine, Italy","2010 Seventh International Conference on Information Technology: New Generations","1 Jul 2010","2010","","","748","755","The increase in computational power of off-the-shelf hardware offers more and more advantageous tradeoffs among efficiency, cost and availability, thus enhancing the feasibility of of cryptanalytic attacks aiming to lower the security of widely used cryptosystems. In this paper we illustrate an GPU-based software implementation of the most efficent variant of Data Encryption Standard (DES), showing the performance of a software breaker which effectively exploits the multi-core Nvidia GT200 graphic architecture. The key point is to assess how well the structure of a symmetric key cipher can fit the GPU programming model and the single instruction multiple data architectural parallelism. The proposed breaker outperforms the fastest general purpose CPU-based implementations by an order of magnitude, and, due to the vast availability of GPUs on the market, the speedup translates into a sound improvement in the cost efficiency of the attack. As opposed to solutions based either on application specific or reconfigurable hardware, the proposed implementation does not require any specific technical knowledge from the attacker in order to be successfully built, once our implementation is available. This turns out in a better cost-availability tradeoff and minimizes the required setup time for such an attack to be mounted.","","978-1-4244-6271-1","10.1109/ITNG.2010.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501641","Brute-force Attacks;DES;GPGPU","Hardware;Costs;Cryptography;Software performance;Availability;Data security;Software standards;Graphics;Computer architecture;Parallel programming","computer graphic equipment;coprocessors;cryptography;parallel architectures","record setting software implementation;DES;data encryption standard;CUDA;off-the-shelf hardware;cryptanalytic attacks;cryptosystems;GPU-based software implementation;software breaker;multicore Nvidia GT200 graphic architecture;symmetric key cipher;GPU programming model;cost-availability tradeoff;single instruction multiple data architectural parallelism","","16","","14","","1 Jul 2010","","","IEEE","IEEE Conferences"
"Optimal loop unrolling for GPGPU programs","G. S. Murthy; M. Ravishankar; M. M. Baskaran; P. Sadayappan","Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA","2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)","24 May 2010","2010","","","1","11","Graphics Processing Units (GPUs) are massively parallel, many-core processors with tremendous computational power and very high memory bandwidth. With the advent of general purpose programming models such as NVIDIA's CUDA and the new standard OpenCL, general purpose programming using GPUs (GPGPU) has become very popular. However, the GPU architecture and programming model have brought along with it many new challenges and opportunities for compiler optimizations. One such classical optimization is loop unrolling. Current GPU compilers perform limited loop unrolling. In this paper, we attempt to understand the impact of loop unrolling on GPGPU programs. We develop a semi-automatic, compile-time approach for identifying optimal unroll factors for suitable loops in GPGPU programs. In addition, we propose techniques for reducing the number of unroll factors evaluated, based on the characteristics of the program being compiled and the device being compiled to. We use these techniques to evaluate the effect of loop unrolling on a range of GPGPU programs and show that we correctly identify the optimal unroll factors. The optimized versions run up to 70 percent faster than the unoptimized versions.","1530-2075","978-1-4244-6443-2","10.1109/IPDPS.2010.5470423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470423","Compiler optimizations;Loop Unrolling;GPGPU","Central Processing Unit;Program processors;Optimizing compilers;Concurrent computing;Registers;Computer graphics;Linear programming;Computer science;Power engineering and energy;Power engineering computing","computer graphic equipment;coprocessors;parallel programming;program compilers","optimal loop unrolling;GPGPU programs;graphics processing units;massively parallel many-core processors;programming model;NVIDIA CUDA;OpenCL;general purpose programming;GPU architecture;compiler optimization;GPU compilers","","34","","15","","24 May 2010","","","IEEE","IEEE Conferences"
"Code generation for embedded heterogeneous architectures on android","R. Membarth; O. Reiche; F. Hannig; J. Teich","Department of Computer Science, University of Erlangen-Nuremberg, Germany; Department of Computer Science, University of Erlangen-Nuremberg, Germany; Department of Computer Science, University of Erlangen-Nuremberg, Germany; Department of Computer Science, University of Erlangen-Nuremberg, Germany","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","21 Apr 2014","2014","","","1","6","The success of Android is based on its unified Java programming model that allows to write platform-independent programs for a variety of different target platforms. However, this comes at the cost of performance. As a consequence, Google introduced APIs that allow to write native applications and to exploit multiple cores as well as embedded GPUs for compute-intensive parts. This paper proposes code generation techniques in order to target the Renderscript and Filterscript APIs. Renderscript harnesses multi-core CPUs and unified shader GPUs, while the more restricted Filterscript also supports GPUs with earlier shader models. Our techniques focus on image processing applications and allow to target these APIs and OpenCL from a common description. We further supersede memory transfers by sharing the same memory region among different processing elements on HSA platforms. As reference, we use an embedded platform hosting a multi-core ARM CPU and an ARM Mali GPU. We show that our generated source code is faster than native implementations in OpenCV as well as the pre-implemented script intrinsics provided by Google for acceleration on the embedded GPU.","1558-1101","978-3-9815370-2-4","10.7873/DATE.2014.099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800300","","Graphics processing units;Kernel;Androids;Humanoid robots;Computer architecture;DSL;Optimization","Android (operating system);application program interfaces;embedded systems;graphics processing units;Java;multiprocessing systems;source code (software)","code generation;embedded heterogeneous architectures;unified Java programming model;platform independent programs;multiple cores;embedded GPU;API;image processing applications;embedded platform;multicore ARM CPU;ARM Mali GPU;generated source code","","1","","11","","21 Apr 2014","","","IEEE","IEEE Conferences"
"A Portable and Fast Stochastic Volatility Model Calibration Using Multi and Many-Core Processors","M. Dixon; J. Lotze; M. Zubair","Dept. of Analytics, Univ. of San Francisco, San Francisco, CA, USA; Xcelerit, Dublin, Ireland; Dept. of Comput. Sci., Old Dominion Univ., Norfolk, VA, USA","2014 Seventh Workshop on High Performance Computational Finance","26 Jan 2015","2014","","","23","28","Financial markets change precipitously and on-demand pricing and risk models must be constantly recalibrated to reduce risk. However, certain classes of models are computationally intensive to robustly calibrate to intraday pricesstochastic volatility models being an archetypal example due to the non-convexity of the objective function. In order to accelerate this procedure through parallel implementation,nancial application developers are faced with an ever growing plethora of low-level high-performance computing frameworks such as OpenMP, OpenCL, CUDA, or SIMD intrinsics, and forced to make a trade-off between performance versus the portability,exibility and modularity of the code required to facilitate rapid in-house model development and productionization.This paper describes the acceleration of stochastic volatility model calibration on multi-core CPUs and GPUs using the Xcelerit platform. By adopting a simple dataow programming model, the Xcelerit platform enables the application developer to write sequential, high-level C++ code, without concern for low-level high-performance computing frameworks. This platform provides the portability,exibility and modularity required by application developers. Speedups of up to 30x and 293x are respectively achieved on an Intel Xeon CPU and NVIDIA Tesla K40 GPU, compared to a sequential CPU implementation. The Xcelerit platform implementation is further shown to be equivalent in performance to a low-level CUDA version. Overall, we are able to reduce the entire calibration process time of the sequential implementation from 6; 189 seconds to 183:8 and 17:8 seconds on the CPU and GPU respectively without requiring the developer to reimplement in low-level high performance computing frameworks.","","978-1-4799-7027-8","10.1109/WHPCF.2014.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016370","Calibration; Stochastic Volatility; GPGPU; C++","Graphics processing units;Calibration;Computational modeling;Stochastic processes;Mathematical model;Optimization;Data models","C++ language;data flow computing;financial data processing;graphics processing units;multiprocessing systems;parallel architectures;pricing;risk management;stock markets","fast stochastic volatility model calibration;multicore processors;many-core processors;financial markets;on-demand pricing model;risk model;risk reduction;objective function nonconvexity;low-level high-performance computing frameworks;OpenMP intrinsic;OpenCL intrinsic;CUDA intrinsic;SIMD intrinsic;multicore CPU;multicore GPU;Xcelerit platform;dataflow programming model;high-level C++ code;sequential code;NVIDIA Tesla K40 GPU;Intel Xeon CPU","","","","16","","26 Jan 2015","","","IEEE","IEEE Conferences"
"On the characterization of OpenCL dwarfs on fixed and reconfigurable platforms","K. Krommydas; W. -c. Feng; M. Owaida; C. D. Antonopoulos; N. Bellas","Department of Computer Science, Virginia Tech, USA; Department of Computer Science, Virginia Tech, USA; Department of Electrical and Computer Engineering, University of Thessaly, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Greece","2014 IEEE 25th International Conference on Application-Specific Systems, Architectures and Processors","31 Jul 2014","2014","","","153","160","The proliferation of heterogeneous computing platforms presents the parallel computing community with new challenges. One such challenge entails evaluating the efficacy of such parallel architectures and identifying the architectural innovations that ultimately benefit applications. To address this challenge, we need benchmarks that capture the execution patterns (i.e., dwarfs or motifs) of applications, both present and future, in order to guide future hardware design. Furthermore, we desire a common programming model for the benchmarks that facilitates code portability across a wide variety of different processors (e.g., CPU, APU, GPU, FPGA, DSP) and computing environments (e.g., embedded, mobile, desktop, server). As such, we present the latest release of OpenDwarfs, a benchmark suite that currently realizes the Berkeley dwarfs in OpenCL, a vendor-agnostic and open-standard computing language for parallel computing. Using OpenDwarfs, we characterize a diverse set of fixed and reconfigurable parallel platforms: multicore CPUs, discrete and integrated GPUs, Intel Xeon Phi coprocessor, as well as a FPGA. We describe the computation and communication patterns exposed by a representative set of dwarfs, obtain relevant profiling data and execution information, and draw conclusions that highlight the complex interplay between dwarfs' patterns and the underlying hardware architecture of modern parallel platforms.","2160-052X","978-1-4799-3609-0","10.1109/ASAP.2014.6868650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868650","OpenDwarfs;benchmarking;evaluation;dwarfs;performance characterization;CPU;FPGA;GPU;OpenCL","Benchmark testing;Field programmable gate arrays;Graphics processing units;Hardware;Computer architecture;Kernel;High definition video","field programmable gate arrays;graphics processing units;multiprocessing systems;parallel architectures;reconfigurable architectures","OpenCL Dwarfs;fixed platforms;reconfigurable platforms;heterogeneous computing platforms;parallel computing community;parallel architectures;future hardware design;OpenDwarfs;Berkeley dwarfs;vendor-agnostic computing language;open-standard computing language;multicore CPU;integrated GPU;discrete GPU;Intel Xeon Phi coprocessor;FPGA;execution information;profiling data;hardware architecture","","14","","14","","31 Jul 2014","","","IEEE","IEEE Conferences"
"Heterogeneous work-stealing across CPU and DSP cores","V. Kumar; A. Sbîrlea; A. Jayaraj; Z. Budimlić; D. Majeti; V. Sarkar","Rice University, Houston, Texas 77005, United States; Rice University, Houston, Texas 77005, United States; Texas Instruments, Dallas, United States; Rice University, Houston, Texas 77005, United States; Rice University, Houston, Texas 77005, United States; Rice University, Houston, Texas 77005, United States","2015 IEEE High Performance Extreme Computing Conference (HPEC)","12 Nov 2015","2015","","","1","6","Due to the increasing power constraints and higher and higher performance demands, many vendors have shifted their focus from designing high-performance computer nodes using powerful multicore general-purpose CPUs, to nodes containing a smaller number of general-purpose CPUs aided by a larger number of more power-efficient special purpose processing units, such as GPUs, FPGAs or DSPs. While offering a lower power-to-performance ratio, unfortunately, such heterogeneous systems are notoriously hard to program, forcing the users to resort to lower-level direct programming of the special purpose processors and manually managing data transfer and synchronization between the parts of the program running on general-purpose CPUs and on special-purpose processors. In this paper, we present HC-K2H, a programming model and runtime system for the Texas Instruments Keystone II Hawking platform, consisting of 4 ARM CPUs and 8 TI DSP processors. This System-on-a-Chip (SoC) offers high floating-point performance with lower power requirements than other processors with comparable performance. We present the design and implementation of a hybrid programming model and work-stealing runtime that allows tasks to be created and executed on both the ARM and DSP, and enables the seamless execution and synchronization of tasks regardless of whether they are running on the ARM or DSP. The design of our programming model and runtime is based on an extension of the Habanero-C programming system. We evaluate our implementation using task-parallel benchmarks on a Hawking board, and demonstrate excellent scaling compared to sequential implementations on a single ARM processor.","","978-1-4673-9286-0","10.1109/HPEC.2015.7322452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322452","Habanero;Keystone-II;load balancing;scheduling;work-stealing","Digital signal processing;Runtime;Programming;Program processors;Hardware;Benchmark testing;Multicore processing","digital signal processing chips;low-power electronics;multiprocessing systems;parallel programming;synchronisation;system-on-chip","heterogeneous work-stealing;DSP cores;power constraints;high-performance computer nodes;multicore general-purpose CPU;power-efficient special purpose processing units;GPU;FPGAs;power-to-performance ratio;heterogeneous systems;lower-level direct programming;special purpose processors;data transfer;synchronization;HC-K2H;runtime system;Texas Instruments Keystone II Hawking platform;ARM CPU;DSP processors;system-on-a-chip;SoC;floating-point performance;lower power requirements;hybrid programming model;work-stealing runtime;Habanero-C programming system;parallel programming model","","6","","23","","12 Nov 2015","","","IEEE","IEEE Conferences"
"Efficient Mapping of Graphic Software on DSP","M. Mody; A. Jayaraj; H. Hariyani; A. Balagopalakrishnan; J. Jones; E. Narvaez; S. Govindarajan; P. Shankar; H. Garud","Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc; Embedded Processor Business, Texas Instruments Inc","2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)","7 Dec 2021","2021","","","1","4","High Performance GPU is critical to render sophisticated user interfaces and general-purpose compute for image and vision processing in the Automotive and Industrial applications. The paper provides an overview of methods that can be used for offloading of part of GPU software (namely shaders) on DSP keeping traditional GPU software programming model intact to improve overall performance in the system. The methods allow efficient and transparent mapping of GPU shaders to DSP without alterations to existing software model. The proposed solution is prototyped in Jacinoto6 Platform of Texas Instruments. Experiments indicate that multiple shader primitives can be efficiently mapped to DSP achieving performance similar to GPU, with exception of trigonometric primitives. This allows DSP to be used as co-processor for graphics rendering applications. Leveraging compute capabilities of dual core C66x DSP in J acinto6 Platform using proposed solution can improve overall GPU Shader performance by up-to 41 % for different shaders corresponding to different use-cases.","2766-2101","978-1-6654-2849-1","10.1109/CONECCT52877.2021.9622613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622613","GPu;DSP;GLOPS;Shader;LLVM;OpenGL;Vulkan;Compute Primitives;Jacinto","Computational modeling;Conferences;Graphics processing units;User interfaces;Programming;Rendering (computer graphics);Software","","","","","","12","","7 Dec 2021","","","IEEE","IEEE Conferences"
"An investigation of Unified Memory Access performance in CUDA","R. Landaverde; Tiansheng Zhang; A. K. Coskun; M. Herbordt","Electrical and Computer Engineering Department, Boston University, MA, USA; Electrical and Computer Engineering Department, Boston University, MA, USA; Electrical and Computer Engineering Department, Boston University, MA, USA; Electrical and Computer Engineering Department, Boston University, MA, USA","2014 IEEE High Performance Extreme Computing Conference (HPEC)","12 Feb 2015","2014","","","1","6","Managing memory between the CPU and GPU is a major challenge in GPU computing. A programming model, Unified Memory Access (UMA), has been recently introduced by Nvidia to simplify the complexities of memory management while claiming good overall performance. In this paper, we investigate this programming model and evaluate its performance and programming model simplifications based on our experimental results. We find that beyond on-demand data transfers to the CPU, the GPU is also able to request subsets of data it requires on demand. This feature allows UMA to outperform full data transfer methods for certain parallel applications and small data sizes. We also find, however, that for the majority of applications and memory access patterns, the performance overheads associated with UMA are significant, while the simplifications to the programming model restrict flexibility for adding future optimizations.","","978-1-4799-6233-4","10.1109/HPEC.2014.7040988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040988","","Graphics processing units;Benchmark testing;Kernel;Data transfer;Programming;Runtime;Acceleration","electronic data interchange;graphics processing units;parallel architectures;performance evaluation;shared memory systems;storage management","unified memory access performance;CUDA;memory management;CPU;GPU computing;UMA;Nvidia;performance evaluation;programming model simplification;on-demand data transfer;data transfer method;memory access pattern","","46","","10","","12 Feb 2015","","","IEEE","IEEE Conferences"
"Measurement and Analysis of GPU-Accelerated OpenCL Computations on Intel GPUs","A. T. Cherian; K. Zhou; D. Grubisic; X. Meng; J. Mellor-Crummey","Rice University,Dept. of Computer Science; Rice University,Dept. of Computer Science; Rice University,Dept. of Computer Science; Rice University,Dept. of Computer Science; Rice University,Dept. of Computer Science","2021 IEEE/ACM International Workshop on Programming and Performance Visualization Tools (ProTools)","22 Dec 2021","2021","","","26","35","Graphics Processing Units (GPUs) have become a key technology for accelerating node performance in supercomputers, including the US Department of Energy’s forthcoming exascale systems. Since the execution model for GPUs differs from that for conventional processors, applications need to be rewritten to exploit GPU parallelism. Performance tools are needed for such GPU-accelerated systems to help developers assess how well applications offload computation onto GPUs.In this paper, we describe extensions to Rice University’s HPC-Toolkit performance tools that support measurement and analysis of Intel’s DPC++ programming model for GPU-accelerated systems atop an implementation of the industry-standard OpenCL framework for heterogeneous parallelism on Intel GPUs. HPCToolkit supports three techniques for performance analysis of programs atop OpenCL on Intel GPUs. First, HPC-Toolkit supports profiling and tracing of OpenCL kernels. Second, HPCToolkit supports CPU-GPU blame shifting for OpenCL kernel executions—a profiling technique that can identify code that executes on one or more CPUs while GPUs are idle. Third, HPCToolkit supports fine-grained measurement, analysis, and attribution of performance metrics to OpenCL GPU kernels, including instruction counts, execution latency, and SIMD waste. The paper describes these capabilities and then illustrates their application in case studies with two applications that offload computations onto Intel GPUs.","","978-1-6654-1110-3","10.1109/ProTools54808.2021.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9651230","Supercomputers;High performance computing;Performance analysis;Parallel programming","Measurement;Visualization;Parallel programming;Computational modeling;High performance computing;Graphics processing units;Parallel processing","","","","","","25","","22 Dec 2021","","","IEEE","IEEE Conferences"
"X<sup>e</sup><inf>HPC</inf> Ponte Vecchio","D. Blythe","Chief GPU Architect,Intel","2021 IEEE Hot Chips 33 Symposium (HCS)","20 Oct 2021","2021","","","1","34","500X Increase In Compute Performance Scalable Compute & Memory Packaging & Interconnect For Density & Scale Full Software Stack/Programming Model","2573-2048","978-1-6654-1397-8","10.1109/HCS52781.2021.9567038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9567038","","Computational modeling;Packaging;Software","","","","","","0","","20 Oct 2021","","","IEEE","IEEE Conferences"
"A Case Study of k-means Clustering using SYCL","Z. Jin; H. Finkel","Leadership Computing Facility Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility Argonne National Laboratory,Lemont,IL,USA","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","4466","4471","As opposed to the OpenCL programming model in which host and device codes are written in two programming languages, the SYCL programming model combines them for an application in a type-safe way to improve development productivity. As a popular cluster analysis algorithm, k-means has been implemented using programming models such as OpenMP, OpenCL, and CUDA. Developing a SYCL implementation of k-means as a case study allows us to have a better understanding of performance portability and programming productivity of the SYCL programming model. Specifically, we explained the k-means benchmark in Rodinia, described our efforts of porting the OpenCL k-means benchmark, and evaluated the performance of the OpenCL and SYCL implementations on the Intel<sup>®</sup> Haswell, Broadwell, and Skylake processors. We summarized the migration steps from OpenCL to SYCL, compiled the SYCL program using Codeplay and Intel<sup>®</sup> SYCL compilers, analyzed the SYCL and OpenCL programs using an open-source profiling tool which can intercept OpenCL runtime calls, and compared the performance of the implementations on Intel<sup>®</sup> CPUs and integrated GPU. The experimental results show that the SYCL version in which the kernels run on the GPU is 2% and 8% faster than the OpenCL version for the two large datasets. However, the OpenCL version is still much faster than the SYCL version on the CPUs. Compared to the Intel<sup>®</sup> Haswell and Skylake CPUs, running the k-means benchmark on the Intel<sup>®</sup> Broadwell low-power processor with a CPU and an integrated GPU can achieve the lowest energy consumption. In terms of programming productivity, the lines of code of the SYCL program are 51% fewer than those of the OpenCL program.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005555","","Kernel;Graphics processing units;Programming;Benchmark testing;C++ languages;Productivity","application program interfaces;graphics processing units;parallel programming;pattern clustering;program compilers;programming languages","OpenCL programming model;programming languages;SYCL programming model;cluster analysis algorithm;SYCL program;SYCL compilers;OpenCL runtime calls","","3","","14","","24 Feb 2020","","","IEEE","IEEE Conferences"
"A Validation Testsuite for OpenACC 1.0","C. Wang; R. Xu; S. Chandrasekaran; B. Chapman; O. Hernandez","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA","2014 IEEE International Parallel & Distributed Processing Symposium Workshops","4 Dec 2014","2014","","","1407","1416","Directive-based programming models provide high-level of abstraction thus hiding complex low-level details of the underlying hardware from the programmer. One such model is OpenACC that is also a portable programming model allowing programmers to write applications that offload portions of work from a host CPU to an attached accelerator (GPU or a similar device). The model is gaining popularity and being used for accelerating many types of applications, ranging from molecular dynamics codes to particle physics models. It is critical to evaluate the correctness of the OpenACC implementations and determine its conformance to the specification. In this paper, we present a robust and scalable testing infrastructure that serves this purpose. We worked very closely with three main vendors that offer compiler support for OpenACC and assisted them in identifying and resolving compiler bugs helping them improve the quality of their compilers. The testsuite also aims to identify and resolve ambiguities within the OpenACC specification. This testsuite has been integrated into the harness infrastructure of the TITAN machine at Oak Ridge National Lab and is being used for production. The testsuite consists of test cases for all the directives and clauses of OpenACC, both for C and Fortran languages. The testsuite discussed in this paper focuses on the OpenACC 1.0 feature set. The framework of the testsuite is robust enough to create test cases for 2.0 and future releases. This work is in progress.","","978-1-4799-4116-2","10.1109/IPDPSW.2014.158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969543","Validation;OpenACC;Compiler","Arrays;Programming;Computer bugs;Vectors;Performance evaluation;Standards;Graphics processing units","C language;formal specification;FORTRAN;graphics processing units;molecular dynamics method;program compilers;program debugging","validation testsuite;OpenACC 1.0;directive-based programming model;high-level of abstraction;programmer;portable programming model;attached accelerator;GPU;molecular dynamics codes;particle physics model;OpenACC implementation;testing infrastructure;compiler support;compiler bugs;OpenACC specification;harness infrastructure;TITAN machine;Oak Ridge National Lab;C language;Fortran language","","7","1","12","","4 Dec 2014","","","IEEE","IEEE Conferences"
"Accelerating the reconstruction of magnetic resonance imaging by three-dimensional dual-dictionary learning using CUDA","J. Li; J. Sun; Y. Song; Y. Xu; J. Zhao","School of Biomedical Engineering, Shanghai Jiao Tong University, China; School of Biomedical Engineering, Shanghai Jiao Tong University, China; School of Biomedical Engineering, Shanghai Jiao Tong University, China; School of Biomedical Engineering, Shanghai Jiao Tong University, China; School of Biomedical Engineering, Shanghai Jiao Tong University, China","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","6 Nov 2014","2014","","","2412","2415","An effective way to improve the data acquisition speed of magnetic resonance imaging (MRI) is using under-sampled k-space data, and dictionary learning method can be used to maintain the reconstruction quality. Three-dimensional dictionary trains the atoms in dictionary in the form of blocks, which can utilize the spatial correlation among slices. Dual-dictionary learning method includes a low-resolution dictionary and a high-resolution dictionary, for sparse coding and image updating respectively. However, the amount of data is huge for three-dimensional reconstruction, especially when the number of slices is large. Thus, the procedure is time-consuming. In this paper, we first utilize the NVIDIA Corporation's compute unified device architecture (CUDA) programming model to design the parallel algorithms on graphics processing unit (GPU) to accelerate the reconstruction procedure. The main optimizations operate in the dictionary learning algorithm and the image updating part, such as the orthogonal matching pursuit (OMP) algorithm and the k-singular value decomposition (K-SVD) algorithm. Then we develop another version of CUDA code with algorithmic optimization. Experimental results show that more than 324 times of speedup is achieved compared with the CPU-only codes when the number of MRI slices is 24.","1558-4615","978-1-4244-7929-0","10.1109/EMBC.2014.6944108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944108","","Graphics processing units;Dictionaries;Image reconstruction;Matching pursuit algorithms;Acceleration;Magnetic resonance imaging;Instruction sets","biomedical MRI;data acquisition;graphics processing units;image coding;image matching;image reconstruction;iterative methods;learning (artificial intelligence);medical image processing;optimisation;parallel algorithms;singular value decomposition;time-frequency analysis","three-dimensional dual-dictionary learning;data acquisition;magnetic resonance imaging;sampled k-space data;spatial correlation;low-resolution dictionary;high-resolution dictionary;sparse coding;image updating;image reconstruction quality;three-dimensional image reconstruction;Corporation compute unified device architecture programming model;NVIDIA-CUDA programming model;parallel algorithms;graphics processing unit;GPU;orthogonal matching pursuit algorithm;OMP algorithm;k-singular value decomposition;K-SVD algorithm;algorithmic optimization;CPU-only codes;MRI slices","Algorithms;Computer Graphics;Humans;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Models, Theoretical;Signal-To-Noise Ratio","","","12","","6 Nov 2014","","","IEEE","IEEE Conferences"
"Many-Core Accelerated LIBOR Swaption Portfolio Pricing","J. Lotze; P. D. Sutton; H. Lahlou","Xcelerit, Dublin, Ireland; Xcelerit, Dublin, Ireland; Xcelerit, Dublin, Ireland","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","11 Apr 2013","2012","","","1185","1192","This paper describes the acceleration of a MonteCarlo algorithm for pricing a LIBOR swaption portfolio using multi-core CPUs and GPUs. Speedups of up to 305x are achieved on two Nvidia Tesla M2050 GPUs and up to 20.8x on two Intel Xeon E5620 CPUs, compared to a sequential CPU implementation. This performance is achieved by using the Xcelerit platform - writing sequential, high-level C++ code and adopting a simple dataflow programming model. It avoids the complexity involved when using low-level high-performance computing frameworks such as OpenMP, OpenCL, CUDA, or SIMD intrinsics. The paper provides an overview of the Xcelerit platform, details how high performance is achieved through various automatic optimisation and parallelisation techniques, and shows how the tool can be used to implement portable accelerated Monte-Carlo algorithms in finance. It illustrates the implementation of the Monte-Carlo LIBOR swaption portfolio pricer and gives performance results. A comparison of the Xcelerit platform implementation with an equivalent low-level CUDA version shows that the overhead introduced is less than 1.5% in all scenarios.","","978-0-7695-4956-9","10.1109/SC.Companion.2012.143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495925","finance;swaption;GPU;GPGPU;derivatives pricing;HPC;CUDA","","C++ language;financial data processing;graphics processing units;investment;Monte Carlo methods;optimisation;parallel programming;pricing","Monte Carlo algorithm;optimisation technique;parallelisation technique;high performance computing framework;dataflow programming model;C++ code;Xcelerit platform;sequential CPU implementation;Intel Xeon E5620 CPU;Nvidia Tesla M2050 GPU;graphics processing unit;multicore CPU;portfolio pricing;many-core accelerated LIBOR swaption portfolio","","6","","15","","11 Apr 2013","","","IEEE","IEEE Conferences"
"Stream Processing on Multi-cores with GPUs: Parallel Programming Models' Challenges","D. A. Rockenbach; C. M. Stein; D. Griebler; G. Mencagli; M. Torquati; M. Danelutto; L. G. Fernandes","School of Technology, Pontifical Catholic University of Rio Grande do Sul (PUCRS) / Laboratory of Advanced Research on Cloud Computing (LARCC), Três de Maio Faculty (SETREM); Laboratory of Advanced Research on Cloud Computing (LARCC), Três de Maio Faculty (SETREM); School of Technology, Pontifical Catholic University of Rio Grande do Sul (PUCRS) / Laboratory of Advanced Research on Cloud Computing (LARCC), Três de Maio Faculty (SETREM); Computer Science Department, University of Pisa (UNIPI); Computer Science Department, University of Pisa (UNIPI); Computer Science Department, University of Pisa (UNIPI); School of Technology, Pontifical Catholic University of Rio Grande do Sul (PUCRS)","2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","29 Jul 2019","2019","","","834","841","The stream processing paradigm is used in several scientific and enterprise applications in order to continuously compute results out of data items coming from data sources such as sensors. The full exploitation of the potential parallelism offered by current heterogeneous multi-cores equipped with one or more GPUs is still a challenge in the context of stream processing applications. In this work, our main goal is to present the parallel programming challenges that the programmer has to face when exploiting CPUs and GPUs' parallelism at the same time using traditional programming models. We highlight the parallelization methodology in two use-cases (the Mandelbrot Streaming benchmark and the PARSEC's Dedup application) to demonstrate the issues and benefits of using heterogeneous parallel hardware. The experiments conducted demonstrate how a high-level parallel programming model targeting stream processing like the one offered by SPar can be used to reduce the programming effort still offering a good level of performance if compared with state-of-the-art programming models.","","978-1-7281-3510-6","10.1109/IPDPSW.2019.00137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778359","Parallel Programming;GPU;multi core;high performance computing;stream processing;structured parallel programming","Graphics processing units;Parallel processing;Parallel programming;Kernel;Instruction sets;Computational modeling","graphics processing units;multiprocessing systems;parallel programming","programming effort;state-of-the-art programming models;parallel programming models;stream processing paradigm;scientific enterprise applications;data items;data sources;potential parallelism;current heterogeneous multicores;stream processing applications;parallel programming challenges;traditional programming models;parallelization methodology;Mandelbrot Streaming benchmark;heterogeneous parallel hardware;high-level parallel programming model targeting stream processing;CPU;GPU parallelism;PARSEC Dedup application","","1","","24","","29 Jul 2019","","","IEEE","IEEE Conferences"
"A Statistical-Feature ML Approach to IP Traffic Classification Based on CUDA","Z. Chen; R. Chen; Y. Zhang; J. Zhang; J. Xu","Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China","2016 IEEE Trustcom/BigDataSE/ISPA","9 Feb 2017","2016","","","2235","2239","In modern networks, there exist different applications which generate various different types of network traffic. In order to improve the performance of network management, it is important to identify and classify the internet traffic. The machine learning (ML) technique based on per-flow statistics has been widely used in traffic classification. Different from traditional classification methods, it is insensitive to port number and payload on application level. Our approach in this work is also based on a machine learning method kNN. kNN is a special case of a variable-bandwidth, kernel density ""balloon"" estimator with a uniform kernel [1]. Although there is no time taken for the construction of the classification model using kNN, it is computationally intensive since it relies on searching neighbor among large sets of d-dimensional vectors. The kNN algorithm may have quite expensive classification steps. CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU) [2]. This paper puts forward a CUDA-based kNN algorithm to classify internet traffic. The experimental results show that the peek speed of traffic classification based on GPU improves greatly compared with that based on CPU. Our approach presents a significant speed improvement through GPU, meanwhile, the results demonstrate the potential applicability of GPU in the field of traffic classification.","2324-9013","978-1-5090-3205-1","10.1109/TrustCom.2016.0344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847227","Traffic Classification;CUDA;k-Nearest Neighbor;GPU","Graphics processing units;Classification algorithms;Kernel;Training;IP networks;Ports (Computers);Internet","graphics processing units;Internet;learning (artificial intelligence);parallel architectures;pattern classification;telecommunication traffic","statistical-feature ML approach;IP traffic classification;machine learning method;compute unified device architecture;parallel computing platform;NVIDIA;graphics processing unit;CUDA-based kNN algorithm;Internet traffic;GPU","","1","","12","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Image registration techniques using parallel computing in multicore environment and its applications in medical imaging: An overview","S. Saxena; S. Sharma; N. Sharma","School of Biomedical engineering, Indian Institute of Technology (BHU), Varanasi, UP, India; School of Biomedical engineering, Indian Institute of Technology (BHU), Varanasi, UP, India; School of Biomedical engineering, Indian Institute of Technology (BHU), Varanasi, UP, India","2014 International Conference on Computer and Communication Technology (ICCCT)","8 Jan 2015","2014","","","97","104","Image Registration is the key step of Image Processing as it is the process to locate most accurate relative orientation among two or more images, captured at the same or different times by distinguishable or indistinguishable sensors to increase the information content. For speed optimization of Image Registration, There have been developed numerous approaches till now based on CPU platforms, GPU, CUDA Programming Models etc. Purpose of this paper is to provide a comprehensive review of the existing literature available on Image registration methods based on parallel computing in Multi core architecture. Another considerable intention of this paper is to describe the various applications of image registration using parallel computing in Medical imaging as it can be applied for different modalities of medical images.","","978-1-4799-6758-2","10.1109/ICCCT.2014.7001475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001475","Image Registration;GPU;CUDA;Parallel Computing;CPU","Image registration;Graphics processing units;Biomedical imaging;Parallel processing;Computational modeling;Histograms;MATLAB","graphics processing units;image registration;medical image processing;parallel architectures","image registration techniques;parallel computing;multicore environment;medical imaging;image processing;information content;speed optimization;CPU platform;GPU;CUDA programming model;multicore architecture;medical images","","3","","70","","8 Jan 2015","","","IEEE","IEEE Conferences"
"Towards Achieving Performance Portability Using Directives for Accelerators","M. G. Lopez; V. V. Larrea; W. Joubert; O. Hernandez; A. Haidar; S. Tomov; J. Dongarra","Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Nat. Center for Comput. Sci., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Comput. Sci. & Math. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Innovative Comput. Lab., Univ. of Tennessee, Knoxville, TN, USA; Innovative Comput. Lab., Univ. of Tennessee, Knoxville, TN, USA; Innovative Comput. Lab., Univ. of Tennessee, Knoxville, TN, USA","2016 Third Workshop on Accelerator Programming Using Directives (WACCPD)","2 Feb 2017","2016","","","13","24","In this paper we explore the performance portability of directives provided by OpenMP 4 and OpenACC to program various types of node architectures with attached accelerators, both self-hosted multicore and offload multicore/GPU. Our goal is to examine how successful OpenACC and the newer offload features of OpenMP 4.5 are for moving codes between architectures, how much tuning might be required and what lessons we can learn from this experience. To do this, we use examples of algorithms with varying computational intensities for our evaluation, as both compute and data access efficiency are important considerations for overall application performance. We implement these kernels using various methods provided by newer OpenACC and OpenMP implementations, and we evaluate their performance on various platforms including both X86_64 with attached NVIDIA GPUs, self-hosted Intel Xeon Phi KNL, as well as an X86_64 host system with Intel Xeon Phi coprocessors. In this paper, we explain what factors affected the performance portability such as how to pick the right programming model, its programming style, its availability on different platforms, and how well compilers can optimize and target to multiple platforms.","","978-1-5090-6152-5","10.1109/WACCPD.2016.006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836577","","Programming;Computer architecture;Computational modeling;Kernel;Writing;Government;Graphics processing units","graphics processing units;multiprocessing systems;parallel architectures;performance evaluation;program compilers","performance portability;OpenMP 4;OpenACC;node architectures;offload multicore-GPU;self-hosted multicore;computational intensities;data access efficiency;X86_64;NVIDIA GPU;KNL;self-hosted Intel Xeon Phi KNL;Intel Xeon Phi coprocessors;programming model;compilers","","16","","29","","2 Feb 2017","","","IEEE","IEEE Conferences"
"Map-reduce as a Programming Model for Custom Computing Machines","J. H. C. Yeung; C. C. Tsang; K. H. Tsoi; B. S. H. Kwan; C. C. C. Cheung; A. P. C. Chan; P. H. W. Leong","Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, China; Hong Kong Sci. & Technol. Park, Cluster Technol. Ltd., Hong Kong, China; Hong Kong Sci. & Technol. Park, Cluster Technol. Ltd., Hong Kong, China; Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Shatin, China","2008 16th International Symposium on Field-Programmable Custom Computing Machines","22 Dec 2008","2008","","","149","159","The map-reduce model requires users to express their problem in terms of a map function that processes single records in a stream, and a reduce function that merges all mapped outputs to produce a final result. By exposing structural similarity in this way, a number of key issues associated with the design of custom computing machines including parallelisation; design complexity; software-hardware partitioning; hardware-dependency, portability and scalability can be easily addressed. We present an implementation of a map-reduce library supporting parallel field programmable gate arrays (FPGAs) and graphics processing units (GPUs). Parallelisation due to pipelining, multiple data paths and concurrent execution of FPGA/GPU hardware is automatically achieved. Users first specify the map and reduce steps for the problem in ANSI Cand no knowledge of the underlying hardware or parallelisation is needed. The source code is then manually translated into a pipelined data path which, along with the map-reduce library, is compiled into appropriate binary configurations for the processing units. We describe our experience in developing a number of benchmark problems in signal processing, Monte Carlo simulation and scientific computing as well as report on the performance of FPGA, GPU and heterogeneous systems.","","978-0-7695-3307-0","10.1109/FCCM.2008.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724898","reconfigurable computing;map reduce;hardware/software codesign","Field programmable gate arrays;Hardware;Concurrent computing;Parallel processing;Graphics;Signal processing;Computer science;Scalability;Software libraries;Pipeline processing","computer graphics;field programmable gate arrays;functional languages;Monte Carlo methods;parallel machines","custom computing machines;map-reduce model;map function;parallelisation;design complexity;software-hardware partitioning;hardware-dependency;field programmable gate arrays;graphics processing units;ANSI C;source code;map-reduce library;Monte Carlo simulation","","50","","17","","22 Dec 2008","","","IEEE","IEEE Conferences"
"On the Design of a Demo for Exhibiting rCUDA","C. Reaño; F. Pérez; F. Silla","Univ. Politec. de Valencia, Valencia, Spain; Univ. Politec. de Valencia, Valencia, Spain; Univ. Politec. de Valencia, Valencia, Spain","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","9 Jul 2015","2015","","","1169","1172","CUDA is a technology developed by NVIDIA which provides a parallel computing platform and programming model for NVIDIA GPUs and compatible ones. It takes benefit from the enormous parallel processing power of GPUs in order to accelerate a wide range of applications, thus reducing their execution time. rCUDA (remote CUDA) is a middleware which grants applications concurrent access to CUDA-compatible devices installed in other nodes of the cluster in a transparent way so that applications are not aware of being accessing a remote device. In this paper we present a demo which shows, in real time, the overhead introduced by rCUDA in comparison to CUDA when running image filtering applications. The approach followed in this work is to develop a graphical demo which contains both an appealing design and technical contents.","","978-1-4799-8006-2","10.1109/CCGrid.2015.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152613","GPGPU;CUDA;HPC;virtualization","Graphics processing units;Gray-scale;Servers;Parallel processing;Acceleration;Middleware;Color","graphics processing units;image filtering;middleware;parallel architectures;parallel programming","rCUDA;demo design;NVIDIA;parallel computing platform;programming model;NVIDIA GPU;parallel processing power;remote CUDA;middleware;concurrent access;CUDA-compatible devices;image filtering applications","","","","4","","9 Jul 2015","","","IEEE","IEEE Conferences"
"Heterogeneous tasking on SMP/FPGA SoCs: The case of OmpSs and the Zynq","A. Filgueras; E. Gil; C. Alvarez; D. Jimenez; X. Martorell; J. Langer; J. Noguera","Barcelona Supercomputing Center, Barcelona; Universitat Politecnica de Catalunya, Barcelona; Barcelona Supercomputing Center, Barcelona; Barcelona Supercomputing Center, Barcelona; Barcelona Supercomputing Center, Barcelona; Xilinx Research Lab, Dublin; Xilinx Research Lab, Dublin","2013 IFIP/IEEE 21st International Conference on Very Large Scale Integration (VLSI-SoC)","25 Nov 2013","2013","","","290","291","OmpSs is a directive-based programming model that uses OpenMP-like directives, that allow to execute the tasks annotated on both the SMPs and as FPGA kernels on modern SoC processors, like the Xilinx Zynq platform. OmpSs includes the support for accelerators (MIC, GPUs, FPGAs) and task dependencies, like OpenMP 4.0 will support. In this paper we present our approach for the support of FPGAs and the Zynq SoC, the current status of the implementation, its analysis and performance evaluation.","2324-8440","978-1-4799-0524-9","10.1109/VLSI-SoC.2013.6673293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673293","","Field programmable gate arrays;Hardware;Runtime;System-on-chip;Ecosystems;Programming;Software","field programmable gate arrays;system-on-chip","heterogeneous tasking;SMP-FPGA SoC;OmpS;directive-based programming model;OpenMP-like directives;FPGA kernels;SoC processors;Xilinx Zynq platform;MIC;GPU;OpenMP 4.0","","3","","1","","25 Nov 2013","","","IEEE","IEEE Conferences"
"A Compiler-assisted Runtime-prefetching Scheme for Heterogenous Platforms","B. Shou; X. Hou; L. Chen","ICT, Beijing, China; ICT, Beijing, China; ICT, Beijing, China","2011 International Conference on Parallel Architectures and Compilation Techniques","29 Dec 2011","2011","","","215","215","GPGPU has been widely adopted by industry and academia. For real applications on industry, however, the data communications between CPUs and GPUs often dramatically slow down the overall performance. Another difficulty raised by GPGPU is the programming productivity. OpenMP is a high-level programming model widely accepted by industry. A software distributed shared memory system (DSM) is implemented to provide a logic shared memory space and to manage data communications between CPUs and GPUs. The DSM is block-based, and the block size is adjustable based on loop partitioning parameters. In this work, we optimize the DSM system using a compiler-assisted data-prefetching scheme. There is a prefetching thread and a prefetching worker for each sepa rated memory. The prefetching thread looks into the future, applies inter-thread use-def analysis to judge which part of the USE region has already been generated by computing threads and produces prefetching requests. The prefetching worker carries out the prefetching operations.","1089-795X","978-1-4577-1794-9","10.1109/PACT.2011.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113825","","Prefetching;Runtime;Data communication;Kernel;Industries;Programming","distributed shared memory systems;graphics processing units;multiprocessing systems;parallel architectures;program compilers;program control structures;storage management","compiler assisted runtime prefetching scheme;heterogenous platforms;OpenMP;high level programming model;Pthreads;CUDA code;software distributed shared memory system;logic shared memory space;data communications manage;CPU;GPU;DSM;loop partitioning parameters;prefetching thread;prefetching worker;interthread use-def analysis;USE region","","","","","","29 Dec 2011","","","IEEE","IEEE Conferences"
"A Scheduling and Runtime Framework for a Cluster of Heterogeneous Machines with Multiple Accelerators","T. Beri; S. Bansal; S. Kumar","Indian Inst. of Technol. Delhi, New Delhi, India; Indian Inst. of Technol. Delhi, New Delhi, India; Indian Inst. of Technol. Delhi, New Delhi, India","2015 IEEE International Parallel and Distributed Processing Symposium","20 Jul 2015","2015","","","146","155","We present a runtime system for simple and efficient programming of CPU+GPU clusters. The programmer focuses on core logic, while the system undertakes task allocation, load balancing, scheduling, data transfer, etc. Our programming model is based on a shared global address space, made efficient by transaction style bulk-synchronous semantics. This model broadly targets coarse-grained data parallel computation particularly suited to multi-GPU heterogeneous clusters. We describe our computation and communication scheduling system and report its performance ona few prototype applications. For example, parallelization of matrix multiplication or 2D FFT using our system requires the regular CPU/GPU implementations and about 30 lines of additional C code to set up the runtime. Our runtime system achieves a performance of 5.61 TFlop/s while multiplying two square matrices of 1.56 billion elements each over a 10-nodecluster with 20 GPUs. This performance is possible due toa number of critical optimizations working in concert. These include perfecting, pipelining, maximizing overlap between computation and communication, and scheduling efficiently across heterogeneous devices of vastly different capacities.","1530-2075","978-1-4799-8649-1","10.1109/IPDPS.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161504","High Performance Computing;Heterogeneous Architectures;Hybrid CPU-GPU Clusters;Work Stealing;Multi Scheduling","Graphics processing units;Runtime;Kernel;Programming;Message systems;Data transfer;Subscriptions","graphics processing units;parallel processing;resource allocation;scheduling","scheduling framework;runtime framework;heterogeneous machine;accelerator;CPU+GPU cluster programming;graphics processing unit;task allocation;load balancing;data transfer;transaction style bulk-synchronous semantics;high-performance computing","","7","","47","","20 Jul 2015","","","IEEE","IEEE Conferences"
"Evaluating Multi-core and Many-Core Architectures through Accelerating an Alternating Direction Implicit CFD Solver","L. Deng; J. Fang; F. Wang; H. Bai","Comput. Aerodynamics Inst., China Aerodynamics R&D Center, Mianyang, China; Software Institue, Nat. Univ. of Defense Technol., Changsha, China; Software Institue, Nat. Univ. of Defense Technol., Changsha, China; Comput. Aerodynamics Inst., China Aerodynamics R&D Center, Mianyang, China","2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)","24 Apr 2017","2016","","","1","10","In this paper, we accelerate a double-precision alternating direction implicit (ADI) solver for three-dimensional compressible Navier-Stokes equations from our in-house computational fluid dynamics (CFD) software on the latest multi-core and many-core architectures (Intel Ivy Bridge CPU, Intel Xeon Phi 7110P coprocessor and NVIDIA Kepler K20c GPU). For the GPU platform, both the OpenACC-based and the CUDA-based versions of the ADI solver are developed. To achieve high performance, we use a series of optimization techniques. For the Ivy Bridge CPU and Xeon Phi, we focus on three categories of optimization techniques: thread parallelism for multi-/many-core scaling, data parallelism to exploit the SIMD mechanism and improving on-chip data reuse, to maximize the performance. Also, we provide an in-depth analysis on the performance differences between Ivy Bridge and Xeon Phi. Our numerical experiments show that the proposed CUDA-based ADI solver can achieve a speedup of 9.7 on a Kepler GPU in contrast to a single naive serial version and our optimization techniques can improve the performance of the ADI solver by 2.5x on two Ivy Bridge CPUs and 1.7x on the Intel Xeon Phi coprocessor. We also notice that the OpenACC-based version runs around 29% slower than the CUDA-based one with careful manual optimizations. Besides, we systematically evaluate the programmability of the three platforms. Our insights facilitate the programmers to select a right platform with a suitable programming model according to their target applications.","","978-1-5090-4152-7","10.1109/ISPDC.2016.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7904262","performance;programmability;optimization techniques;alternating direction implicit;CFD solver;Ivy Bridge;Xeon Phi;GPU;CUDA;OpenACC","Graphics processing units;Mathematical model;Optimization;Instruction sets;Programming;Computer architecture;Bridges","computational fluid dynamics;graphics processing units;microprocessor chips;multiprocessing systems;multi-threading;Navier-Stokes equations;parallel architectures","multicore architectures;many-core architectures;alternating direction implicit CFD solver;double-precision alternating direction implicit solver;ADI solver;three-dimensional compressible Navier-Stokes equations;computational fluid dynamics software;CFD software;Intel Ivy Bridge CPU;Intel Xeon Phi 7110P coprocessor;NVIDIA Kepler K20c GPU;OpenACC;CUDA;optimization techniques;thread parallelism;data parallelism;on-chip data reuse","","","","26","","24 Apr 2017","","","IEEE","IEEE Conferences"
"Characteristic mode analysis of arbitrary electromagnetic structures using FEKO","D. J. Ludick; E. Lezar; U. Jakobus","EM Software & Systems - S.A. (Pty) Ltd, 32 Techno Avenue, Technopark, Stellenbosch, 7600, South Africa; EM Software & Systems - S.A. (Pty) Ltd, 32 Techno Avenue, Technopark, Stellenbosch, 7600, South Africa; EM Software & Systems - S.A. (Pty) Ltd, 32 Techno Avenue, Technopark, Stellenbosch, 7600, South Africa","2012 International Conference on Electromagnetics in Advanced Applications","11 Oct 2012","2012","","","208","211","This paper considers the characteristic mode analysis (CMA) of arbitrary electromagnetic structures using the comprehensive 3D electromagnetic field solver, FEKO [1]. The theory of characteristic modes, as presented in [2], is used to derive the real orthogonal current modes. These modes are obtained by solving a generalised symmetric eigenvalue problem defined by the real and imaginary parts of the Method-of-Moments (MoM) impedance matrix. The research presented in this article discusses the techniques used in FEKO to solve this generalised eigenproblem. Furthermore, paralleli-sation using both distributed and shared memory programming models, as well as GPU computation is considered within the FEKO framework to accelerate the CMA.","","978-1-4673-0335-4","10.1109/ICEAA.2012.6328622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328622","","Eigenvalues and eigenfunctions;Moment methods;Runtime;Graphics processing unit;Impedance;Symmetric matrices;Equations","distributed memory systems;eigenvalues and eigenfunctions;electromagnetic field theory;matrix algebra;method of moments;shared memory systems","characteristic mode analysis;arbitrary electromagnetic structures;CMA;comprehensive 3D electromagnetic field solver;real orthogonal current modes;generalised symmetric eigenvalue problem;method-of-moments impedance matrix;MoM impedance matrix;GPU computation;FEKO framework;shared memory programming model","","14","","10","","11 Oct 2012","","","IEEE","IEEE Conferences"
"Efficient parallel CKY parsing using GPUs","Y. Yi; C. Lai; S. Petrov",NA; NA; NA,"Journal of Logic and Computation","18 Jan 2018","2014","24","2","375","393","Low-latency solutions for syntactic parsing are needed if parsing is to become an integral part of user-facing natural language applications. Unfortunately, most state-of-the-art constituency parsers employ large probabilistic context-free grammars for disambiguation, which renders them impractical for real-time use. Meanwhile, Graphics Processor Units (GPUs) have become widely available, offering the opportunity to alleviate this bottleneck by exploiting the fine-grained data parallelism found in the Cocke–Kasami–Younger (CKY) algorithm. In this article, we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm. We use the Compute Unified Device Architecture (CUDA) programming model to reimplement a state-of-the-art parser, and compare its performance on three recent GPUs with different architectural features. Our best results show a 33-fold speedup for the CUDAparser compared to a sequential C implementation.","1465-363X","","10.1093/logcom/exs078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8200101","CKY parsing;Viterbi parsing;parallel parsing;GPU;CUDA","","","","","","","","","18 Jan 2018","","","OUP","OUP Journals"
"Performance and Portability Studies with OpenACC Accelerated Version of GTC-P","Y. Wei; Y. Wang; L. Cai; W. Tang; B. Wang; S. Ethier; S. See; J. Lin","Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China; Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China; Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China; Princeton Inst. of Comput. Sci. & Eng., Princeton Univ., Princeton, NJ, USA; Princeton Inst. of Comput. Sci. & Eng., Princeton Univ., Princeton, NJ, USA; Princeton Plasma Phys. Lab., Princeton, NJ, USA; Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China; Center for High Performance Comput., Shanghai Jiao Tong Univ., Shanghai, China","2016 17th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)","8 Jun 2017","2016","","","13","18","Accelerator-based heterogeneous computing is of paramount importance to High Performance Computing. The increasing complexity of the cluster architectures requires more generic, high-level programming models. OpenACC is a directive-based parallel programming model, which provides performance on and portability across a wide variety of platforms, including GPU, multicore CPU, and many-core processors. GTC-P is a discovery-science-capable real-world application code based on the Particle-In-Cell (PIC) algorithm that is well-established in the HPC area. Several native versions of GTC-P have been developed for supercomputers on TOP500 with different architectures, including Titan, Mira, etc. Motivated by the state-of-art portability, we implemented the first OpenACC version of GTC-P and evaluated its performance portability across NVIDIA GPUs, Intel x86 and OpenPOWER CPUs. In this paper, we also proposed two key optimization methods for OpenACC implementation of PIC algorithm on multicore CPU and GPU including removing atomic operation and taking advantage of shared memory. OpenACC shows both impressive productivity and performance in a perspective of portability and scalability. The OpenACC version achieves more than 90% performance compared with the native versions with only about 300 LOC.","","978-1-5090-5081-9","10.1109/PDCAT.2016.019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7943324","Gyrokinetic PIC code;GTC-P;OpenACC;CUDA;GPU;OpenPOWER","Graphics processing units;Multicore processing;Instruction sets;Optimization;Acceleration;Scalability","computational complexity;graphics processing units;multiprocessing systems;parallel processing;performance evaluation","GTC-P;accelerator-based heterogeneous computing;high performance computing;cluster architecture complexity;directive-based parallel programming model;multicore CPU;many-core processors;discovery-science-capable real-world application code;particle-in-cell algorithm;PIC algorithm;performance portability;NVIDIA GPUs;Intel x86;OpenPOWER CPUs;optimization methods;OpenACC implementation","","1","","10","","8 Jun 2017","","","IEEE","IEEE Conferences"
"HSAemu - A full system emulator for HSA platforms","J. -H. Ding; W. Hsu; BaiCheng Jeng; S. Hung; Y. Chung","National Tsing Hua University, Hsinchu, 30013, Taiwan; National Taiwan University, Taipei, 10617, Taiwan; National Tsing Hua University, Hsinchu, 30013, Taiwan; National Taiwan University, Taipei, 10617, Taiwan; National Tsing Hua University, Hsinchu, 30013, Taiwan","2014 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","4 Dec 2014","2014","","","1","10","Heterogeneous System Architecture (HSA) is an open industry standard designed to support a large variety of data-parallel and task-parallel programming models. Currently, most of HSA hardware and software components are still in development. It is helpful to provide various heterogeneous simulation environments for HSA developers in developing HSA software stacks. This paper presents the design of HSAemu, a full system emulator for the HSA platform, and illustrates how those HSA features are implemented in the simulator. HSAemu provides an infrastructure of heterogeneous simulation environments by supporting required HSA features, including hUMA, hQ and HSAIL. Based on the infrastructure, HSAemu provide two simulation models, FastSim and DeepSim, for high-speed functional emulation and slow cycle-accurate simulation, respectively. In our preliminary experiments, HSAemu helps test a complete HSA software stack and profile system performance. Our case studies show that HSAemu is very useful as a hardware/software co-design tool for heterogeneous systems.","","978-1-4503-3051-0","10.1145/2656075.2656088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971842","HSA;GPU simulation;parallel simulation","Graphics processing units;Computational modeling;Kernel;Hardware;Computer architecture;Synchronization","digital simulation;graphics processing units","HSAemu;full-system emulator;HSA platforms;heterogeneous system architecture;open industry standard;data-parallel programming model;task-parallel programming model;HSA hardware components;HSA software components;heterogeneous simulation environments;HSA software stacks;hUMA;hQ;HSAIL;FastSim simulation model;DeepSim simulation model;high-speed functional emulation;slow-cycle-accurate simulation;profile system performance;hardware/software co-design tool","","3","","27","","4 Dec 2014","","","IEEE","IEEE Conferences"
"Evaluation of Performance Portability of Applications and Mini-Apps across AMD, Intel and NVIDIA GPUs","J. Kwack; J. Tramm; C. Bertoni; Y. Ghadar; B. Homerding; E. Rangel; C. Knight; S. Parker","Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA; Leadership Computing Facility / Computational Science Division, Argonne National Laboratory,Lemont,IL,USA","2021 International Workshop on Performance, Portability and Productivity in HPC (P3HPC)","28 Dec 2021","2021","","","45","56","This paper will evaluate the progress being made on achieving performance portability by a sub-set of ECP applications, or their related mini-apps, across a diverse spectrum of applications domains and approaches to achieving performance portability. The applications or mini-apps evaluated are AMR-Wind, HACC, SW4, GAMESS RI-MP2, XSBench, and TestSNAP. These codes are being redeveloped using the SYCL, OpenMP, RAJA, or Kokkos programming models, or the AMReX framework and in this paper we assess their performance portability across the AMD MI100, Intel Gen9, and NVIDIA A100 GPUs. Since each GPU has different performance characteristics we have utilized the roofline performance model to compute the performance efficiency and evaluate performance portability across the three platforms. The merits of different metrics for quantifying performance portability are considered and a metric based on the standard deviation of roofline efficiencies is proposed as a preferred metric. Finally, observations on developer productivity are made based on the experience gained working with these applications.","","978-1-6654-2439-4","10.1109/P3HPC54578.2021.00008","DOE Office of Science User Facility(grant numbers:DE-AC02-06CHl1357); National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652861","high performance computing;performance portability;performance efficiency;roofline performance analysis;GPU;portable programming model;software framework","Measurement;Productivity;Analytical models;Codes;Computational modeling;Conferences;Graphics processing units","","","","","","40","","28 Dec 2021","","","IEEE","IEEE Conferences"
"A Compiler-Based Tool for Array Analysis in HPC Applications","A. Qawasmeh; B. Chapman; A. Banerjee","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Pet. Geo-Services, Houston, TX, USA","2012 41st International Conference on Parallel Processing Workshops","25 Oct 2012","2012","","","454","463","Array region analysis plays a significant role in various optimizations at compile time. Displaying array access information efficiently in HPC applications has been a vital challenge for scientists and developers for the past few years. Dragon array region analysis tool is a powerful and interactive tool that was built on top of the Open UH compiler, an open source C/C++/Fortran compiler, that supports OpenMP and CAF programming models. We have extended the linear-based Region analysis method and the high level IR (WHIRL) of Open UH to visualize the static and interprocedural array region accesses, the frequency of these accesses per access mode, the access mode in which the array is processed, the number of dimensions, the size of each dimension, the total size in bytes allocated to this array statically, and the memory location. We have also defined the access density term which illustrates the frequency of accesses per bytes allocated to these arrays. The information provided enables users to efficiently develop and optimize HPC applications by understanding procedure side effects and finding inefficiencies in defining arrays, which guides to a better memory allocation and cache usage. Moreover, we demonstrate the access density of the portions of arrays that have been accessed, which is crucial to reduce data transfers between host and device when using directive-based GPU programming models.","2332-5690","978-1-4673-2509-7","10.1109/ICPPW.2012.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337513","Analysis tool;Linear-based techniques;Compiler-based tool;Array Region Analysis","Arrays;Optimization;Programming;Data mining;Indexes;Graphics processing unit","application program interfaces;C++ language;cache storage;data visualisation;FORTRAN;graphics processing units;multi-threading;program compilers;public domain software;shared memory systems","compiler-based tool;Dragon array region analysis tool;HPC applications;interactive tool;Open UH compiler;open source C compiler;open source C++ compiler;open source Fortran compiler;OpenMP programming model;CAF programming model;linear-based region analysis method;high level IR;WHIRL;static array region access visualization;interprocedural array region access visualization;memory location;memory allocation;cache usage;data transfer reduction;directive-based GPU programming models;multithreaded programming API","","3","","28","","25 Oct 2012","","","IEEE","IEEE Conferences"
"Exploration of OpenCL for FPGAs using SDAccel and comparison to GPUs and multicore CPUs","L. Kalms; D. Göhringer","Technische Universität Dresden, Dresden, Germany; Technische Universität Dresden, Dresden, Germany","2017 27th International Conference on Field Programmable Logic and Applications (FPL)","5 Oct 2017","2017","","","1","4","Due to energy efficiency, heterogeneous computing is gaining more and more attention. Since FPGA implementations are time consuming, high-level synthesis (HLS) is used to close the productivity gap. OpenCL has become accepted as a good programming model for HLS, due to its portability, good capability of design verification and rich instruction set. This work implements different optimization strategies using OpenCL for a heterogeneous system containing CPU, integrated GPU, GPU and FPGA. Energy efficiency and performance of the architectures are compared using a feature detection algorithm. It is shown how to maximize performance while hitting the maximum memory bandwidth and keeping the resource utilization low for the SDAccel tool from Xilinx. The evaluation shows the great streaming capability of OpenCL for FPGAs. The FPGA achieves a speed up of 62.8 and consumes 49 times less energy for the application in comparison to an optimized single threaded CPU implementation in full HD.","1946-1488","978-9-0903-0428-1","10.23919/FPL.2017.8056847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056847","OpenCL;SDAccel;GPU;CPU;FPGA;Energy Efficiency;Performance;Image Processing;Accelerators","Kernel;Field programmable gate arrays;Graphics processing units;Bandwidth;Optimization;Resource management","electronic engineering computing;energy conservation;feature extraction;field programmable gate arrays;graphics processing units;high level synthesis;microprocessor chips;multiprocessing systems;optimisation;power aware computing;public domain software","optimization strategies;integrated GPU;Xilinx;HLS;high-level synthesis;FPGA implementations;heterogeneous computing;SDAccel tool;feature detection algorithm;energy efficiency;heterogeneous system;OpenCL;multicore CPU","","7","","18","","5 Oct 2017","","","IEEE","IEEE Conferences"
"Resource Centered Computing Delivering High Parallel Performance","J. Gustedt; S. Vialle; P. Mercier","INRIA Nancy - Grand Est, Nancy, France; SUPELEC, Metz, France; SUPELEC, Metz, France","2014 IEEE International Parallel & Distributed Processing Symposium Workshops","4 Dec 2014","2014","","","77","88","Modern parallel programming requires a combination of different paradigms, expertise and tuning, that correspond to the different levels in today's hierarchical architectures. To cope with the inherent difficulty, ORWL (ordered read-write locks) presents a new paradigm and toolbox centered around local or remote resources, such as data, processors or accelerators. ORWL programmers describe their computation in terms of access to these resources during critical sections. Exclusive or shared access to the resources is granted through FIFOs and with read-write semantic. ORWL partially replaces a classical runtime and offers a new API for resource centric parallel programming. We successfully ran an ORWL benchmark application on different parallel architectures (a multicore CPU cluster, a NUMA machine, a CPU+GPU cluster). When processing large data we achieved scalability and performance similar to a reference code built on top of MPI+OpenMP+CUDA. The integration of optimized kernels of scientific computing libraries (ATLAS and cuBLAS) has been almost effortless, and we were able to increase performance using both CPU and GPU cores on our hybrid hierarchical cluster simultaneously. We aim to make ORWL a new easy-to-use and efficient programming model and toolbox for parallel developers.","","978-1-4799-4116-2","10.1109/IPDPSW.2014.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969373","resource centered computing;read-write locks;clusters;accelerators;GPU;experiments;performance","Computational modeling;Graphics processing units;Kernel;Computer architecture;Parallel processing;Parallel programming","application program interfaces;graphics processing units;natural sciences computing;parallel architectures;parallel programming","resource centered computing;high parallel performance;ordered read-write locks;ORWL programmers;remote resources;local resources;critical sections;read-write semantic;API;resource centric parallel programming;parallel architectures;MPI+OpenMP+CUDA;scientific computing libraries;CPU cores;GPU cores;hybrid hierarchical cluster;parallel developers","","","","26","","4 Dec 2014","","","IEEE","IEEE Conferences"
"Implementation and Evaluation of One-Sided PGAS Communication in XcalableACC for Accelerated Clusters","A. Tabuchi; M. Nakao; H. Murai; T. Boku; M. Sato","Grad. Sch. of Syst. & Inf. Eng., Univ. of Tsukuba, Tsukuba, Japan; RIKEN Adv. Inst. for Comput. Sci., Kobe, Japan; RIKEN Adv. Inst. for Comput. Sci., Kobe, Japan; Center for Comput. Sci., Univ. of Tsukuba, Tsukuba, Japan; RIKEN Adv. Inst. for Comput. Sci., Kobe, Japan","2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","13 Jul 2017","2017","","","625","634","Clusters equipped with accelerators such as graphics processing unit (GPU) and Many Integrated Core (MIC) are widely used. For such clusters, programmers write programs for their applications by combining MPI with one of the available accelerator programming models. In particular, OpenACC enables programmers to develop their applications easily, but with lower productivity owing to complex MPI programming. XcalableACC (XACC) is a new programming model, which is an ""orthogonal"" integration of a partitioned global address space (PGAS) language XcalableMP (XMP) and OpenACC. While XMP enables distributed-memory programming on both global-view and local-view models, OpenACC allows operations to be offloaded to a set of accelerators. In the local-view model, programmers can describe communication with the coarray features adopted from Fortran 2008, and we extend them to communication between accelerators. We have designed and implemented an XACC compiler for NVIDIA GPU and evaluated its performance and productivity by using two benchmarks, Himeno benchmark and NAS Parallel Benchmarks CG (NPB-CG). The performance of the XACC version with the Himeno benchmark and NPB-CG are over 85% and 97% in the local-view model against the MPI+OpenACC version, respectively. Moreover, using non-blocking communication makes the performance of local-view version over 89% with the Himeno benchmark. From the viewpoint of productivity, the local-view model provides an intuitive form of array assignment statement for communication.","","978-1-5090-6611-7","10.1109/CCGRID.2017.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973750","Accelerator;GPU;Cluster;PGAS;Coarray;OpenACC","Graphics processing units;Programming;Synchronization;Electronics packaging;Syntactics;Benchmark testing;Productivity","distributed memory systems;distributed programming;FORTRAN;graphics processing units;message passing;parallel processing","one-sided PGAS communication;XcalableACC;accelerated clusters;graphics processing unit;many integrated core;MIC;accelerator programming models;OpenACC;complex MPI programming;partitioned global address space;PGAS language XcalableMP;distributed-memory programming;global-view models;local-view models;coarray features;Fortran 2008;XACC compiler;NVIDIA GPU;Himeno benchmark;NAS Parallel Benchmarks CG;NPB-CG;nonblocking communication;array assignment statement","","3","","25","","13 Jul 2017","","","IEEE","IEEE Conferences"
"Complete solution of eight puzzle problem using BFS in CUDA environment","M. Sultana; R. N. Dutta; S. K. Setua","Dept. of CSE, University of Calcutta, Kolkata, India; Dept. of CSE, University of Calcutta, Kolkata, India; Dept. of CSE, University of Calcutta, Kolkata, India","2015 IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE)","31 Mar 2016","2015","","","333","337","The eight puzzle problem is the largest completely solvable problem of n×n sliding puzzle problems. It is combinatorial in nature, but there is a large problem space of 9! /2. Objective of this work is to find the complete solution of eight puzzle problem i.e. examining all the permutations for solvability. Using Breadth First search (BFS) graph traversal we can reach the solution for a definite goal. The parallel algorithm is capable of providing us with much more faster solution using Compute Unified Device Architecture (CUDA). CUDA facility enables us to use best possible available computation power of GPU (Graphics Processing Unit). In this paper, we present fast implementation of common graph operation like breadth-first search to find out complete solution of eight puzzle problem on the GPU using the CUDA programming model. Our implementations exhibit better performance. The availability and spread of GPUs to desktops and laptops make them ideal candidates to accelerate graph operations over the CPU-only implementations.","","978-1-4673-8786-6","10.1109/WIECON-ECE.2015.7443931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443931","Eight puzzle problem;Complete Solution;Breadth-first search;GPU;CUDA","Graphics processing units;Arrays;Indexes;Instruction sets;Search problems;Computational modeling","computability;graph theory;graphics processing units;parallel algorithms;parallel architectures;tree searching","eight puzzle problem;sliding puzzle problems;permutations;solvability;breadth first search graph traversal;BFS graph traversal;parallel algorithm;compute unified device architecture;GPU;graphics processing unit;CUDA programming","","","","12","","31 Mar 2016","","","IEEE","IEEE Conferences"
"On the Programmability and Performance of Heterogeneous Platforms","K. Krommydas; T. R. W. Scogland; W. -C. Feng","Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA","2013 International Conference on Parallel and Distributed Systems","1 May 2014","2013","","","224","231","General-purpose computing on an ever-broadening array of parallel devices has led to an increasingly complex and multi-dimensional landscape with respect to programmability and performance optimization. The growing diversity of parallel architectures presents many challenges to the domain scientist, including device selection, programming model, and level of investment in optimization. All of these choices influence the balance between programmability and performance. In this paper, we characterize the performance achievable across a range of optimizations, along with their programmability, for multi- and many-core platforms - specifically, an Intel Sandy Bridge CPU, Intel Xeon Phi co-processor, and NVIDIA Kepler K20 GPU - in the context of an n-body, molecular-modeling application called GEM. Our systematic approach to optimization delivers implementations with speed-ups of 194.98×, 885.18×, and 1020.88× on the CPU, Xeon Phi, and GPU, respectively, over the naive serial version. Beyond the speed-ups, we characterize the incremental optimization of the code from naive serial to fully hand-tuned on each platform through four distinct phases of increasing complexity to expose the strengths and weaknesses of the programming models offered by each platform.","1521-9097","978-1-4799-2081-5","10.1109/ICPADS.2013.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808178","performance;programmability;optimization;AVX;GPU;Intel MIC;NVIDIA Kepler K20;Xeon Phi;CUDA;OpenACC","Optimization;Graphics processing units;Vectors;Computer architecture;Programming;Performance evaluation;Mathematical model","coprocessors;general purpose computers;multiprocessing systems;optimisation;parallel architectures;performance evaluation","programmability;heterogeneous platform performance;general-purpose computing;parallel devices;performance optimization;parallel architectures;device selection;optimization investment level;multicore platform;many-core platforms;Intel Sandy Bridge CPU;Intel Xeon Phi coprocessor;NVIDIA Kepler K20 GPU;n-body molecular-modeling application;GEM;incremental optimization;naive serial;programming models","","6","","12","","1 May 2014","","","IEEE","IEEE Conferences"
"Compute Intensive Algorithm on Heterogeneous System: A Case Study about Fourier Transform","A. Galizia; E. Danovaro; G. Ripepi; A. Clematis","Inst. for Appl. Math. & Inf. Technol., Genoa, Italy; Inst. for Appl. Math. & Inf. Technol., Genoa, Italy; Inst. for Appl. Math. & Inf. Technol., Genoa, Italy; Inst. for Appl. Math. & Inf. Technol., Genoa, Italy","2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","14 Apr 2014","2014","","","223","227","Current workstations can offer really amazing raw computational power: up to 10 TFlops on a single machine equipped with multiple CPUs and accelerators as the Intel Xeon Phi or GPU devices. Such results can only be achieved with a massive parallelism of computational devices, thus the actual barrier posed by the exploitation of modern heterogeneous HPC resources is the difficulty in development and/or (performance) efficient porting of software on such architectures. In this paper, we present an experimental study about achievable performance of a widely used, computational intensive application the Fourier Transform, i.e. Discrete Fourier Transform (DFT) and Fast Fourier Transform. We propose an evaluation of the benefits obtained exploiting such resources in terms of performance and programming efforts in the development of the code with a emphasis on the programming approach adopted for code parallelization. With the exception of the interesting performance achieved exploiting GPU for the DFT algorithm, the use state-ofthe- art software libraries provide the best solution since they represent a good compromise to balance programming efforts and performance achievements.","2377-5750","978-1-4799-2729-6","10.1109/PDP.2014.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787277","Complex Heterogeneous System;Parallel Programming Model;Fourier Transform","Graphics processing units;Discrete Fourier transforms;Computer architecture;Programming;Libraries;Hardware","discrete Fourier transforms;fast Fourier transforms;graphics processing units;multiprocessing systems","compute intensive algorithm;heterogeneous system;multiple CPU;multiple accelerators;GPU device;Intel Xeon Phi device;heterogeneous HPC resources;discrete Fourier transform;fast Fourier transform;code parallelization;DFT algorithm;software libraries","","","","10","","14 Apr 2014","","","IEEE","IEEE Conferences"
"VComputeBench: A Vulkan Benchmark Suite for GPGPU on Mobile and Embedded GPUs","N. Mammeri; B. Juurlink",Technische Universität Berlin; Technische Universität Berlin,"2018 IEEE International Symposium on Workload Characterization (IISWC)","13 Dec 2018","2018","","","25","35","GPUs have become immensely important computational units on embedded and mobile devices. However, GPGPU developers are often not able to exploit the compute power offered by GPUs on these devices mainly due to the lack of support of traditional programming models such as CUDA and OpenCL. The recent introduction of the Vulkan API provides a new programming model that could be explored for GPGPU computing on these devices, as it supports compute and promises to be portable across different architectures. In this paper we propose VComputeBench, a set of benchmarks that help developers understand the differences in performance and portability of Vulkan. We also evaluate the suitability of Vulkan as an emerging cross-platform GPGPU framework by conducting a thorough analysis of its performance compared to CUDA and OpenCL on mobile as well as on desktop platforms. Our experiments show that Vulkan provides better platform support on mobile devices and can be regarded as a good cross-platform GPGPU framework. It offers comparable performance and with some low-level optimizations it can offer average speedups of 1.53× and 1.66× compared to CUDA and OpenCL respectively on desktop platforms and 1.59× average speedup compared to OpenCL on mobile platforms. However, while Vulkan's low-level control can enhance performance, it requires a significantly higher programming effort.","","978-1-5386-6780-4","10.1109/IISWC.2018.8573477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573477","VComputeBench;Vulkan;SPIR-V;GPGPU;CUDA;OpenCL;Rodinia;Mobile","Graphics processing units;Benchmark testing;Programming;Computational modeling;Performance evaluation;Mobile handsets;Computer architecture","application program interfaces;electronic engineering computing;graphics processing units;optimisation","VComputeBench;Vulkan benchmark suite;computational units;mobile devices;GPGPU developers;traditional programming models;Vulkan API;programming model;GPGPU computing;desktop platforms;platform support;mobile platforms;embedded GPU;cross-platform GPGPU framework;Vulkan low-level control;mobile GPU;low-level optimizations","","2","","34","","13 Dec 2018","","","IEEE","IEEE Conferences"
"Using Graphics Processor Units (GPUs) for Automatic Video Structuring","P. Kehoe; A. F. Smeaton","Dublin City University, Glasnevin, Dublin 9, Ireland.; Dublin City University, Glasnevin, Dublin 9, Ireland.","Eighth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS '07)","30 Jul 2007","2007","","","18","18","The rapid pace of development of graphic processor units (GPUs) in recent years in terms of performance and programmability has attracted the attention of those seeking to leverage alternative architectures for better performance than that which commodity CPUs can provide. In this paper, the potential of the GPU in automatically structuring video is examined, specifically in shot boundary detection and representative keyframe selection techniques. We first introduce the programming model of the GPU and outline the implementation of techniques for shot boundary detection and representative keyframe selection on both the CPU and GPU, using histogram comparisons. We compare the approaches and present performance results for both the CPU and GPU. Overall these results demonstrate the significant potential for the GPU in this domain.","","0-7695-2818-X","10.1109/WIAMIS.2007.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279126","","Graphics;Gunshot detection systems;Rendering (computer graphics);Bandwidth;Random access memory;Read-write memory;Histograms;Video compression;Programming profession;Acceleration","digital signal processing chips;image representation;video signal processing","graphics processor units;automatic video structuring;programmability h;alternative architectures;shot boundary detection;representative keyframe selection;programming model;histogram comparisons","","2","","4","","30 Jul 2007","","","IEEE","IEEE Conferences"
"Streaming FFT Asynchronously on Graphics Processor Units","L. Zhao; Z. Shengbing; Z. Meng; Z. Yi","NA; Eng. Res. Center of Embedded Syst. Integration, Northwestern Polytech. Univ. (NWPU), Xi'an, China; Eng. Res. Center of Embedded Syst. Integration, Northwestern Polytech. Univ. (NWPU), Xi'an, China; Sch. of Comput., Northwestern Polytech. Univ. (NWPU), Xi'an, China","2010 International Forum on Information Technology and Applications","11 Nov 2010","2010","1","","308","312","The Fast Fourier Transform (FFT), which charactered in memory-access-intensive, follows a divide-and-conquer strategy, is one of the most important and heavily used kernel in scientific computing. The newest generation of Graphics Processor Units (GPUs) implement a stream architecture besides acting as powerful massively parallel coprocessor. Fouthermore, the intruduction of APIs for general-purpose computation on GPUs mades GPUs an attractive choice for high-performance numerical and scientific computing. In this work we deal with the implementation of the FFT on a novel NVIDIA GPU, using the CUDA programming model. By optimizing the organiztion of signal data, exploiting the memory hierairchy, and associating the stream to different operations, we efficiently overlap kernel execution and data transfer. Our results indicate a significant performance improvement over GPU-based and CPU-based FFT algorithms. The speedup is 18 percent higher than the original GPU-based on average.","","978-1-4244-7622-0","10.1109/IFITA.2010.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635067","FFT;GPUs;stream;asynchronous communication","Graphics processing unit;Instruction sets;Kernel;Graphics;Programming;Memory management","application program interfaces;computer graphic equipment;coprocessors;fast Fourier transforms;general purpose computers;parallel programming","graphics processor units;streaming;FFT;fast Fourier Transform;divide and conquer strategy;stream architecture;parallel coprocessor;API;general purpose computation;GPU;CUDA programming model","","1","","17","","11 Nov 2010","","","IEEE","IEEE Conferences"
"Accelerating SVMs by integrating GPUs into MapReduce clusters","S. Herrero-Lopez","Intelligent Engineering Systems Laboratory, Massachusetts Institute of Technology, Cambridge, 02139, USA","2011 IEEE International Conference on Systems, Man, and Cybernetics","21 Nov 2011","2011","","","1298","1305","The uninterrupted growth of information repositories has progressively lead data-intensive applications, such as MapReduce-based systems, to the mainstream. The MapReduce paradigm has frequently proven to be a simple yet flexible and scalable technique to distribute algorithms across thousands of nodes and petabytes of information. Under these circumstances, classic data mining algorithms have been adapted to this model, in order to run in production environments. Unfortunately, the high latency nature of this architecture has relegated the applicability of these algorithms to batch-processing scenarios. In spite of this shortcoming, the emergence of massively threaded shared-memory multiprocessors, such as Graphics Processing Units (GPU), on the commodity computing market has enabled these algorithms to be executed orders of magnitude faster, while keeping the same MapReduce based model. In this paper, we propose the integration of massively threaded shared-memory multiprocessors into MapReduce-based clusters creating a unified heterogeneous architecture that enables executing Map and Reduce operators on thousands of threads across multiple GPU devices and nodes, while maintaining the built-in reliability of the baseline system. For this purpose, we created a programming model that facilitates the collaboration of multiple CPU cores and multiple GPU devices towards the resolution of a data intensive problem. In order to prove the potential of this hybrid system, we take a popular NP-Hard supervised learning algorithm, the Support Vector Machine (SVM) and show that a 36x - 192x speedup can be achieved on large datasets without changing the model or leaving the commodity hardware paradigm.","1062-922X","978-1-4577-0653-0","10.1109/ICSMC.2011.6083839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083839","Multiprocessing;Parallel Algorithms;Pattern Classification","Message systems;Graphics processing unit;Support vector machines;Computer architecture;Parallel processing;Computational modeling;Instruction sets","computational complexity;coprocessors;data mining;learning (artificial intelligence);shared memory systems;support vector machines","SVM;GPU;MapReduce clusters;information repositories;data-intensive applications;MapReduce-based systems;MapReduce paradigm;data mining;massively threaded shared-memory multiprocessors;graphics processing units;programming model;NP-hard supervised learning;support vector machine","","11","","22","","21 Nov 2011","","","IEEE","IEEE Conferences"
"Exploiting bit-level parallelism in GPGPUs: A case study on KeeLoq exhaustive key search attack","G. Agosta; A. Barenghi; G. Pelosi","Dipartimento di Elettronica e Informazione (DEI) Politecnico di Milano, 20133 Milano (MI), Italy; Dipartimento di Elettronica e Informazione (DEI) Politecnico di Milano, 20133 Milano (MI), Italy; Dipartimento di Elettronica e Informazione (DEI) Politecnico di Milano, 20133 Milano (MI), Italy","ARCS 2012","21 Jun 2012","2012","","","1","7","Graphic Processing Units (GPU) are increasingly popular in the field of high-performance computing for their ability to provide computational power for massively parallel problems at a reduced cost. However, the programming model exposed by the GPGPU software development tools is often insufficient to achieve full performance, and a major rethinking of algorithmic choices is needed. In this paper, we showcase such an effect on a case study drawn from the cryptography application domain. The pervasive use of cryptographic primitives in modern embedded systems is a growing trend. Small, efficient cryptosystems have been effectively employed to design and implement keyless password-based access control systems in various wireless authentication applications. The security margin provided by these lightweight ciphers should be accurately examined in light of the speed and area constraints imposed by the target environment. We present a re-design of the ASIC-oriented KEELOQ implementation to perform efficient exhaustive key search attacks while fitting tightly the parallel programming model exposed by modern GPUs. Indeed, the bitslicing technique allows the intrinsic parallelism offered by word-oriented SIMD computations to be effectively exploited. Through proper adaptation of the algorithm implementation to a platform radically different from the one it was designed for, we achieved a ×40 speedup in the computation time with respect to a single-core CPU bruteforce attack, employing only consumer grade hardware. The outstanding speedup obtainable points to a significant weakening of the cipher security margin, since it proves that anyone with off-the-shelf hardware is able to circumvent the security measures in place.","","978-3-00-037922-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6222213","","Graphics processing unit;Parallel processing;Registers;Programming;Instruction sets;Computer architecture;Hardware","authorisation;cryptography;embedded systems;graphics processing units;message authentication;parallel programming","bit-level parallelism;KEELOQ exhaustive key search attack;general purpose computing on graphics processing units;high-performance computing;massively parallel problems;programming model;GPGPU software development tools;algorithmic choice thinking;cryptography;cryptographic primitives;embedded systems;cryptosystems;keyless password-based access control systems;wireless authentication applications;lightweight ciphers;ASIC-oriented KEELOQ implementation redesign;parallel programming model;bitslicing technique;word-oriented SIMD computations;single-core CPU bruteforce attack;consumer grade hardware;cipher security margin;off-the-shelf hardware","","","","17","","21 Jun 2012","","","IEEE","IEEE Conferences"
"Study of BDRM Asynchronous Parallel Computing Model Based on Multiple CUDA Streams","X. Sun; L. Da; Y. Li","Navy Underwater Battlefield Environ. Instn., Navy Submarine Acad., Qingdao, China; Navy Underwater Battlefield Environ. Instn., Navy Submarine Acad., Qingdao, China; Navy Underwater Battlefield Environ. Instn., Navy Submarine Acad., Qingdao, China","2014 Seventh International Symposium on Computational Intelligence and Design","9 Apr 2015","2014","1","","181","184","In order to improve the computing speed of ocean acoustic field using the Beam-Displacement Ray-Mode (BDRM) theory, a BDRM parallel computing model based on Compute Unified Device Architecture (CUDA) is designed by virtue of the powerful parallel computing ability of GPU and the character of BDRM theory. The emphasis is how to implement parallel computing of eigen value and eigen function in CUDA programming model. The results of simulation experiment show that the CPU elapsed time increases fast but the GPU elapsed time increases slow with the frequency of the sound source reaching higher. The speedup in blue-water is bigger than that in shallow-water under the same frequency of the sound source. The speedups are 7.84× and 33.36× respectively in shallow-water and blue-water when the frequency of the sound source is 1000Hz. The BDRM parallel computing model based on CUDA has higher computing efficiency than the BDRM serial computing model based on CPU under large scale operations. It could achieve the requirement of fast forecast of ocean acoustic field and engineering application.","","978-1-4799-7005-6","10.1109/ISCID.2014.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064168","CUDA;BDRM;eigenvalue;eigenfunction;acoustic field;parallel computing","Graphics processing units;Computational modeling;Parallel processing;Acoustics;Eigenvalues and eigenfunctions;Oceans;Instruction sets","eigenvalues and eigenfunctions;graphics processing units;parallel architectures;parallel programming","BDRM asynchronous parallel computing model;multiple CUDA stream;beam-displacement ray-mode theory;Compute Unified Device Architecture;GPU parallel computing ability;graphics processing unit;CUDA programming;eigenvalue;eigenfunction;CPU elapsed time;GPU elapsed time;sound source frequency;ocean acoustic field;engineering application;BDRM serial computing model","","2","","11","","9 Apr 2015","","","IEEE","IEEE Conferences"
"Numerical Simulation of Transit-Time Ultrasonic Flowmeters by a Direct Approach","A. Luca; R. Marchiano; J. Chassaing","Ultraflux, Éragny, France; Institut Jean Le Rond d’Alembert, 4 place Jussieu, Sorbonne Universités, UPMC Univ Paris 06, CNRS, UMR 7190, Paris, France; Institut Jean Le Rond d’Alembert, 4 place Jussieu, Sorbonne Universités, UPMC Univ Paris 06, CNRS, UMR 7190, Paris, France","IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control","23 May 2016","2016","63","6","886","897","This paper deals with the development of a computational code for the numerical simulation of wave propagation through domains with a complex geometry consisting in both solids and moving fluids. The emphasis is on the numerical simulation of ultrasonic flowmeters (UFMs) by modeling the wave propagation in solids with the equations of linear elasticity (ELE) and in fluids with the linearized Euler equations (LEEs). This approach requires high performance computing because of the high number of degrees of freedom and the long propagation distances. Therefore, the numerical method should be chosen with care. In order to minimize the numerical dissipation which may occur in this kind of configuration, the numerical method employed here is the nodal discontinuous Galerkin (DG) method. Also, this method is well suited for parallel computing. To speed up the code, almost all the computational stages have been implemented to run on graphical processing unit (GPU) by using the compute unified device architecture (CUDA) programming model from NVIDIA. This approach has been validated and then used for the two-dimensional simulation of gas UFMs. The large contrast of acoustic impedance characteristic to gas UFMs makes their simulation a real challenge.","1525-8955","","10.1109/TUFFC.2016.2545714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439838","Numerical simulation;transit-time flowmeters;linear elasticity;Euler equations;discontinuous Galerkin methods;GPU computing;Discontinuous Galerkin methods;Euler equations;graphical processing unit computing;linear elasticity;numerical simulation;transit-time flowmeters","Mathematical model;Acoustics;Solids;Numerical simulation;Propagation;Computational modeling;Transducers","elasticity;flowmeters;Galerkin method;ultrasonic measurement;wave propagation","numerical simulation;transit-time ultrasonic flowmeters;computational code;wave propagation;complex geometry;solids;moving fluids;linear elasticity;linearized Euler equations;nodal discontinuous Galerkin method;graphical processing unit;compute unified device architecture programming model;NVIDIA;acoustic impedance characteristic","Computer Simulation;Image Processing, Computer-Assisted;Rheology;Ultrasonics","21","","51","IEEE","23 Mar 2016","","","IEEE","IEEE Journals"
"GPGPU implementation of fractal image coding","O. Alvarado-Nava; H. M. Chablé Martínez; E. Rodríguez-Martínez","Departamento de Electrónica, Divisi ón de Ciencias Básicas e Ingeniería, Universidad Autónoma Metropolitana, Unidad Azcapotzalco, México D.F., México; Departamento de Electrónica, Divisi ón de Ciencias Básicas e Ingeniería, Universidad Autónoma Metropolitana, Unidad Azcapotzalco, México D.F., México; Departamento de Electrónica, Divisi ón de Ciencias Básicas e Ingeniería, Universidad Autónoma Metropolitana, Unidad Azcapotzalco, México D.F., México","3rd IEEE International Work-Conference on Bioinspired Intelligence","2 Oct 2014","2014","","","106","110","The programming model of general propose computing on graphic processing units (GPGPU) offers great efficiency for applications acceleration. This feature is granted by the ability of partitioning a sequential application into smaller subproblems with high computing requirements; those subproblems can be executed in parallel by a graphics processing unit (GPU) and partial results can be transferred to main memory where the central processing unit (CPU) collects and presents them. On the other hand, Fractal Image Coding (FIC) is a lossy compression technique with promising features, however it has been relegated due to its large coding time. The present article propose a parallel implementation of FIC on a GPGPU system which achieves an acceleration on coding time of about 129 times.","","978-1-4799-6174-0","10.1109/IWOBI.2014.6913947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913947","Fractal Image Compression;GPGPU;GPU;Parallel Computing","Graphics processing units;Image coding;Instruction sets;Fractals;Image resolution;Central Processing Unit;Acceleration","fractals;graphics processing units;image coding","GPGPU implementation;fractal image coding;programming model;applications acceleration;partitioning;sequential application;high computing requirements;central processing unit;CPU;FIC;lossy compression technique;parallel implementation;graphic processing units","","2","","15","","2 Oct 2014","","","IEEE","IEEE Conferences"
"Self-Adaptive OmpSs Tasks in Heterogeneous Environments","J. Planas; R. M. Badia; E. Ayguadé; J. Labarta","Barcelona Supercomput. Center, Univ. Politec. de Catalunya, Barcelona, Spain; Artificial Intell. Res. Inst. (IIIA), Barcelona Supercomput. Center, Barcelona, Spain; Barcelona Supercomput. Center, Univ. Politec. de Catalunya, Barcelona, Spain; Barcelona Supercomput. Center, Univ. Politec. de Catalunya, Barcelona, Spain","2013 IEEE 27th International Symposium on Parallel and Distributed Processing","29 Jul 2013","2013","","","138","149","As new heterogeneous systems and hardware accelerators appear, high performance computers can reach a higher level of computational power. Nevertheless, this does not come for free: the more heterogeneity the system presents, the more complex becomes the programming task in terms of resource management. OmpSs is a task-based programming model and framework focused on the runtime exploitation of parallelism from annotated sequential applications. This paper presents a set of extensions to this framework: we show how the application programmer can expose different specialized versions of tasks (i.e. pieces of specific code targeted and optimized for a particular architecture) and how the system can choose between these versions at runtime to obtain the best performance achievable for the given application. From the results obtained in a multi-GPU system, we prove that our proposal gives flexibility to application's source code and can potentially increase application's performance.","1530-2075","978-1-4673-6066-1","10.1109/IPDPS.2013.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569807","multi-gpu management;heterogeneous architectures;parallel programming models;scheduling techniques","Runtime;Graphics processing units;Programming;Computer architecture;Reliability;Proposals;Kernel","graphics processing units;parallel programming;resource allocation;scheduling;source coding","self-adaptive OmpSs tasks;heterogeneous environments;heterogeneous systems;hardware accelerators;high performance computers;task-based programming model;resource management;computational power;runtime parallelism exploitation;sequential applications;application programmer;multiGPU system;application source code;application performance","","34","","25","","29 Jul 2013","","","IEEE","IEEE Conferences"
"Design and Performance Evaluation of Image Processing Algorithms on GPUs","I. K. Park; N. Singhal; M. H. Lee; S. Cho; C. Kim","Inha University, Incheon; Samsung Electronics Co., Ltd., Suwon; Inha University, Incheon; Samsung Electronics Co., Ltd., Suwon; NVIDIA Corporation, Seoul","IEEE Transactions on Parallel and Distributed Systems","29 Nov 2010","2011","22","1","91","104","We construe key factors in design and evaluation of image processing algorithms on the massive parallel graphics processing units (GPUs) using the compute unified device architecture (CUDA) programming model. A set of metrics, customized for image processing, is proposed to quantitatively evaluate algorithm characteristics. In addition, we show that a range of image processing algorithms map readily to CUDA using multiview stereo matching, linear feature extraction, JPEG2000 image encoding, and nonphotorealistic rendering (NPR) as our example applications. The algorithms are carefully selected from major domains of image processing, so they inherently contain a variety of subalgorithms with diverse characteristics when implemented on the GPU. Performance is evaluated in terms of execution time and is compared to the fastest host-only version implemented using OpenMP. It is shown that the observed speedup varies extensively depending on the characteristics of each algorithm. Intensive analysis is conducted to show the appropriateness of the proposed metrics in predicting the effectiveness of an application for parallel implementation.","1558-2183","","10.1109/TPDS.2010.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477417","GPU;CUDA;image processing;parallel implementation;GPGPU.","Algorithm design and analysis;Image processing;Concurrent computing;Computer architecture;Graphics processing unit;Parallel programming;Computer vision;Scattering;Parallel processing","computer graphic equipment;coprocessors;feature extraction;image coding;performance evaluation","image processing algorithms;massive parallel graphics processing units;compute unified device architecture programming model;performance evaluation;multiview stereo matching;linear feature extraction;JPEG2000 image encoding;nonphotorealistic rendering;OpenMP;execution time","","90","2","38","IEEE","3 Jun 2010","","","IEEE","IEEE Journals"
"Software Pipelined Execution of Stream Programs on GPUs","A. Udupa; R. Govindarajan; M. J. Thazhuthaveetil","Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bangalore; Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bangalore; Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bangalore","2009 International Symposium on Code Generation and Optimization","5 May 2009","2009","","","200","209","The StreamIt programming model has been proposed to exploit parallelism in streaming applications on general purpose multi-core architectures. This model allows programmers to specify the structure of a program as a set of filters that act upon data, and a set of communication channels between them. The StreamIt graphs describe task, data and pipeline parallelism which can be exploited on modern graphics processing units (GPUs), as they support abundant parallelism in hardware. In this paper, we describe the challenges in mapping StreamIt to GPUs and propose an efficient technique to software pipeline the execution of stream programs on GPUs. We formulate this problem - both scheduling and assignment of filters to processors - as an efficient integer linear program (ILP), which is then solved using ILP solvers. We also describe a novel buffer layout technique for GPUs which facilitates exploiting the high memory bandwidth available in GPUs. The proposed scheduling utilizes both the scalar units in GPU, to exploit data parallelism, and multiprocessors, to exploit task and pipeline parallelism. Further it takes into consideration the synchronization and bandwidth limitations of GPUs, and yields speedups between 1.87X and 36.83X over a single threaded CPU.","","978-0-7695-3576-0","10.1109/CGO.2009.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907664","CUDA;GPU Programming;Software Pipelining;Stream Programming","Pipelines;Filters;Processor scheduling;Bandwidth;Parallel programming;Application software;Computer architecture;Programming profession;Communication channels;Graphics","linear programming;parallel programming;pipeline processing","software pipelined execution;stream programs;StreamIt programming model;multi-core architectures;StreamIt graphs;graphics processing units;integer linear program;high memory bandwidth","","60","2","23","","5 May 2009","","","IEEE","IEEE Conferences"
"Exploring Memory Persistency Models for GPUs","Z. Lin; M. Alshboul; Y. Solihin; H. Zhou",North Carolina State University; North Carolina State University; University of Central Florida; North Carolina State University,"2019 28th International Conference on Parallel Architectures and Compilation Techniques (PACT)","7 Nov 2019","2019","","","311","323","Given its high integration density, high speed, byte addressability, and low standby power, non-volatile or persistent memory is expected to supplement/replace DRAM as main memory. Through persistency programming model (which defines durability ordering of stores) and durable transaction constructs, the programmer can provide recoverable data structure (RDS) which allows programs to recover to a consistent state after a failure. While persistency models have been well studied for CPUs, they have been neglected for graphics processing units (GPUs). Considering the importance of GPUs as a dominant accelerator for high performance computing, we investigate persistency models for GPUs. GPU applications exhibit substantial differences with CPUs applications, hence in this paper we adapt, re-architect, and optimize CPU persistency models for GPUs. We design a pragma-based compiler scheme for expressing persistency model for GPUs. We identify that the thread hierarchy in GPUs offers intuitive scopes to form epochs and durable transactions. We find that undo logging produces significant performance overheads. We propose to use idempotency analysis to reduce both logging frequency and the size of logs. Through both real-system and simulation evaluations, we show low overheads of our proposed architecture support.","2641-7936","978-1-7281-3613-4","10.1109/PACT.2019.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8891613","GPU;Memory Persistency","Graphics processing units;Instruction sets;Adaptation models;Nonvolatile memory;Kernel;Bandwidth;Random access memory","data structures;DRAM chips;graphics processing units;microprocessor chips;parallel processing;program compilers","memory persistency models;GPUs;high integration density;low standby power;persistent memory;persistency programming model;persistency model;high performance computing;CPU persistency models;durable transactions;byte addressability;nonvolatile memory;recoverable data structure;graphics processing units;DRAM;pragma-based compiler scheme","","4","","28","","7 Nov 2019","","","IEEE","IEEE Conferences"
"HadoopCL2: Motivating the Design of a Distributed, Heterogeneous Programming System With Machine-Learning Applications","M. Grossman; M. Breternitz; V. Sarkar","Department of Computer Science, 6100 Main St., Rice University, Houston, TX; AMD Research, 7171 Southwest Parkway, Austin, TX; Department of Computer Science, 6100 Main St., Rice University, Houston, TX","IEEE Transactions on Parallel and Distributed Systems","11 Feb 2016","2016","27","3","762","775","Machine learning (ML) algorithms have garnered increased interest as they demonstrate improved ability to extract meaningful trends from large, diverse, and noisy data sets. While research is advancing the state-of-the-art in ML algorithms, it is difficult to drastically improve the real-world performance of these algorithms. Porting new and existing algorithms from single-node systems to multi-node clusters, or from architecturally homogeneous systems to heterogeneous systems, is a promising optimization technique. However, performing optimized ports is challenging for domain experts who may lack experience in distributed and heterogeneous software development. This work explores how challenges in ML application development on heterogeneous, distributed systems shaped the development of the HadoopCL2 (HCL2) programming system. ML applications guide this work because they exhibit features that make application development difficult: large & diverse datasets, complex algorithms, and the need for domain-specific knowledge. The goal of this work is a general, MapReduce programming system that outperforms existing programming systems. This work evaluates the performance and portability of HCL2 against five ML applications from the Mahout ML framework on two hardware platforms. HCL2 demonstrates speedups of greater than 20x relative to Mahout for three computationally heavy algorithms and maintains minor performance improvements for two I/O bound algorithms.","1558-2183","","10.1109/TPDS.2015.2414943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064791","MapReduce;heterogeneous;distributed;programming model;GPU;auto-scheduling","Programming;Performance evaluation;Vectors;Java;Kernel;Object oriented modeling;Computational modeling","data handling;distributed programming;learning (artificial intelligence);optimisation;parallel processing;software engineering","distributed programming system;heterogeneous programming system;machine-learning applications;optimization technique;distributed software development;heterogeneous software development;ML application development;HadoopCL2 programming system;HCL2 programming system;domain-specific knowledge;MapReduce programming system;Mahout ML framework;I/O bound algorithms","","6","","16","IEEE","20 Mar 2015","","","IEEE","IEEE Journals"
"Running High Performance Linpack on CPUGPU clusters","D. Tomić; D. Ogrizović","Hewlett-Packard Croatia, Zagreb, Croatia; Center for advanced computing and modeling / Faculty of Maritime Studies, Rijeka, Croatia","2012 Proceedings of the 35th International Convention MIPRO","16 Jul 2012","2012","","","400","404","A trend is developing in High-Performance Computing with cluster nodes built of general purpose CPUs and GPU accelerators. The common name of these systems is CPUGPU clusters. High Performance Linpack (HPL) benchmarking of High Performance Clusters consisting of nodes with both CPUs and GPUs is still a challenging task and deserves a high attention. In order to make HPL on such clusters more efficient, a multi-layered programming model consisting of at least Message Passing Interface (MPI), Multiprocessing (MP) and Streams Programming (Streams) needs to be utilized. Besides multi-layered programming model, it is crucial to deploy a right load-balancing scheme if someone wants to run HPL efficiently on CPUGPU systems. That means, besides the highest possible utilization rate, both fast and slow processors needs to receive appropriate portion of load, in order to avoid faster resources waiting on slower to finish their jobs. Moreover, in HPC clusters on Cloud, one has to take into account not only computing nodes of different processing power, but also a communication links of different speed between nodes as well. For this reasons we propose a load balancing method based on a semidefinite optimization. We hope that this method, coupled with a multi-layered programming, can perform a HPL benchmark on CPUGPU clusters and HPC Cloud systems more efficiently than methods used today.","","978-953-233-068-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240677","","Benchmark testing;Graphics processing unit;Programming;Symmetric matrices;Load management;Optimization;Clustering algorithms","application program interfaces;benchmark testing;graphics processing units;message passing;multiprocessing systems;parallel programming;performance evaluation;resource allocation","high performance Linpack;CPUGPU clusters;high-performance computing;general purpose CPU;GPU accelerators;HPL benchmarking;multilayered programming model;least message passing interface;multiprocessing;stream programming;load-balancing scheme;processing power;load balancing method;semidefinite optimization","","2","","10","","16 Jul 2012","","","IEEE","IEEE Conferences"
"OpenMP 4.5 compiler optimization for GPU offloading","E. Tiotto; B. Mahjour; W. Tsang; X. Xue; T. Islam; W. Chen",NA; NA; NA; NA; NA; NA,"IBM Journal of Research and Development","15 May 2020","2020","64","3/4","14:1","14:11","Ability to efficiently offload computational workloads to graphic processing units (GPUs) is critical for the success of hybrid CPU–GPU architectures, such as the Summit and Sierra supercomputing systems. OpenMP 4.5 is a high-level programming model that enables the development of architecture- and accelerator-independent applications. This article describes aspects of the OpenMP implementation in the IBM XL C/C++ and XL Fortran OpenMP compilers that aid programmers to achieve performance objectives. This includes an interprocedural static analysis the XL optimizer uses to specialize code generation of the OpenMP <italic>distribute parallel do</italic> loop within the dynamic context of a target region, and other compiler optimizations designed to reduce the overhead of data transferred to an offloaded target region. We introduce the heuristic used at runtime to select optimal grid sizes for offloaded target team constructs. These tuned heuristics lead to an average improvement of 2× in the runtime of several target regions in the SPEC ACCEL V1.2 benchmark suite. In addition to performance enhancement, this article also presents an advanced diagnostic feature implemented in the XL Fortran compiler to aid in debugging OpenMP applications offloaded to accelerators.","0018-8646","","10.1147/JRD.2019.2962428","CORAL; U.S. Department of Energy(grant numbers:B604142); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943339","","Graphics processing units;Runtime;Programming;Context;Computer architecture;Optimization","","","","","","11","IBM","25 Dec 2019","","","IBM","IBM Journals"
"A Runtime Library for Platform-Independent Task Parallelism","P. E. Hadjidoukas; E. Lappas; V. V. Dimakopoulos","Dept. of Comput. Sci., Univ. of Ioannina, Ioannina, Greece; Dept. of Comput. Sci., Univ. of Ioannina, Ioannina, Greece; Dept. of Comput. Sci., Univ. of Ioannina, Ioannina, Greece","2012 20th Euromicro International Conference on Parallel, Distributed and Network-based Processing","15 Mar 2012","2012","","","229","236","With the increasing diversity of computing systems and the rapid performance improvement of commodity hardware, heterogeneous clusters become the dominant platform for low-cost, high-performance computing. Grid-enabled and heterogeneous implementations of MPI establish it as the de facto programming model for these environments. On the other hand, task parallelism provides a natural way for exploiting their hierarchical architecture. This hierarchy has been further extended with the advent of general-purpose GPU devices. In this paper we present the implementation of an MPI-based task library for heterogeneous and GPU clusters. The library offers an intuitive programming interface for multilevel task parallelism with transparent data management and load balancing. We discuss design and implementation issues regarding heterogeneity support and report performance results on heterogeneous cluster computing environments.","2377-5750","978-1-4673-0226-5","10.1109/PDP.2012.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169554","task parallelism;heterogeneous computing;runtime support;message passing","Graphics processing unit;Kernel;Programming;Parallel processing;Libraries;Central Processing Unit;Computer architecture","application program interfaces;data handling;graphics processing units;message passing;parallel processing;resource allocation","runtime library;platform-independent task parallelism;computing system;commodity hardware;heterogeneous cluster computing environment;high-performance computing;Grid-enabled MPI implementation;heterogeneous MPI implementation;message passing interface;de facto programming model;general-purpose GPU device;graphics processing unit;intuitive programming interface;transparent data management;load balancing;MPI-based task library;multilevel task parallelism","","6","","22","","15 Mar 2012","","","IEEE","IEEE Conferences"
"Parallel Computing Experiences with CUDA","M. Garland; S. Le Grand; J. Nickolls; J. Anderson; J. Hardwick; S. Morton; E. Phillips; Y. Zhang; V. Volkov","NVIDIA; NVIDIA; NVIDIA; Iowa State University and Ames Laboratory; TechniScan Medical Systems; Hess; University of California, Davis; University of California, Davis; University of California, Berkeley","IEEE Micro","19 Sep 2008","2008","28","4","13","27","The CUDA programming model provides a straightforward means of describing inherently parallel computations, and NVIDIA's Tesla GPU architecture delivers high computational throughput on massively parallel problems. This article surveys experiences gained in applying CUDA to a diverse set of problems and the parallel speedups over sequential codes running on traditional CPU architectures attained by executing key computations on the GPU.","1937-4143","","10.1109/MM.2008.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626815","","Parallel processing;Programming profession;Parallel programming;Concurrent computing;Computer architecture;Computer graphics;Kernel;Throughput;Central Processing Unit","coprocessors;parallel processing","parallel computing;CUDA programming model;NVIDIA;Tesla GPU architecture;sequential codes","","291","10","22","IEEE","19 Sep 2008","","","IEEE","IEEE Magazines"
"An Initial Assessment of NVSHMEM for High Performance Computing","C. Hsu; N. Imam; A. Langer; S. Potluri; C. J. Newburn","Oak Ridge National Laboratory Oak Ridge TN,Computing & Computational Sciences,USA; Oak Ridge National Laboratory Oak Ridge TN,Computing & Computational Sciences,USA; NVIDIA Corporation,Compute Software,Santa Clara, CA,USA; NVIDIA Corporation,Compute Software,Santa Clara, CA,USA; NVIDIA Corporation,Compute Software,Santa Clara, CA,USA","2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","28 Jul 2020","2020","","","1","10","High Performance Computing has been a driving force behind important tasks such as scientific discovery and deep learning. It tends to achieve performance through greater concurrency and heterogeneity, where the underlying complexity of richer topologies is managed through software abstraction.In this paper, we present our initial assessment of NVSHMEM, an experimental programming library that supports the Partitioned Global Address Space programming model for NVIDIA GPU clusters. NVSHMEM offers several concrete advantages. One is that it reduces overheads and software complexity by allowing communication and computation to be interleaved vs. separating them into different phases. Another is that it implements the OpenSHMEM specification to provide efficient finegrained one-sided communication, streamlining away overheads due to tag matching, wildcards, and unexpected messages which have compounding effect with increasing concurrency. It also offers ease of use by abstracting away low-level configuration operations that are required to enable low-overhead communication and direct loads and stores across processes.We evaluated NVSHMEM in terms of usability, functionality, and scalability by running two math kernels, matrix multiplication and Jacobi solver, on the 27,648-GPU Summit supercomputer. Our exercise of NVSHMEM at scale contributed to making NVSHMEM more robust and preparing it for production release.","","978-1-7281-7445-7","10.1109/IPDPSW50202.2020.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150438","high Performance Computing (HPC);CUDA;openSHMEM;scalability","Graphics processing units;Programming;Kernel;Message systems;Jacobian matrices;Electronics packaging","application program interfaces;graphics processing units;matrix multiplication;message passing;parallel programming;software libraries;software performance evaluation","NVIDIA GPU clusters;Partitioned Global Address Space programming model;experimental programming library;software abstraction;deep learning;scientific discovery;driving force;high Performance Computing;low-overhead communication;one-sided communication;software complexity;NVSHMEM","","1","","8","","28 Jul 2020","","","IEEE","IEEE Conferences"
"Accelerating a C++ CFD Code with OpenACC","J. Kraus; M. Schlottke; A. Adinetz; D. Pleiter","NVIDIA GmbH, Wurselen, Germany; RWTH Aachen Univ., Aachen, Germany; Forschungszentrum Jυlich, Julich, Germany; Forschungszentrum Jυlich, Julich, Germany","2014 First Workshop on Accelerator Programming using Directives","9 Apr 2015","2014","","","47","54","Todays HPC systems are increasingly utilizing accelerators to lower time to solution for their users and reduce power consumption. To utilize the higher performance and energy efficiency of these accelerators, application developers need to rewrite at least parts of their codes. Taking the C++ flow solver ZFS as an example, we show that the directive-based programming model allows one to achieve good performance with reasonable effort, even for mature codes with many lines of code. Using OpenACC directives permitted us to incrementally accelerate ZFS, focusing on the parts of the program that are relevant for the problem at hand. The two new OpenACC 2.0 features, unstructured data regions and atomics, are required for this. OpenACC's interoperability with existing GPU libraries via the host_data use_device construct allowed to use CUDAaware MPI to achieve multi-GPU scalability comparable to the CPU version of ZFS. Like many other codes, the data structures of ZFS have been designed with traditional CPUs and their relatively large private caches in mind. This leads to suboptimal memory access patterns on accelerators, such as GPUs. We show how the texture cache on NVIDIA GPUs can be used to minimize the performance impact of these suboptimal patterns without writing platform specific code. For the kernel most affected by the memory access pattern, we compare the initial array of structures memory layout with a structure of arrays layout.","","978-1-4673-6753-0","10.1109/WACCPD.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081677","","Graphics processing units;Acceleration;Computer architecture;Microprocessors;Programming;Kernel","application program interfaces;C++ language;computational fluid dynamics;graphics processing units;message passing;open systems;parallel architectures","C++ flow solver;CFD code;computational fluid dynamics;OpenACC interoperability;directive-based programming model;CUDA aware MPI;NVIDIA GPU","","16","","21","","9 Apr 2015","","","IEEE","IEEE Conferences"
"Accelerating bootstrapping in FHEW using GPUs","M. S. Lee; Y. Lee; J. H. Cheon; Y. Paek","Dept. of Mathematical Sciences, Seoul National University, Korea; Dept. of Electrical and Computer Engineering and Inter-University Semiconductor Research Center (ISRC), Seoul National University, Korea; Dept. of Mathematical Sciences, Seoul National University, Korea; Dept. of Electrical and Computer Engineering and Inter-University Semiconductor Research Center (ISRC), Seoul National University, Korea","2015 IEEE 26th International Conference on Application-specific Systems, Architectures and Processors (ASAP)","10 Sep 2015","2015","","","128","135","Recently, the usage of GPU is not limited to the jobs associated with graphics and a wide variety of applications take advantage of the flexibility of GPUs to accelerate the computing performance. Among them, one of the most emerging applications is the fully homomorphic encryption (FHE) scheme, which enables arbitrary computations on encrypted data. Despite much research effort, it cannot be considered as practical due to the enormous amount of computations, especially in the bootstrapping procedure. In this paper, we accelerate the performance of the recently suggested fast bootstrapping method in FHEW scheme using GPUs, as a case study of a FHE scheme. In order to optimize, we explored the reference code and carried out profiling to find out candidates for performance acceleration. Based on the profiling results, combined with more flexible tradeoff method, we optimized the bootstrapping algorithm in FHEW using GPU and CUDA's programming model. The empirical result shows that the bootstrapping of FHEW ciphertext can be done in less than 0.11 second after optimization.","2160-052X","978-1-4799-1925-3","10.1109/ASAP.2015.7245720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7245720","","Graphics processing units;Encryption;Acceleration;Libraries;Noise;Polynomials","cryptography;graphics processing units;parallel programming","bootstrapping algorithm;FHEW scheme;fully homomorphic encryption scheme;GPU;graphics processing unit;performance acceleration;fast bootstrapping method;CUDA programming model;Compute Unified Device Architecture;FHEW ciphertext bootstrapping","","7","","26","","10 Sep 2015","","","IEEE","IEEE Conferences"
