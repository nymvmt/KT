"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Can GPGPU Programming Be Liberated from the Data-Parallel Bottleneck?","B. R. Gaster; L. Howes",Advanced Micro Devices; Advanced Micro Devices,"Computer","17 Aug 2012","2012","45","8","42","52","With the growth in transistor counts in modern hardware, heterogeneous systems are becoming commonplace. Core counts are increasing such that GPU and CPU designs are reaching deep into the tens of cores. For performance reasons, different cores in a heterogeneous platform follow different design choices. Based on throughput computing goals, GPU cores tend to support wide vectors and substantial register files. Current designs optimize CPU cores for latency, dedicating logic to caches and out-of-order dependence control. Heterogeneous parallel primitives (HPP) addresses two major shortcomings in current GPGPU programming models: it supports full composability by defining abstractions and increases flexibility in execution by introducing braided parallelism. Heterogeneous parallel primitives is an object-oriented, C++11-based programming model that addresses these shortcomings on both CPUs and massively multithreaded GPUs: it supports full composability by defining abstractions using distributed arrays and barrier objects, and it increases flexibility in execution by introducing braided parallelism. This paper implemented a feature-complete version of HPP, including all syntactic constructs, that runs on top of a task-parallel runtime executing on the CPU. They continue to develop and improve the model, including reducing overhead due to channel management, and plan to make a public version available sometime in the future.","1558-0814","","10.1109/MC.2012.257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272260","massively threaded computing systems;heterogeneous parallel primitives;braided parallelism;persistent threading;GPGPU programming;data-parallel execution;distributed arrays;hardware","Graphics processing unit;Programming;Performance evaluation;Parellel processing;Indexes;Hardware;Multithreading","C++ language;graphics processing units;integrated circuit design;multi-threading;object-oriented programming","GPGPU programming;data-parallel bottleneck;heterogeneous systems;core counts;GPU designs;CPU designs;heterogeneous parallel primitives;general-purpose computing-on-graphics processing units;object-oriented C++11-based programming model;distributed arrays;barrier objects;task-parallel runtime;channel management;braided parallelism","","24","2","16","","17 Aug 2012","","","IEEE","IEEE Magazines"
"CUDAsmith: A Fuzzer for CUDA Compilers","B. Jiang; X. Wang; W. K. Chan; T. H. Tse; N. Li; Y. Yin; Z. Zhang","Beihang University, China; Beihang University, China; City University of Hong Kong; The University of Hong Kong, Hong Kong; CAST; Beihang University, China; Chinese Academy of Sciences, China","2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)","22 Sep 2020","2020","","","861","871","CUDA is a parallel computing platform and programming model for the graphics processing unit (GPU) of NVIDIA. With CUDA programming, general purpose computing on GPU (GPGPU) is possible. However, the correctness of CUDA programs relies on the correctness of CUDA compilers, which is difficult to test due to its complexity. In this work, we propose CUDAsmith, a fuzzing framework for CUDA compilers. Our tool can randomly generate deterministic and valid CUDA kernel code with several different strategies. Moreover, it adopts random differential testing and EMI testing techniques to solve the test oracle problems of CUDA compiler testing. In particular, we lift live code injection to CUDA compiler testing to help generate EMI variants. Our fuzzing experiments with both the NVCC compiler and the Clang compiler for CUDA have detected thousands of failures, some of which have been confirmed by compiler developers. Finally, the cost-effectiveness of CUDAsmith is also thoroughly evaluated in our fuzzing experiment.","0730-3157","978-1-7281-7303-0","10.1109/COMPSAC48688.2020.0-156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202798","Compiler, compute unified device architecture (CUDA), differential testing, equivalence modulo inputs (EMI) testing, fuzzing, general purpose computing on graphics processing unit (GPGPU)","Graphics processing units;Kernel;Electromagnetic interference;Fuzzing;Tools;Computational modeling","computational complexity;graphics processing units;program compilers;program testing","CUDA programming;general purpose computing;GPU;CUDA programs;CUDA compilers;CUDAsmith;deterministic CUDA kernel code;random differential testing;test oracle problems;CUDA compiler testing;NVCC compiler;Clang compiler;compiler developers;parallel computing platform;programming model","","1","","29","","22 Sep 2020","","","IEEE","IEEE Conferences"
"Task-based parallel breadth-first search in heterogeneous environments","L. Munguía; D. A. Bader; E. Ayguade","Barcelona School of Informatics, Universitat Politècnica de Catalunya, Barcelona, Spain; College of Computing Georgia Institute of Technology, Atlanta GA 30332; Barcelona Supercomputing Center (BSC), Spain","2012 19th International Conference on High Performance Computing","25 Apr 2013","2012","","","1","10","Breadth-first search (BFS) is an essential graph traversal strategy widely used in many computing applications. Because of its irregular data access patterns, BFS has become a non-trivial problem hard to parallelize efficiently. In this paper, we introduce a parallelization strategy that allows the load balancing of computation resources as well as the execution of graph traversals in hybrid environments composed of CPUs and GPUs. To achieve that goal, we use a fine-grained task-based parallelization scheme and the OmpSs programming model. We obtain processing rates up to 2.8 billion traversed edges per second with a single GPU and a multi-core processor. Our study shows high processing rates are achievable with hybrid environments despite the GPU communication latency and memory coherence.","","978-1-4673-2371-0","10.1109/HiPC.2012.6507474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507474","","","graph theory;graphics processing units;multiprocessing systems;parallel processing;resource allocation;tree searching","task-based parallel breadth-first search;heterogeneous environment;BFS;graph traversal strategy;irregular data access pattern;parallelization strategy;load balancing;computation resource;hybrid environment;CPU;fine-grained task-based parallelization scheme;OmpSs programming model;processing rate;multicore processor;GPU communication latency;memory coherence","","16","1","15","","25 Apr 2013","","","IEEE","IEEE Conferences"
"SOLAR: Services-Oriented Learning Architectures","C. Wang; X. Li; Q. Yu; A. Wang; P. Hung; X. Zhou","Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China","2016 IEEE International Conference on Web Services (ICWS)","1 Sep 2016","2016","","","662","665","Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data sizes have posed significant challenge to construct a flexible and high efficient implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA based approaches. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability.","","978-1-5090-2675-3","10.1109/ICWS.2016.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558064","Services-oriented Architecture;Deep Learning;Neural Network;Accelerator","Hardware;Machine learning;Field programmable gate arrays;Computer architecture;Graphics processing units;Service-oriented architecture","field programmable gate arrays;graphics processing units;learning (artificial intelligence);multiprocessing systems;neural nets;service-oriented architecture","SOLAR;machine learning;deep learning neural networks;services-oriented deep learning architecture;GPU;FPGA based approaches;uniform programming model;ubiquitous framework;Intel i5 processors","","7","","9","","1 Sep 2016","","","IEEE","IEEE Conferences"
"Massively Parallel Network Coding on GPUs","X. Chu; K. Zhao; M. Wang","Department of Computer Science, Hong Kong Baptist University, Hong Kong, P.R.C, chxw@comp.hkbu.edu.hk; Department of Computer Science, Hong Kong Baptist University, Hong Kong, P.R.C, kyzhao@comp.hkbu.edu.hk; Department of Computer Science, University of Calgary, Alberta, Canada, meawang@ucalgary.ca","2008 IEEE International Performance, Computing and Communications Conference","9 Jan 2009","2008","","","144","151","Network coding has recently been widely applied in various networks for system throughput improvement and/or resilience to network dynamics. However, the computational overhead introduced by the network coding operations is not negligible and has become the cornerstone for real deployment of network coding. In this paper, we exploit the computing power of contemporary Graphic Processing Units (GPUs) to accelerate the network coding operations. We proposed three parallel algorithms that maximize the parallelism of the encoding and decoding processes, i.e., the power of GPUs is fully utilized. This paper also shares our optimization design choices and our workarounds to the challenges encountered in working with GPUs. With our implementation of the algorithms, we are able to achieve up to 12 times of speedup over the highly optimized CPU counterpart, using the NVIDIA GPU and the Computer Unified Device Architecture (CUDA) programming model.","2374-9628","978-1-4244-3368-1","10.1109/PCCC.2008.4745113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4745113","Network coding;GPU computing;CUDA","Network coding;Computer networks;Throughput;Resilience;Graphics;Acceleration;Parallel algorithms;Parallel processing;Encoding;Decoding","microprocessor chips;parallel algorithms","massively parallel network coding;system throughput improvement;graphic processing units;parallel algorithms;NVIDIA GPU;computer unified device architecture programming","","22","","21","","9 Jan 2009","","","IEEE","IEEE Conferences"
"The Intel® Many Integrated Core Architecture","A. Duran; M. Klemm",Intel Corporation; Intel Corporation,"2012 International Conference on High Performance Computing & Simulation (HPCS)","16 Aug 2012","2012","","","365","366","In recent years, an observable trend in High Performance Computing (HPC) architectures has been the inclusion of accelerators, such as GPUs and field programmable arrays (FPGAs), to improve the performance of scientific applications. To rise to this challenge Intel announced the Intel<sup>®</sup> Many Integrated Core Architecture (Intel<sup>®</sup> MIC Architecture). In contrast with other accelerated platforms, the Intel MIC Architecture is a general purpose, manycore coprocessor that improves the programmability of such devices by supporting the well-known shared-memory execution model that is the base of most nodes in HPC machines. In this presentation, we will introduce key properties of the Intel MIC Architecture and we will also cover programming models for parallelization and vectorization of applications targeting this architecture.","","978-1-4673-2362-8","10.1109/HPCSim.2012.6266938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266938","","Coprocessors;Computer architecture;Microwave integrated circuits;Programming;Hardware;Program processors;Syntactics","field programmable gate arrays;graphics processing units;parallel architectures;shared memory systems","Intel many integrated core architecture;high performance computing;accelerators;GPU;field programmable arrays;Intel MIC architecture;many core coprocessor;shared-memory execution model;programming model;parallelization;vectorization","","48","1","2","","16 Aug 2012","","","IEEE","IEEE Conferences"
"Monte Carlo simulation of X-ray imaging using a graphics processing unit","A. Badal; A. Badano","U.S. Food and Drug Administration, Division of Imaging and Applied Mathematics, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, Silver Spring, MD 20993-0002 USA; U.S. Food and Drug Administration, Division of Imaging and Applied Mathematics, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, Silver Spring, MD 20993-0002 USA","2009 IEEE Nuclear Science Symposium Conference Record (NSS/MIC)","29 Jan 2010","2009","","","4081","4084","A code for Monte Carlo simulations of radiation transport using a Graphics Processing Unit (GPU) is introduced. The code has been developed using the CUDA¿ programming model, an extension to the C language that allows the execution of general purpose computations on the new generation of GPUs from NVIDIA. The accurate Compton and Rayleigh interaction models and interaction mean free paths from the PENELOPE package, and a generic voxelized geometry model, have been implemented in the new code. The secondary particles generated by Compton, photoelectric and pair-production events are not transported. An ideal x-ray detector and a cone beam source can be defined to reproduce an imaging system and facilitate the simulations of medical imaging applications. A 24-fold speed up factor with the GPU compared to the CPU is reported for a radiographic projection of a detailed anthropomorphic female phantom. A description of the simulation algorithm and the technical implementation in the GPU are provided. This work shows that GPUs are already a good alternative to CPUs for Monte Carlo simulation of x-ray transport.","1082-3654","978-1-4244-3961-4","10.1109/NSSMIC.2009.5402382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5402382","","X-ray imaging;Graphics;Solid modeling;Medical simulation;Packaging;Geometry;X-ray detectors;Optical imaging;Biomedical imaging;Central Processing Unit","biomedical imaging;Monte Carlo methods;radiography;X-ray detection;X-ray imaging","Monte Carlo simulation;X-ray imaging;graphics processing unit;radiation transport;CUDA¿ programming model;general purpose computations;Compton interaction model;Rayleigh interaction model;PENELOPE package;generic voxelized geometry model;X-ray detector;cone beam source;imaging system;medical imaging applications;radiographic projection;anthropomorphic female phantom;simulation algorithm;X-ray transport","","13","","12","","29 Jan 2010","","","IEEE","IEEE Conferences"
"Improving 3D lattice boltzmann method stencil with asynchronous transfers on many-core processors","M. Q. Ho; C. Obrecht; B. Tourancheau; B. D. de Dinechin; J. Hascoet","CNRS, LIG UMR 5217, Grenoble Alps University, F-38058 Grenoble, France; Univ Lyon, CNRS, INSA-Lyon, Université Claude Bernard Lyon 1, CETHIL UMR5008, F-69621 Villeurbanne, France; CNRS, LIG UMR 5217, Grenoble Alps University, F-38058 Grenoble, France; Kalray S.A., F-38330 Montbonnot, France; Kalray S.A., F-38330 Montbonnot, France","2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)","5 Feb 2018","2017","","","1","9","CPU-based many-core processors present an alternative to multicore CPU and GPU processors. In particular, the 93-Petaflops Sunway supercomputer, built from clustered many-core processors, has opened a new era for high performance computing that does not rely on GPU acceleration. However, memory bandwidth remains the main challenge for these architectures. This motivates our endeavor for optimizing one of the most data-intensive kind of stencil computations, namely the three-dimensional applications of the lattice Boltzmann method (LBM). We propose optimizations on many-cores processors by using local memory and asynchronous software-prefetching on a representative 3D LBM solver as an example. We achieve 33 % performance gain on the Kalray MPPA-256 many-core processor by actively streaming data from/to local memory, compared to the “passive” OpenCL programming model.","2374-9628","978-1-5090-6468-7","10.1109/PCCC.2017.8280472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280472","","Three-dimensional displays;Program processors;Lattices;Computer architecture;Bandwidth;Computational modeling;Kernel","graphics processing units;lattice Boltzmann methods;multiprocessing systems;parallel processing","3D lattice Boltzmann method stencil;many-core processor cluster;LBM;asynchronous software-prefetching;representative 3D LBM solver;data streaming;OpenCL programming;Kalray MPPA-256 many-core processor;local memory;many-cores processors;stencil computations;GPU acceleration;high performance computing;93-Petaflops Sunway supercomputer;GPU processors;CPU;asynchronous transfers","","","","15","","5 Feb 2018","","","IEEE","IEEE Conferences"
"Automating CUDA Synchronization via Program Transformation","M. Wu; L. Zhang; C. Liu; S. H. Tan; Y. Zhang",Southern University of Science and Technology; University of Texas at Dallas; University of Texas at Dallas; Southern University of Science and Technology; Southern University of Science and Technology,"2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)","9 Jan 2020","2019","","","748","759","While CUDA has been the most popular parallel computing platform and programming model for general purpose GPU computing, CUDA synchronization undergoes significant challenges for GPU programmers due to its intricate parallel computing mechanism and coding practices. In this paper, we propose AuCS, the first general framework to automate synchronization for CUDA kernel functions. AuCS transforms the original LLVM-level CUDA program control flow graph in a semantic-preserving manner for exploring the possible barrier function locations. Accordingly, AuCS develops mechanisms to correctly place barrier functions for automating synchronization in multiple erroneous (challenging-to-be-detected) synchronization scenarios, including data race, barrier divergence, and redundant barrier functions. To evaluate the effectiveness and efficiency of AuCS, we conduct an extensive set of experiments and the results demonstrate that AuCS can automate 20 out of 24 erroneous synchronization scenarios.","2643-1572","978-1-7281-2508-4","10.1109/ASE.2019.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952529","CUDA;program repair;synchronization automation;program transformation","Graphics processing units;Synchronization;Computer bugs;Kernel;Instruction sets;Parallel processing;Computer science","flow graphs;graphics processing units;parallel architectures;parallel programming;program processors","CUDA synchronization;program transformation;parallel computing platform;general purpose GPU computing;GPU programmers;AuCS;CUDA kernel functions;original LLVM-level CUDA;program control flow graph;multiple erroneous synchronization scenarios","","5","","60","","9 Jan 2020","","","IEEE","IEEE Conferences"
"One Size Doesn't Fit All: Quantifying Performance Portability of Graph Applications on GPUs","T. Sorensen; S. Pai; A. F. Donaldson","Princeton University, USA,UC Santa Cruz,USA; University of Rochester,USA; Imperial College London,UK","2019 IEEE International Symposium on Workload Characterization (IISWC)","19 Mar 2020","2019","","","155","166","Hand-optimising graph algorithm code for different GPUs is particularly labour-intensive and error-prone, involving complex and ill-understood interactions between GPU chips, applications, and inputs. Although the generation of optimised variants has been automated through graph algorithm DSL compilers, these do not yet use an optimisation policy. Instead they defer to techniques like autotuning, which can produce good results, but at the expense of portability. In this work, we propose a methodology to automatically identify portable optimisation policies that can be tailored (“semi-specialised”) as needed over a combination of chips, applications and inputs. Using a graph algorithm DSL compiler that targets the OpenCL programming model, we demonstrate optimising graph algorithms to run in a portable fashion across a wide range of GPU devices for the first time. We use this compiler and its optimisation space as the basis for a large empirical study across 17 graph applications, 3 diverse graph inputs and 6 GPUs spanning multiple vendors. We show that existing automatic approaches for building a portable optimisation policy fall short on our dataset, providing trivial or biased results. Thus, we present a new statistical analysis which can characterise optimisations and quantify performance trade-offs at various degrees of specialisation. We use this analysis to quantify the performance tradeoffs as portability is sacrificed for specialisation across three natural dimensions: chip, application, and input. Compared to not optimising programs at all, a fully portable approach provides a 1.15× improvement in geometric mean performance, rising to 1.29 × when specialised to application and inputs (but not hardware). Furthermore, these semi-specialised optimisations provide insights into performance-critical features of specialisation. For example, optimisations specialised by chip reveal subtle, yet performance-critical, characteristics of various GPUs.","","978-1-7281-4045-2","10.1109/IISWC47752.2019.9042139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042139","","","graph theory;graphics processing units;program compilers;statistical analysis","hand-optimising graph algorithm code;GPU chips;graph algorithm DSL compiler;portable optimisation policies;graph algorithms;optimisation space;GPU;optimising programs;portable optimisation policy","","1","","33","","19 Mar 2020","","","IEEE","IEEE Conferences"
"Fast parallel cutoff pair interactions for molecular dynamics on heterogeneous systems","Q. Wu; C. Yang; T. Tang; K. Lu","School of Computer Science, National University of Defense Technology, Changsha 410073, China; School of Computer Science, National University of Defense Technology, Changsha 410073, China; School of Computer Science, National University of Defense Technology, Changsha 410073, China; School of Computer Science, National University of Defense Technology, Changsha 410073, China","Tsinghua Science and Technology","15 Jun 2012","2012","17","3","265","277","Heterogeneous systems with both Central Processing Units (CPUs) and Graphics Processing Units (GPUs) are frequently used to accelerate short-ranged Molecular Dynamics (MD) simulations. The most time-consuming task in short-ranged MD simulations is the computation of particle-to-particle interactions. Beyond a certain distance, these interactions decrease to zero. To minimize the operations to investigate distance, previous works have tiled interactions by employing the spatial attribute, which increases the memory access and GPU computations, hence decreasing performance. Other studies ignore the spatial attribute and construct an all-versus-all interaction matrix, which has poor scalability. This paper presents an improved algorithm. The algorithm first bins particles into voxels according to the spatial attributes, and then tiles the all-versus-all matrix into voxel-versus-voxel sub-matrixes. Only the sub-matrixes between neighboring voxels are computed on the GPU. Therefore, the algorithm reduces the distance examine operations and limits additional memory access and GPU computations. This paper also adopts a multi-level programming model to implement the algorithm on multi-nodes of Tianhe-lA. By employing (1) a patch design to exploit parallelism across the simulation domain, (2) a communication overlapping method to overlap the communications between CPUs and GPUs, and (3) a dynamic workload balancing method to adjust the workloads among compute nodes, the implementation achieves a speedup of 4.16× on one NVIDIA Tesla M2050 GPU compared to a 2.93 GHz six-core Intel Xeon X5670 CPU. In addition, it runs 2.41× faster on 256 compute nodes of Tianhe-lA (with two CPUs and one GPU inside a node) than on 256 GPU-excluded nodes.","1007-0214","","10.1109/TST.2012.6216756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216756","cutoff pair interactions;molecular dynamics;heterogeneous computing;GPU computing","Graphics processing unit;Instruction sets;Computational modeling;Heuristic algorithms;Computer architecture;Central Processing Unit","","","","1","","","","15 Jun 2012","","","TUP","TUP Journals"
"Integration of CUDA Processing within the C++ Library for Parallelism and Concurrency (HPX)","P. Diehl; M. Seshadri; T. Heller; H. Kaiser","Center for Comput. & Technol., Louisiana State Univ., Baton Rouge, LA, USA; Nanyang Technol. Univ., Singapore, Singapore; Center for Comput. & Technol., Louisiana State Univ., Baton Rouge, LA, USA; Center for Comput. & Technol., Louisiana State Univ., Baton Rouge, LA, USA","2018 IEEE/ACM 4th International Workshop on Extreme Scale Programming Models and Middleware (ESPM2)","14 Feb 2019","2018","","","19","28","Experience shows that on today's high performance systems the utilization of different acceleration cards in conjunction with a high utilization of all other parts of the system is difficult. Future architectures, like exascale clusters, are expected to aggravate this issue as the number of cores are expected to increase and memory hierarchies are expected to become deeper. One big aspect for distributed applications is to guarantee high utilization of all available resources, including local or remote acceleration cards on a cluster while fully using all the available CPU resources and the integration of the GPU work into the overall programming model. For the integration of CUDA code we extended HPX, a general purpose C++ run time system for parallel and distributed applications of any scale, and enabled asynchronous data transfers from and to the GPU device and the asynchronous invocation of CUDA kernels on this data. Both operations are well integrated into the general programming model of HPX which allows to seamlessly overlap any GPU operation with work on the main cores. Any user defined CUDA kernel can be launched on any (local or remote) GPU device available to the distributed application. We present asynchronous implementations for the data transfers and kernel launches for CUDA code as part of a HPX asynchronous execution graph. Using this approach we can combine all remotely and locally available acceleration cards on a cluster to utilize its full performance capabilities. Overhead measurements show, that the integration of the asynchronous operations (data transfer + launches of the kernels) as part of the HPX execution graph imposes no additional computational overhead and significantly eases orchestrating coordinated and concurrent work on the main cores and the used GPU devices.","","978-1-7281-0178-1","10.1109/ESPM2.2018.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638479","Asynchronous-many-task-systems-(ATM),-CUDA,-parallelism,-concurrency,-HPX","Graphics processing units;Kernel;C++ languages;Data transfer;Task analysis;Programming;Message systems","C++ language;concurrency (computers);graphics processing units;parallel architectures;parallel processing","local acceleration cards;remote acceleration cards;CUDA code;distributed application;asynchronous data transfers;asynchronous invocation;CUDA kernel;GPU operation;data transfer;HPX asynchronous execution graph;CUDA processing;high performance systems;exascale clusters;memory hierarchies;CPU resources;general purpose C++ run time system;parallel applications","","2","","26","","14 Feb 2019","","","IEEE","IEEE Conferences"
"Parallel computing with CUDA","M. Garland",NVIDIA,"2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)","24 May 2010","2010","","","1","1","Summary form only given. NVIDIA's CUDA architecture provides a powerful platform for writing highly parallel programs. By providing simple abstractions for hierarchical thread organization, memories, and synchronization, the CUDA programming model allows programmers to write scalable programs without the burden of learning a multitude of new programming constructs. The CUDA architecture can support many languages and programming environments, including C, Fortran, OpenCL, and DirectX Compute. In this tutorial, I will provide an overview of modern GPU processor design and its implications for successful parallel programming models. I will present the programming model adopted by the CUDA architecture, and demonstrate how this is exposed in the C/C++ language. Finally, I will sketch some techniques for implementing common data-parallel algorithms in the CUDA model.","1530-2075","978-1-4244-6443-2","10.1109/IPDPS.2010.5470378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470378","","Parallel processing;Computer architecture;Visualization;Parallel programming;Multicore processing;Programming environments;Process design","C++ language;computer graphic equipment;coprocessors;parallel algorithms;parallel architectures;parallel programming","parallel computing;NVIDIA CUDA architecture;hierarchical thread organization;synchronization;C;Fortran;OpenCL;DirectX Compute;GPU processor design;parallel programming;C++ language;data parallel algorithms","","12","","","","24 May 2010","","","IEEE","IEEE Conferences"
"Programming for GPUs: The Directive-Based Approach","L. Grillo; F. De Sande; J. J. Fumero; R. Reyes","Dept. de EIO y Comput., Univ. de La Laguna, La Laguna, Spain; Dept. de EIO y Comput., Univ. de La Laguna, La Laguna, Spain; Inst. Tecnol. de Energias Renovables, Granadilla de Abona, Spain; Edinburgh Parallel Comput. Centre, Univ. of Edinburgh, Edinburgh, UK","2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","12 Dec 2013","2013","","","612","617","In the last years, hardware accelerators, such as GPUs have become ubiquitous in the HPC landscape and GPGPU has been massively adopted by the HPC research community. If something is slowing down further expansion of this technology are its difficulties in terms of programmability. Although several libraries and applications providing GPU support are available, the need of implementing new algorithms from scratch, or adapting sequential programs to accelerators, still exist. Writing programs to be executed on accelerators is not easy, particularly for non-expert developers coming from the science or engineering fields, as it requires deep understanding of the underlying architecture. Different alternatives have appeared aimed to diminish the GPU programming effort. In the wake of the success of OpenMP, several directive-oriented programming models have been created. Although future OpenMP releases will integrate accelerators, tools are needed in the meantime. In this work, we present a comparison of directive-based approaches for GPU platforms, hiCUDA, PGI Accelerator and OpenACC. For the last, in addition to the two commercial compilers available, we include results using accULL, our own OpenACC implementation. To illustrate the portability of these alternatives, we show performance figures for both Fermi and Kepler NVIDIA cards.","","978-0-7695-5094-7","10.1109/3PGCIC.2013.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681300","OpenACC;GPGPU;Compilers;PGI;HMPP;accULL;CUDA","Graphics processing units;Kernel;Programming;Standards;Runtime;Parallel processing","graphics processing units;message passing;parallel architectures;program compilers;software libraries","hardware accelerator;HPC;sequential program;program execution;GPU programming;OpenMP;directive-oriented programming model;hiCUDA;PGI accelerator;OpenACC;compiler;accULL;Kepler NVIDIA card;Fermi card;program library","","1","","12","","12 Dec 2013","","","IEEE","IEEE Conferences"
"Use case analysis of OpenACC directives in the implementation of image processing algorithms","M. J. Mišić; D. D. Dašić; M. V. Tomašević","Elektroteh. Fak., Univ. u Beogradu, Belgrade, Serbia; Elektroteh. Fak., Univ. u Beogradu, Belgrade, Serbia; Elektroteh. Fak., Univ. u Beogradu, Belgrade, Serbia","2013 21st Telecommunications Forum Telfor (TELFOR)","20 Jan 2014","2013","","","959","962","Graphics processing units have been intensively used in general-purpose computations in the recent years. Although new programming models (CUDA, OpenCL) have been developed and widely applied to support GPU programming, they still impose demanding environments for those users who want to speed up their applications without the knowledge of low level details of the underlying hardware. To cope with this problem, the paper analyzes and evaluates the usage of OpenACC directive-based programming model in the case of image processing algorithms that are amenable for GPU execution.","","978-1-4799-1420-3","10.1109/TELFOR.2013.6716390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716390","","Graphics processing units;Computational modeling;Programming;Central Processing Unit;Kernel;Electronic mail;Image processing","graphics processing units;image processing","case analysis;OpenACC directives;image processing algorithms;graphics processing units programming;GPU programming;demanding environments","","1","","6","","20 Jan 2014","","","IEEE","IEEE Conferences"
"Hedgehog: Understandable Scheduler-Free Heterogeneous Asynchronous Multithreaded Data-Flow Graphs","A. Bardakoff; B. Bachelet; T. Blattner; W. Keyrouz; G. C. Kroiz; L. Yon","National Institute of Standards & Technology,Gaithersburg,MD,20899-8970; Université Clermont Auvergne, CNRS, LIMOS,Clermont-Ferrand,France,F-63000; National Institute of Standards & Technology,Gaithersburg,MD,20899-8970; National Institute of Standards & Technology,Gaithersburg,MD,20899-8970; University of Maryland,Department of Mathematics and Statistics,Baltimore,MD,USA,21250; ISIMA, CNRS, LIMOS,Clermont-Ferrand,France,F-63000","2020 IEEE/ACM 3rd Annual Parallel Applications Workshop: Alternatives To MPI+X (PAW-ATM)","29 Dec 2020","2020","","","1","15","Getting performance on high-end heterogeneous nodes is challenging. This is due to the large semantic gap between a computation's specification-possibly mathematical formulas or an abstract sequential algorithm-and its parallel implementation; this gap obscures the program's parallel structures and how it gains or loses performance. We present Hedgehog, a library aimed at coarse-grain parallelism. It explicitly embeds a dataflow graph in a program and uses this graph at runtime to drive the program's execution so it takes advantage of hardware parallelism (multicore CPUs and multiple accelerators). Hedgehog has asynchronicity built in. It statically binds individual threads to graph nodes, which are ready to fire when any of their inputs are available. This allows Hedgehog to avoid using a global scheduler and the loss of performance associated with global synchronizations and managing of thread pools. Hedgehog provides a separation of concerns and distinguishes between compute and state maintenance tasks. Its API reflects this separation and allows a developer to gain a better understanding of performance when executing the graph. Hedgehog is implemented as a C++ 17 headers-only library. One feature of the framework is its low overhead; it transfers control of data between two nodes in ≈ 1 μs. This low overhead combines with Hedgehog's API to provide essentially cost-free profiling of the graph, thereby enabling experimentation for performance, which enhances a developer's insight into a program's performance. Hedgehog's asynchronous data-flow graph supports a data streaming programming model both within and between graphs. We demonstrate the effectiveness of this approach by highlighting the performance of streaming implementations of two numerical linear algebra routines, which are comparable to existing libraries: matrix multiplication achieves >95 % of the theoretical peak of 4 GPUs; LU decomposition with partial pivoting starts streaming partial final result blocks 40× earlier than waiting for the full result. The relative ease and understandability of obtaining performance with Hedgehog promises to enable non-specialists to target performance on high-end single nodes.","","978-1-7281-5450-3","10.1109/PAWATM51920.2020.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306983","high performance computing, data-flow, heterogeneous computing, dataflow, pipelining, HPC, parallelism, heterogeneity, metaprogramming, C++, GPU","Task analysis;Libraries;Parallel processing;Hardware;Computational modeling;Signal processing algorithms;Tools","application program interfaces;data flow computing;data flow graphs;graph theory;linear algebra;matrix multiplication;multiprocessing systems;multi-threading;parallel algorithms;scheduling","coarse-grain parallelism;hardware parallelism;graph nodes;data streaming programming model;scheduler-free heterogeneous asynchronous multithreaded data-flow graphs;Hedgehog API;numerical linear algebra routines","","","","22","","29 Dec 2020","","","IEEE","IEEE Conferences"
"CLACC: Translating OpenACC to OpenMP in Clang","J. E. Denny; S. Lee; J. S. Vetter",Oak Ridge National Laboratory; Oak Ridge National Laboratory; Oak Ridge National Laboratory,"2018 IEEE/ACM 5th Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC)","14 Feb 2019","2018","","","18","29","OpenACC was launched in 2010 as a portable programming model for heterogeneous accelerators. Although various implementations already exist, no extensible, open-source, production-quality compiler support is available to the community. This deficiency poses a serious risk for HPC application developers targeting GPUs and other accelerators, and it limits experimentation and progress for the OpenACC specification. To address this deficiency, Clacc is a recent effort funded by the US Exascale Computing Project to develop production OpenACC compiler support for Clang and LLVM. A key feature of the Clacc design is to translate OpenACC to OpenMP to build on Clang's existing OpenMP compiler and runtime support. In this paper, we describe the Clacc goals and design. We also describe the challenges that we have encountered so far in our prototyping efforts, and we present some early performance results.","","978-1-7281-0188-0","10.1109/LLVM-HPC.2018.8639349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8639349","OpenACC;OpenMP;LLVM;multicore;GPU;accelerators;source-to-source translation;compiler","Tools;Runtime;Ecosystems;Production;Programming;Graphics processing units;Computer architecture","parallel processing;program compilers;public domain software;software portability","OpenMP;portable programming model;heterogeneous accelerators;production-quality compiler support;OpenACC specification;US Exascale Computing Project;HPC application;CLACC;Clang;open-source software;OpenACC compiler support","","4","","20","","14 Feb 2019","","","IEEE","IEEE Conferences"
"MDACCER: Modified Distributed Assessment of the Closeness CEntrality Ranking in Complex Networks for Massively Parallel Environments","F. L. Cabral; C. Osthoff; D. Ramos; R. Nardes","Lab. Nac. de Comput. Cienc., Centro de Comput. de Alto Desempenho, Brazil; Lab. Nac. de Comput. Cienc., Centro de Comput. de Alto Desempenho, Brazil; Lab. Nac. de Comput. Cienc., Centro de Comput. de Alto Desempenho, Brazil; Lab. Nac. de Comput. Cienc., Centro de Comput. de Alto Desempenho, Brazil","2015 International Symposium on Computer Architecture and High Performance Computing Workshop (SBAC-PADW)","3 Mar 2016","2015","","","43","48","We propose a new method derived from DACCER (Distributed Assessment of the Closeness CEntrality Ranking): the modified DACCER (MDACCER), for assessing traditional closeness centrality ranking. MDACCER presents a relaxation that allows it to take advantage of massively parallel environments like General Purpose Graphics Processing Units (GPGPUs). Traditional DACCER proposal assesses Closeness centrality ranking in a limited neighborhood using only information around each node at low computational cost and capability to be executed in a distributed environment. Despite all the advantages, DACCER presents some difficulties in GPGPUs programming model that increases its computational cost at this particular environment. In contrast to the poor performance of DACCER on GPGPUs, experimental results demonstrate MDACCER is as simple and efficient as DACCER to assess Closeness centrality ranking in complex networks and moreover it does not have the same bottlenecks in GPGPUs computing about memory usage and time complexity. We performed MDACCER for some synthetically generated networks, specifically Barabási-Albert ones and results indicate MADCCER correlates Closeness centrality ranking almost as well as DACCER does with lower computational costs.","","978-1-4673-8621-0","10.1109/SBAC-PADW.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423179","Parallel Computing;Network Centrality Ranking;DACCER;Closeness;GPU;CUDA","Correlation;Kernel;Proposals;Computational efficiency;Computer architecture;Complex networks;Graphics processing units","complex networks;computational complexity;graphics processing units;parallel programming","time complexity;GPGP programming model;general purpose graphics processing unit;modified DACCER;massively parallel environment;complex network;closeness centrality ranking;MDACCER;modified distributed assessment","","","","13","","3 Mar 2016","","","IEEE","IEEE Conferences"
"High-Performance Graph Analytics on Manycore Processors","G. M. Slota; S. Rajamanickam; K. Madduri","Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; Scalable Algorithms Dept., Sandia Nat. Labs., Albuquerque, NM, USA; Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","2015 IEEE International Parallel and Distributed Processing Symposium","20 Jul 2015","2015","","","17","27","The divergence in the computer architecture landscape has resulted in different architectures being considered mainstream at the same time. For application and algorithm developers, a dilemma arises when one must focus on using underlying architectural features to extract the best performance on each of these architectures, while writing portable code at the same time. We focus on this problem with graph analytics as our target application domain. In this paper, we present an abstraction-based methodology for performance-portable graph algorithm design on manicure architectures. We demonstrate our approach by systematically optimizing algorithms for the problems of breadth-first search, color propagation, and strongly connected components. We use Kokkos, a manicure library and programming model, for prototyping our algorithms. Our portable implementation of the strongly connected components algorithm on the NVIDIA Tesla K40M is up to 3.25× faster than a state-of-the-art parallel CPU implementation on a dual-socket Sandy Bridge compute node.","1530-2075","978-1-4799-8649-1","10.1109/IPDPS.2015.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161272","graph computations;BFS;color propagation;GPU;parallel performance;portability","Instruction sets;Silicon;Optimization;Synchronization;Color;Arrays;Parallel processing","feature extraction;graph theory;multiprocessing systems;parallel architectures;tree searching","high-performance graph analytics;manycore processors;computer architecture landscape;architectural feature extraction;portable code writing;abstraction-based methodology;performance-portable graph algorithm design;manicure architectures;breadth-first search problems;color propagation;optimizing algorithms;Kokkos manicure library;programming model;strongly connected components algorithm;NVIDIA Tesla K40M;parallel CPU;dual-socket Sandy Bridge compute node","","8","","37","","20 Jul 2015","","","IEEE","IEEE Conferences"
"Real-time block matching motion estimation onto GPGPU","E. Monteiro; M. Maule; F. Sampaio; C. Diniz; B. Zatt; S. Bampi","Informatics Institute - PPGC - PGMICRO, Federal University of Rio Grande do Sul (UFRGS) - Porto Alegre, Brazil; Informatics Institute - PPGC - PGMICRO, Federal University of Rio Grande do Sul (UFRGS) - Porto Alegre, Brazil; Informatics Institute - PPGC - PGMICRO, Federal University of Rio Grande do Sul (UFRGS) - Porto Alegre, Brazil; Informatics Institute - PPGC - PGMICRO, Federal University of Rio Grande do Sul (UFRGS) - Porto Alegre, Brazil; Informatics Institute - PPGC - PGMICRO, Federal University of Rio Grande do Sul (UFRGS) - Porto Alegre, Brazil; Informatics Institute - PPGC - PGMICRO, Federal University of Rio Grande do Sul (UFRGS) - Porto Alegre, Brazil","2012 19th IEEE International Conference on Image Processing","21 Feb 2013","2012","","","1693","1696","This work presents an efficient method to map Motion Estimation (ME) algorithms onto General Purpose Graphic Processing Unit (GPGPU) architectures using CUDA programming model. Our method jointly exploits the massive parallelism available in current GPGPU devices and the parallelization potential of ME algorithms: Full Search (FS) and Diamond Search (DS). Our main goal is to evaluate the feasibility of achieving real-time high-definition video encoding performance running on GPUs. For comparison reasons, multi-core parallel and distributed versions of these algorithms were developed using OpenMP and MPI (Message Passing Interface) libraries, respectively. The CUDA-based solutions achieve the highest speed-up in comparison with OpenMP and MPI versions for both algorithms and, when compared to the state-of-the-art, our FS and DS solutions reach up to 18x and 11x speed-up, respectively.","2381-8549","978-1-4673-2533-2","10.1109/ICIP.2012.6467204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6467204","Motion Estimation;Block Matching Algorithms;GPU;CUDA;OpenMP;MPI","Graphics processing units;Motion estimation;Algorithm design and analysis;Computer architecture;Real-time systems;Diamonds;Programming","application program interfaces;graphics processing units;image matching;message passing;motion estimation;multiprocessing systems;parallel algorithms;parallel architectures;parallel programming;search problems;video coding","real-time block matching motion estimation;map motion estimation algorithms;general purpose graphic processing unit architectures;GPGPU architectures;CUDA programming model;GPGPU devices;parallelization potential;full search;diamond search;real-time high-definition video encoding performance;multicore parallel algorithms;distributed algorithms;MPI libraries;OpenMP libraries;message passing interface libraries;CUDA-based solutions;DS solutions;FS solutions","","9","","13","","21 Feb 2013","","","IEEE","IEEE Conferences"
"Locality-Aware Scheduling for Scalable Heterogeneous Environments","A. V. Kamatar; R. D. Friese; R. Gioiosa","Pacific Northwest National Laboratory,High Performance Computing,Richland,US; Pacific Northwest National Laboratory,High Performance Computing,Richland,US; Pacific Northwest National Laboratory,High Performance Computing,Richland,US","2020 IEEE/ACM International Workshop on Runtime and Operating Systems for Supercomputers (ROSS)","29 Dec 2020","2020","","","50","58","Heterogeneous computing promise boost performance of scientific applications by allowing massively parallel execution of computational tasks. However, manually managing extremely heterogeneous, multi-device systems is complicated and may result in sub-optimal performance. Specifically, data management is an extremely challenging problem on multi-device systems. In this work, we introduce two locality-aware schedulers for the Minos Computing Library (MCL), an asynchronous, task-based programming model and runtime for extremely heterogeneous systems. The first scheduler implements a pure locality-aware algorithm to maximize data reuse, though it might incur in “hot-spots” that limit system utilization. The second scheduler mitigates this drawback by dynamically targeting between locality-awareness and system utilization based on the current workload and available computing devices. Our results show that locality-awareness greatly benefit applications that exhibit data reuse, providing up to 6.9x and 7.9x over the original MCL scheduler and equivalent OpenCL implementations, respectively. Moreover, our schedulers introduce negligible overhead compared with the original MCL scheduler and achieve similar performance for applications that don't benefit from data locality.","","978-1-6654-2268-0","10.1109/ROSS51935.2020.00011","Research and Development; Advanced Scientific Computing Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307939","heterogeneous computing, gpu, gpgpu, openCL","Task analysis;Performance evaluation;Runtime;Programming;Computational modeling;Round robin;Graphics processing units","multiprocessing systems;parallel architectures;parallel processing;parallel programming;resource allocation;scheduling;shared memory systems","pure locality-aware algorithm;limit system utilization;locality-awareness;available computing devices;exhibit data reuse;original MCL scheduler;data locality;locality-aware scheduling;scalable heterogeneous environments;heterogeneous computing promise;scientific applications;massively parallel execution;computational tasks;multidevice systems;sub-optimal performance;data management;extremely challenging problem;locality-aware schedulers;Minos Computing Library;task-based programming model;extremely heterogeneous systems","","1","","31","","29 Dec 2020","","","IEEE","IEEE Conferences"
"Implementation and Optimization of a 1D2V PIC Method for Nonlinear Kinetic Models on GPUs","M. Korch; P. Raithel; T. Werner","University of Bayreuth,Department of Computer Science,Bayreuth,Germany,95440; University of Bayreuth,Department of Computer Science,Bayreuth,Germany,95440; University of Bayreuth,Department of Computer Science,Bayreuth,Germany,95440","2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)","14 May 2020","2020","","","30","37","This paper considers the parallel numerical simulation of the time evolution of galaxies and globular clusters on GPUs. The model used is the Einstein-Vlasov system, which is designed, in particular, to study the formation of black holes and spacetime singularities in a general relativistic framework.First, a reference implementation is derived using NVIDIA CUDA as programming model, which is then optimized in several steps. Bottlenecks are identified by profiling, and different approaches, namely particle sort, improved treatment of atomic operations, and kernel fusion are investigated to overcome these bottlenecks. Each optimized variant is evaluated in relation to the other variants using detailed runtime experiments and profiling results. Using in the order of 10<sup>7</sup> to 10<sup>8</sup> particles, speedups between 1.84 and 2.38 w.r.t. the reference implementation have been observed.","2377-5750","978-1-7281-6582-0","10.1109/PDP50117.2020.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092350","particle-in-cell;kinetic models;Einstein–Vlasov;parallelization;GPU","Graphics processing units;Kernel;Mathematical model;Numerical models;Computational modeling;Measurement;Arrays","astronomy computing;black holes;general relativity;numerical analysis;parallel architectures;space-time configurations","atomic operations;kernel fusion;optimized variant;runtime experiments;profiling results;reference implementation;1D2V PIC Method;nonlinear kinetic models;GPUs;parallel numerical simulation;globular clusters;Einstein-Vlasov system;black holes;spacetime singularities;general relativistic framework;NVIDIA CUDA;programming model;particle sort","","","","20","","14 May 2020","","","IEEE","IEEE Conferences"
"Convergence and scalarization for data-parallel architectures","Y. Lee; R. Krashinsky; V. Grover; S. W. Keckler; K. Asanović",University of California at Berkeley; NVIDIA; NVIDIA; NVIDIA; University of California at Berkeley,"Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","8 Apr 2013","2013","","","1","11","Modern throughput processors such as GPUs achieve high performance and efficiency by exploiting data parallelism in application kernels expressed as threaded code. One draw-back of this approach compared to conventional vector architectures is redundant execution of instructions that are common across multiple threads, resulting in energy inefficiency due to excess instruction dispatch, register file accesses, and memory operations. This paper proposes to alleviate these overheads while retaining the threaded programming model by automatically detecting the scalar operations and factoring them out of the parallel code. We have developed a scalarizing compiler that employs convergence and variance analyses to statically identify values and instructions that are invariant across multiple threads. Our compiler algorithms are effective at identifying convergent execution even in programs with arbitrary control flow, identifying two-thirds of the opportunity captured by a dynamic oracle. The compile-time analysis leads to a reduction in instructions dispatched by 29%, register file reads and writes by 31% memory address counts by 47%, and data access counts by 38%.","","978-1-4673-5525-4","10.1109/CGO.2013.6494995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494995","CUDA;GPU;Scalarization","Instruction sets;Convergence;Registers;Kernel;Computer architecture;Graphics processing units;Algorithm design and analysis","convergence;multi-threading;optimising compilers;parallel architectures;power aware computing","data-parallel architecture scalarization;data-parallel architecture convergence;throughput processors;GPUs;data parallelism;application kernels;threaded code;vector architectures;energy inefficiency;instruction dispatch;memory operations;threaded programming model;automatic scalar operation detection;parallel code;scalarizing compiler;compiler algorithms;convergent execution;arbitrary control flow;dynamic oracle;compile-time analysis;register file reads;register file writes;memory address counts;data access counts","","21","3","29","","8 Apr 2013","","","IEEE","IEEE Conferences"
"OpenMPC: Extended OpenMP Programming and Tuning for GPUs","S. Lee; R. Eigenmann","Sch. of ECE, Purdue Univ. West Lafayette, West Lafayette, IN, USA; Sch. of ECE, Purdue Univ. West Lafayette, West Lafayette, IN, USA","SC '10: Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis","29 Nov 2010","2010","","","1","11","General-Purpose Graphics Processing Units (GPGPUs) are promising parallel platforms for high performance computing. The CUDA (Compute Unified Device Architecture) programming model provides improved programmability for general computing on GPGPUs. However, its unique execution model and memory model still pose significant challenges for developers of efficient GPGPU code. This paper proposes a new programming interface, called OpenMPC, which builds on OpenMP to provide an abstraction of the complex CUDA programming model and offers high-level controls of the involved parameters and optimizations. We have developed a fully automatic compilation and user-assisted tuning system supporting OpenMPC. In addition to a range of compiler transformations and optimizations, the system includes tuning capabilities for generating, pruning, and navigating the search space of compilation variants. Our results demonstrate that OpenMPC offers both programmability and tunability. Our system achieves 88% of the performance of the hand-coded CUDA programs.","2167-4337","978-1-4244-7559-9","10.1109/SC.2010.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5644879","","Graphics processing unit;Kernel;Optimization;Instruction sets;Tuning;Programming;Registers","computer graphic equipment;coprocessors;parallel programming;public domain software","OpenMPC;extended OpenMP programming;GPU;general purpose graphics processing units;high performance computing;CUDA programming model;user assisted tuning system;compiler transformations;compiler optimizations;Compute Unified Device Architecture","","128","1","16","","29 Nov 2010","","","IEEE","IEEE Conferences"
"Medical Imaging Processing on a Big Data Platform Using Python: Experiences with Heterogeneous and Homogeneous Architectures","E. Serrano; J. G. Blas; J. Carretero; M. Abella; M. Desco","Comput. Archit. & Technol. Group, Univ. Carlos III of Madrid, Leganes, Spain; Comput. Archit. & Technol. Group, Univ. Carlos III of Madrid, Leganes, Spain; Comput. Archit. & Technol. Group, Univ. Carlos III of Madrid, Leganes, Spain; Inst. de Investigacin Sanitaria Gregorio Maranon, Madrid, Spain; Inst. de Investigacin Sanitaria Gregorio Maranon, Madrid, Spain","2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","13 Jul 2017","2017","","","830","837","The apparition of new paradigms, programming models, and languages that offer better programmability and better performance turns the implementation of current scientific applications into a less time-consuming task than years ago. One significant example of this trend is the MapReduce programming model and its implementation using Apache Spark. Nowadays, this programming model is mainly used for data analysis and machine learning applications, although it has been expanded to its usage in the HPC community. On the side of programming languages, Python has positioned itself as an alternative to other scientific programming languages, such as Matlab or Julia. In this work we explore the capabilities of Python and Apache Spark as partners in the implementation of the backprojection operator of a CT reconstruction application. We present two interesting approaches with two different types of architectures: a heterogeneous architecture including NVidia GPUs and a full performance CPU mode with the compatibility with C/C++ native source code. We experimentally demonstrate that current CPU-based implementations scale with the number of computational units.","","978-1-5090-6611-7","10.1109/CCGRID.2017.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973788","CUDA;Big Data;Apache Spark;CT;backprojection;Python","Computer architecture;Programming;Computed tomography;Computational modeling;Sparks;Image reconstruction;Big Data","Big Data;C++ language;data analysis;graphics processing units;learning (artificial intelligence);medical image processing;microprocessor chips;parallel processing;programming languages;public domain software;source code (software);workstation clusters","medical imaging processing;Big Data platform;Python;heterogeneous architectures;homogeneous architectures;MapReduce programming model;Apache Spark;data analysis;machine learning;HPC community;programming languages;NVidia GPU;CPU mode;C/C++ native source code","","7","","27","","13 Jul 2017","","","IEEE","IEEE Conferences"
"Porting Real-World Applications to GPU Clusters: A Celerity and Cronos Case Study","P. Gschwandtner; R. Kissmann; D. Huber; P. Salzmann; F. Knorr; P. Thoman; T. Fahringer","University of Innsbruck,Research Center HPC,Innsbruck,Austria; University of Innsbruck,Institute for Astro- and Particle Physics,Innsbruck,Austria; University of Innsbruck,Institute for Astro- and Particle Physics,Innsbruck,Austria; University of Innsbruck,Department of Computer Science,Innsbruck,Austria; University of Innsbruck,Department of Computer Science,Innsbruck,Austria; University of Innsbruck,Department of Computer Science,Innsbruck,Austria; University of Innsbruck,Department of Computer Science,Innsbruck,Austria","2021 IEEE 17th International Conference on eScience (eScience)","26 Oct 2021","2021","","","90","98","Accelerator clusters are an ongoing trend in high performance computing, continuously gaining traction and forming a ubiquitous hardware resource for domain scientists to run large-scale simulations on. However, there is often a gap between new hardware technologies and adoption by legacy code bases. Porting real-world applications to new programming models is a difficult undertaking, aggravated by the need for support for both distributed-memory and accelerator parallelism. In this work, we present a case study of porting Cronos, a real-world code from the field of magnetohydrodynamics, to Celerity, a high-level programming model for distributed-memory accelerator clusters. We discuss the numerical, algorithmic and implementation properties of the application and motivate our decisions for adapting them where necessary. Preliminary results show a parallel efficiency of up to 87% for 16 GPUs.","","978-1-6654-0361-0","10.1109/eScience51609.2021.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582351","accelerators;gpus;sycl;celerity;distributed memory;parallel programming","Codes;Magnetohydrodynamics;Parallel programming;Computational modeling;High performance computing;Graphics processing units;Parallel processing","","","","","","20","","26 Oct 2021","","","IEEE","IEEE Conferences"
"Impacts of Multi-GPU MPI Collective Communications on Large FFT Computation","A. Ayala; S. Tomov; X. Luo; H. Shaeik; A. Haidar; G. Bosilca; J. Dongarra","University of Tennessee, USA; University of Tennessee, USA; University of Tennessee, USA; University of Tennessee, USA; Nvidia Corporation, USA; University of Tennessee, USA; University of Tennessee, USA","2019 IEEE/ACM Workshop on Exascale MPI (ExaMPI)","13 Jan 2020","2019","","","12","18","Most applications targeting exascale, such as those part of the Exascale Computing Project (ECP), are designed for heterogeneous architectures and rely on the Message Passing Interface (MPI) as their underlying parallel programming model. In this paper we analyze the limitations of collective MPI communication for the computation of fast Fourier transforms (FFTs), which are relied on heavily for large-scale particle simulations. We present experiments made at one of the largest heterogeneous platforms, the Summit supercomputer at ORNL. We discuss communication models from state-of-the-art FFT libraries, and propose a new FFT library, named HEFFTE (Highly Efficient FFTs for Exascale), which supports heterogeneous architectures and yields considerable speedups compared with CPU libraries, while maintaining good weak as well as strong scalability.","","978-1-7281-6009-2","10.1109/ExaMPI49596.2019.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955469","","","application program interfaces;fast Fourier transforms;graphics processing units;message passing;parallel programming","multiGPU MPI collective communications;FFT computation;MPI communication;fast Fourier transforms;particle simulations;FFT library;Highly Efficient FFTs for Exascale;HEFFTE","","7","","18","","13 Jan 2020","","","IEEE","IEEE Conferences"
"Optimal Kernel Design for Finite-Element Numerical Integration on GPUs","K. Banaś; F. Krużel; J. Bielański",AGH University of Science and Technology; Cracow University of Technology; AGH University of Science and Technology,"Computing in Science & Engineering","13 Oct 2020","2020","22","6","61","74","This article presents the design and optimization of the GPU kernels for numerical integration, as it is applied in the standard form in finite-element codes. The optimization process employs autotuning, with the main emphasis on the placement of variables in the shared memory or registers. OpenCL and the first order finite-element method (FEM) approximation are selected for code design, but the techniques are also applicable to the CUDA programming model and other types of finite-element discretizations (including discontinuous Galerkin and isogeometric). The autotuning optimization is performed for four example graphics processors and the obtained results are discussed.","1558-366X","","10.1109/MCSE.2019.2940656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843948","","Finite element analysis;Graphics processing units;Jacobian matrices;Solid modeling;Approximation algorithms;Optimization;Computational modeling","finite element analysis;Galerkin method;graphics processing units;integration;optimisation;parallel architectures;parallel programming;shared memory systems","isogeometric discretization;discontinuous Galerkin discretization;OpenCL;graphics processors;first order finite-element method approximation;autotuning optimization;finite-element discretizations;CUDA programming model;code design;shared memory;finite-element codes;GPU kernels;finite-element numerical integration;optimal kernel design","","","","21","IEEE","18 Sep 2019","","","IEEE","IEEE Magazines"
"Towards an efficient multi-stage Riemann solver for nuclear physics simulations","S. Cygert; J. Porter-Sobieraj; D. Kikoła; J. Sikorski; M. Słodkowski","Warsaw University of Technology, Faculty of Mathematics and Information Science, Koszykowa 75, 00-662, Poland; Warsaw University of Technology, Faculty of Mathematics and Information Science, Koszykowa 75, 00-662, Poland; Purdue University, Department of Physics, 525 Northwestern Ave., West Lafayette, IN 47907, United States; Warsaw University of Technology, Faculty of Physics, Koszykowa 75, 00-662, Poland; Warsaw University of Technology, Faculty of Physics, Koszykowa 75, 00-662, Poland","2013 Federated Conference on Computer Science and Information Systems","7 Nov 2013","2013","","","441","446","Relativistic numerical hydrodynamics is an important tool in high energy nuclear science. However, such simulations are extremely demanding in terms of computing power. This paper focuses on improving the speed of solving the Riemann problem with the MUSTA-FORCE algorithm by employing the CUDA parallel programming model. We also propose a new approach to 3D finite difference algorithms, which employ a GPU that uses surface memory. Numerical experiments show an unprecedented increase in the computing power compared to a CPU.","","978-83-60810-52-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6644038","","Graphics processing units;Computational modeling;Instruction sets;Mathematical model;Hydrodynamics;Registers;Numerical models","digital simulation;finite difference methods;hydrodynamics;mathematics computing;nuclear engineering computing;parallel architectures;parallel programming","multistage Riemann solver;nuclear physics simulations;relativistic numerical hydrodynamics;high energy nuclear science;MUSTA-FORCE algorithm;CUDA parallel programming model;3D finite difference algorithms;GPU;surface memory;CPU","","","","15","","7 Nov 2013","","","IEEE","IEEE Conferences"
"Using Fermi Architecture Knowledge to Speed up CUDA and OpenCL Programs","Y. Torres; A. Gonzalez-Escribano; D. R. Llanos","Dipt. Inf., Univ. Valladolid, Valladolid, Spain; Dipt. Inf., Univ. Valladolid, Valladolid, Spain; Dipt. Inf., Univ. Valladolid, Valladolid, Spain","2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications","23 Aug 2012","2012","","","617","624","The NVIDIA graphics processing units (GPUs) are playing an important role as general purpose programming devices. The implementation of parallel codes to exploit the GPU hardware architecture is a task for experienced programmers. The threadblock size and shape choice is one of the most important user decisions when a parallel problem is coded. The threadblock configuration has a significant impact on the global performance of the program. While in CUDA parallel programming model it is always necessary to specify the threadblock size and shape, the OpenCL standard also offers an automatic mechanism to take this delicate decision. In this paper we present a study of these criteria for Fermi architecture, introducing a general approach for threadblock choice, and showing that there is considerable room for improvement in OpenCL automatic strategy.","2158-9208","978-1-4673-1631-6","10.1109/ISPA.2012.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6280352","GPGPU;automatic code tuning;Fermi;CUDA;OpenCL","Instruction sets;Shape;Graphics processing unit;Computer architecture;Kernel;Benchmark testing;Tuning","graphics processing units;parallel architectures;parallel programming","fermi architecture knowledge;speed up CUDA programs;speed up OpenCL programs;NVIDIA graphics processing units;GPU hardware architecture;threadblock size;parallel problem;threadblock configuration;CUDA parallel programming model;automatic mechanism","","6","","17","","23 Aug 2012","","","IEEE","IEEE Conferences"
"High-Performance and Real-Time Volume Rendering in CUDA","Y. Zhao; X. Cui; Y. Cheng","Sino-Dutch Biomed. & Inf. Eng. Sch., Northeastern Univ., Shenyang, China; Sino-Dutch Biomed. & Inf. Eng. Sch., Northeastern Univ., Shenyang, China; Sino-Dutch Biomed. & Inf. Eng. Sch., Northeastern Univ., Shenyang, China","2009 2nd International Conference on Biomedical Engineering and Informatics","30 Oct 2009","2009","","","1","4","In order to improve the image quality and rendering speed, how to deal with a large scale of voxel computation is a challenge for programmers who work at medical image visualization. CUDA is a parallel programming model and software environment designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C. In this paper, we propose an optimization algorithm of volume ray casting using CUDA; compare the performance to previous implementation in old version of GPU. The experiments show that our method can achieve a high image quality and rendering speed.","1948-2922","978-1-4244-4132-7","10.1109/BMEI.2009.5304981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5304981","","Rendering (computer graphics);Image quality;Programming profession;Large-scale systems;Biomedical imaging;Visualization;Parallel programming;Software maintenance;Software performance;Software design","coprocessors;data visualisation;medical image processing;rendering (computer graphics)","real-time volume rendering;CUDA;image quality;voxel computation;medical image visualization;parallel programming model;C programming language;volume ray casting;GPU;Compute United Device Architecture;graphics processor unit","","6","","21","","30 Oct 2009","","","IEEE","IEEE Conferences"
"Shortening Design Time through Multiplatform Simulations with a Portable OpenCL Golden-model: The LDPC Decoder Case","G. Falcao; M. Owaida; D. Novo; M. Purnaprajna; N. Bellas; C. D. Antonopoulos; G. Karakonstantis; A. Burg; P. Ienne","Dept. of Electr. & Comput. Eng., Univ. of Coimbra, Coimbra, Portugal; Dept. of Comput. & Commun. Eng., Univ. of Thessaly Volos, Volos, Greece; Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland; Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland; Dept. of Comput. & Commun. Eng., Univ. of Thessaly Volos, Volos, Greece; Dept. of Comput. & Commun. Eng., Univ. of Thessaly Volos, Volos, Greece; Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland; Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland; Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland","2012 IEEE 20th International Symposium on Field-Programmable Custom Computing Machines","16 Jul 2012","2012","","","224","231","Hardware designers and engineers typically need to explore a multi-parametric design space in order to find the best configuration for their designs using simulations that can take weeks to months to complete. For example, designers of special purpose chips need to explore parameters such as the optimal bit width and data representation. This is the case for the development of complex algorithms such as Low-Density Parity-Check (LDPC) decoders used in modern communication systems. Currently, high-performance computing offers a wide set of acceleration options, that range from multicore CPUs to graphics processing units (GPUs) and FPGAs. Depending on the simulation requirements, the ideal architecture to use can vary. In this paper we propose a new design flow based on Open CL, a unified multiplatform programming model, which accelerates LDPC decoding simulations, thereby significantly reducing architectural exploration and design time. Open CL-based parallel kernels are used without modifications or code tuning on multicore CPUs, GPUs and FPGAs. We use SOpen CL (Silicon to Open CL), a tool that automatically converts Open CL kernels to RTL for mapping the simulations into FPGAs. To the best of our knowledge, this is the first time that a single, unmodified Open CL code is used to target those three different platforms. We show that, depending on the design parameters to be explored in the simulation, on the dimension and phase of the design, the GPU or the FPGA may suit different purposes more conveniently, providing different acceleration factors. For example, although simulations can typically execute more than 3× faster on FPGAs than on GPUs, the overhead of circuit synthesis often outweighs the benefits of FPGA-accelerated execution.","","978-1-4673-1605-7","10.1109/FCCM.2012.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6239819","design space exploration;simulation tools;parallel computing;FPGAs;GPUs;LDPC decoding","Kernel;Parity check codes;Field programmable gate arrays;Computational modeling;Decoding;Hardware;Algorithm design and analysis","codecs;field programmable gate arrays;logic design;logic simulation;microprocessor chips;parity check codes;public domain software","design time shortening;multiplatform simulation;portable OpenCL golden model;LDPC decoder case;hardware designers;multiparametric design space;complex algorithms;low density parity check decoders;unified multiplatform programming model;SOpen CL;Silicon to Open CL;RTL;FPGA;GPU;register transfer level","","16","","17","","16 Jul 2012","","","IEEE","IEEE Conferences"
"Accelerating error correction in high-throughput short-read DNA sequencing data with CUDA","H. Shi; B. Schmidt; W. Liu; W. Muller-Wittig","School of Computer Engineering, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798","2009 IEEE International Symposium on Parallel & Distributed Processing","10 Jul 2009","2009","","","1","8","Emerging DNA sequencing technologies open up exciting new opportunities for genome sequencing by generating read data with a massive throughput. However, produced reads are significantly shorter and more error-prone compared to the traditional Sanger shotgun sequencing method. This poses challenges for de-novo DNA fragment assembly algorithms in terms of both accuracy (to deal with short, error-prone reads) and scalability (to deal with very large input data sets). In this paper we present a scalable parallel algorithm for correcting sequencing errors in high-throughput short-read data. It is based on spectral alignment and uses the CUDA programming model. Our computational experiments on a GTX 280 GPU show runtime savings between 10 and 19 times (for different error-rates using simulated datasets as well as real Solexa/Illumina datasets).","1530-2075","978-1-4244-3751-1","10.1109/IPDPS.2009.5160924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5160924","","Acceleration;Error correction;DNA;Genomics;Bioinformatics;Throughput;Assembly;Scalability;Parallel algorithms;Runtime","biology computing;DNA;genomics;molecular biophysics;parallel algorithms","error correction;high-throughput short-read DNA sequencing data;CUDA programming model;genome sequencing;scalable parallel algorithm;GTX 280 GPU","","2","","20","","10 Jul 2009","","","IEEE","IEEE Conferences"
"Fast BFS-Based Triangle Counting on GPUs","L. Wang; J. D. Owens","University of California,Department of Computer Science,Davis,California,95616; University of California,Department of Electrical & Computer Engineering,Davis,California,95616","2019 IEEE High Performance Extreme Computing Conference (HPEC)","28 Nov 2019","2019","","","1","6","In this paper, we propose a novel method to compute triangle counting on GPUs. Unlike previous formulations of graph matching, our approach is BFS-based by traversing the graph in an all-source-BFS manner and thus can be mapped onto GPUs in a massively parallel fashion. Our implementation uses the Gunrock programming model and we evaluate our implementation in runtime and memory consumption compared with previous state-of-the-art work. We sustain a peak traversed-edgesper-second (TEPS) rate of nearly 10 GTEPS. Our algorithm is the most scalable and parallel among all existing GPU implementations and also outperforms all existing CPU distributed implementations. This work specifically focuses on leveraging our implementation on the triangle counting problem for the Subgraph Isomorphism Graph Challenge 2019, demonstrating a geometric mean speedup over the 2018 champion of 3.84×.","2643-1971","978-1-7281-5020-8","10.1109/HPEC.2019.8916434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8916434","","Graphics processing units;Memory management;Computational complexity;Parallel algorithms;Decision trees;Graph theory","computational complexity;graph theory;graphics processing units;parallel algorithms;tree searching","Subgraph Isomorphism Graph Challenge 2019;triangle counting problem;existing GPU implementations;peak traversed-edgesper-second rate;previous state-of-the-art work;memory consumption;Gunrock programming model;massively parallel fashion;all-source-BFS manner;graph matching;GPUs;fast BFS-based triangle counting","","3","","11","","28 Nov 2019","","","IEEE","IEEE Conferences"
"Ultra low latency dataflow renderer","S. Friston; A. Steed; S. Tilbury; G. Gaydadjiev","University College London, Computer Science Department, Gower Street, WC1E 6BT, UK; University College London, Computer Science Department, Gower Street, WC1E 6BT, UK; Maxeler Technologies Ltd., 1 Down Place, London W6 9JH, UK; Maxeler Technologies Ltd., 1 Down Place, London W6 9JH, UK","2015 25th International Conference on Field Programmable Logic and Applications (FPL)","8 Oct 2015","2015","","","1","4","Reconfigurable hardware has been used before for low latency image synthesis. These are typically low level implementations with tight vertical integration. For example the apparatus of both Regan et al and Ng et al had the tracker driven by the same device performing the rendering. Reconfigurable hardware combined with the dataflow programming model can make application specific rendering hardware cost effective. Our sprite renderer has comparable scope to both prior examples, but our dataflow graph can be adapted to other use cases with an effort comparable to GPU shader programming.","1946-1488","978-0-9934-2800-5","10.1109/FPL.2015.7293974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293974","","Computer architecture;Image color analysis;Hardware;Rendering (computer graphics);Streaming media;Transceivers;Sprites (computer)","graphics processing units;image processing;rendering (computer graphics)","ultra low latency dataflow renderer;low latency image synthesis;tight vertical integration;dataflow programming model;application specific rendering hardware cost;sprite renderer;dataflow graph;GPU shader programming","","","","18","","8 Oct 2015","","","IEEE","IEEE Conferences"
"A Dynamic Task-Based D3Q19 Lattice-Boltzmann Method for Heterogeneous Architectures","J. V. F. Lima; G. Freytag; V. G. Pinto; C. Schepke; P. O. A. Navaux","Universidade Federal de Santa Maria, Brazil; Universidade Federal do Rio Grande do Sul, Brazil; Universidade Federal do Rio Grande do Sul, Brazil; Universidade Federal do Pampa, Campus Alegrete, Brazil; Universidade Federal do Rio Grande do Sul, Brazil","2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)","21 Mar 2019","2019","","","108","115","Nowadays computing platforms expose a significant number of heterogeneous processing units such as multicore processors and accelerators. The task-based programming model has been a de facto standard model for such architectures since its model simplifies programming by unfolding parallelism at runtime based on data-flow dependencies between tasks. Many studies have proposed parallel strategies over heterogeneous platforms with accelerators. However, to the best of our knowledge, no dynamic task-based strategy of the Lattice-Boltzmann Method (LBM) has been proposed to exploit CPU+GPU computing nodes. In this paper, we present a dynamic task-based D3Q19 LBM implementation using three runtime systems for heterogeneous architectures: OmpSs, StarPU, and XKaapi. We detail our implementations and compare performance over two heterogeneous platforms. Experimental results demonstrate that our task-based approach attained up to 8.8 of speedup over an OpenMP parallel loop version.","2377-5750","978-1-7281-1644-0","10.1109/EMPDP.2019.8671583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8671583","","Task analysis;Runtime;Graphics processing units;Computational modeling;Parallel processing;Programming;Computer architecture","flow simulation;graphics processing units;lattice Boltzmann methods;multiprocessing systems;parallel algorithms;parallel architectures;parallel processing;parallel programming;shared memory systems","CPU+GPU computing nodes;dynamic task-based D3Q19 LBM implementation;heterogeneous architectures;heterogeneous platforms;task-based approach;significant number;heterogeneous processing units;multicore processors;accelerators;task-based programming model;facto standard model;data-flow dependencies;parallel strategies;dynamic task-based strategy;dynamic task-based D3Q19 Lattice-Boltzmann method;OmpSs;StarPU;XKaapi;OpenMP parallel loop version","","1","","27","","21 Mar 2019","","","IEEE","IEEE Conferences"
"Breaking Weak 1024-bit RSA Keys with CUDA","K. Scharfglass; D. Weng; J. White; C. Lupo","Comput. Sci. Dept., California Polytech. State Univ., San Luis Obispo, CA, USA; Comput. Sci. Dept., California Polytech. State Univ., San Luis Obispo, CA, USA; Comput. Sci. Dept., California Polytech. State Univ., San Luis Obispo, CA, USA; Comput. Sci. Dept., California Polytech. State Univ., San Luis Obispo, CA, USA","2012 13th International Conference on Parallel and Distributed Computing, Applications and Technologies","9 Sep 2013","2012","","","207","212","An exploit involving the greatest common divisor (GCD) of RSA moduli was recently discovered [1]. This paper presents a tool that can efficiently and completely compare a large number of 1024-bit RSA public keys, and identify any keys that are susceptible to this weakness. NVIDIA's graphics processing units (GPU) and the CUDA massively-parallel programming model are powerful tools that can be used to accelerate this tool. Our method using CUDA has a measured performance speedup of 27.5 compared to a sequential CPU implementation, making it a more practical method to compare large sets of keys. A computation for finding GCDs between 200,000 keys, i.e., approximately 20 billion comparisons, was completed in 113 minutes, the equivalent of approximately 2.9 million 1024-bit GCD comparisons per second.","2379-5352","978-0-7695-4879-1","10.1109/PDCAT.2012.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6589265","CUDA;RSA;greatest common divisor;parallel computation","Graphics processing units;Instruction sets;Arrays;Organizations;Public key;Matrix decomposition;Kernel","graphics processing units;parallel architectures;parallel programming;performance evaluation;public key cryptography","CUDA;greatest common divisor;GCD;RSA moduli;1024-bit RSA public keys;NVIDIA;graphics processing units;GPU;massively-parallel programming model;performance speedup;word length 1024 bit","","6","","9","","9 Sep 2013","","","IEEE","IEEE Conferences"
"Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling","M. Wu; Y. Ouyang; H. Zhou; L. Zhang; C. Liu; Y. Zhang","Southern University of Science and Technology,Shenzhen,China; Southern University of Science and Technology,Shenzhen,China; University of Texas at Dallas,Dallas,USA; University of Texas at Dallas,Dallas,USA; University of Texas at Dallas,Dallas,USA; Southern University of Science and Technology,Shenzhen,China","2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)","21 Dec 2020","2020","","","937","948","While CUDA has become a mainstream parallel computing platform and programming model for general-purpose GPU computing, how to effectively and efficiently detect CUDA synchronization bugs remains a challenging open problem. In this paper, we propose the first lightweight CUDA synchronization bug detection framework, namely Simulee, to model CUDA program execution by interpreting the corresponding LLVM bytecode and collecting the memory-access information for automatically detecting general CUDA synchronization bugs. To evaluate the effectiveness and efficiency of Simulee, we construct a benchmark with 7 popular CUDA-related projects from GitHub, upon which we conduct an extensive set of experiments. The experimental results suggest that Simulee can detect 21 out of the 24 manually identified bugs in our preliminary study and also 24 previously unknown bugs among all projects, 10 of which have already been confirmed by the developers. Furthermore, Simulee significantly outperforms state-of-the-art approaches for CUDA synchronization bug detection.","1558-1225","978-1-4503-7121-6","10.1145/3377811.3380358","National Natural Science Foundation of China(grant numbers:61902169); Shenzhen Peacock Plan(grant numbers:KQTD2016112514355531); National Science Foundation(grant numbers:CCF-1763906,CCF-1942430); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284094","","Computational modeling;Computer bugs;Graphics processing units;Benchmark testing;Synchronization;Software engineering;Software development management","graphics processing units;multiprocessing systems;parallel architectures;parallel programming;program compilers;program debugging","Simulee;CUDA program execution;memory-access information;general CUDA synchronization bugs;7 popular CUDA-related projects;24 manually identified bugs;24 previously unknown bugs;memory-access modeling;mainstream parallel computing platform;programming model;general-purpose GPU computing;challenging open problem;lightweight CUDA synchronization bug detection framework","","1","","54","","21 Dec 2020","","","IEEE","IEEE Conferences"
"Performance and Power Efficient Massive Parallel Computational Model for HPC Heterogeneous Exascale Systems","M. U. Ashraf; F. Alburaei Eassa; A. Ahmad Albeshri; A. Algarni","Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","IEEE Access","15 May 2018","2018","6","","23095","23107","The emerging high-performance computing Exascale supercomputing system, which is anticipated to be available in 2020, will unravel many scientific mysteries. This extraordinary processing framework will accomplish a thousand-folds increment in figuring power contrasted with the current Petascale framework. The prospective framework will help development communities and researchers in exploring from conventional homogeneous to the heterogeneous frameworks that will be joined into energy efficient GPU devices along with traditional CPUs. For accomplishing ExaFlops execution through the Ultrascale framework, the present innovations are confronting several challenges. Huge parallelism is one of these challenges, which requires a novel low power consuming parallel programming approach for attaining massive performance. This paper introduced a new parallel programming model that achieves massive parallelism by combining coarse-grained and fine-grained parallelism over inter-node and intranode computation respectively. The proposed framework is tri-hybrid of MPI, OpenMP, and compute unified device architecture (MOC) that compute input data over heterogeneous framework. We implemented the proposed model in linear algebraic dense matrix multiplication application, and compared the quantified metrics with well-known basic linear algebra subroutine libraries such as CUDA basic linear algebra subroutines library and KAUST basic linear algebra subprograms. MOC outperformed to all implemented methods and achieved massive performance by consuming less power. The proposed MOC approach can be considered an initial and leading model to deal emerging Exascale computing systems.","2169-3536","","10.1109/ACCESS.2018.2823299","Deanship of Scientific Research at King Abdulaziz University, Jeddah(grant numbers:RG-3-611-38); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334408","Exascale computing;HPC;massive parallelism;super computing;energy efficiency;hybrid programming;CUDA;OpenMP;MPI","Graphics processing units;Computational modeling;Parallel processing;Performance evaluation;Computer architecture;Acceleration;Parallel programming","application program interfaces;graphics processing units;mathematics computing;matrix multiplication;message passing;multiprocessing systems;parallel architectures;parallel machines;parallel programming;power aware computing","achieves massive parallelism;fine-grained parallelism;intranode computation;compute unified device architecture;compute input data;heterogeneous framework;parallel programming model;Ultrascale framework;power efficient massive parallel computational model;OpenMP;MPI;internode computation;coarse-grained parallelism;low power consuming parallel programming approach;ExaFlops execution;Petascale framework;high-performance computing Exascale supercomputing system;traditional CPU;energy efficient GPU devices;development communities;extraordinary processing framework;scientific mysteries;HPC heterogeneous Exascale systems;Exascale computing systems;initial leading model;KAUST basic linear algebra subprograms;CUDA basic linear algebra subroutines library;linear algebraic dense matrix multiplication application","","15","","65","OAPA","9 Apr 2018","","","IEEE","IEEE Journals"
"The Memory Controller Wall: Benchmarking the Intel FPGA SDK for OpenCL Memory Interface","H. R. Zohouri; S. Matsuoka","Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan","2019 IEEE/ACM International Workshop on Heterogeneous High-performance Reconfigurable Computing (H2RC)","2 Jan 2020","2019","","","11","18","Supported by their high power efficiency and recent advancements in High Level Synthesis (HLS), FPGAs are quickly finding their way into HPC and cloud systems. Large amounts of work have been done so far on loop and area optimizations for different applications on FPGAs using HLS. However, a comprehensive analysis of the behavior and efficiency of the memory controller of FPGAs is missing in literature, which becomes even more crucial when the limited memory bandwidth of modern FPGAs compared to their GPU counterparts is taken into account. In this work, we will analyze the memory interface generated by Intel FPGA SDK for OpenCL with different configurations for input/output arrays, vector size, interleaving, kernel programming model, on-chip channels, operating frequency, padding, and multiple types of overlapped blocking. Our results point to multiple shortcomings in the memory controller of Intel FPGAs, especially with respect to memory access alignment, that can hinder the programmer's ability in maximizing memory performance in their design. For some of these cases, we will provide work-arounds to improve memory bandwidth efficiency; however, a general solution will require major changes in the memory controller itself.","","978-1-7281-5999-7","10.1109/H2RC49586.2019.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945518","","Kernel;Field programmable gate arrays;Memory management;Benchmark testing;Bandwidth;Indexes;Graphics processing units","application program interfaces;cloud computing;electronic engineering computing;field programmable gate arrays;graphics processing units;high level synthesis;optimisation;power aware computing","loop;area optimizations;HLS;Intel FPGA SDK;memory access alignment;memory controller wall;OpenCL memory interface;power efficiency;high level synthesis;cloud systems;GPU;kernel programming model;on-chip channels;operating frequency","","5","","15","","2 Jan 2020","","","IEEE","IEEE Conferences"
"Space-Efficient Pointwise Computation of the Distance Transform on GPUs","N. Khan; M. Zahran","Comput. Sci. Dept., Brown Univ., Providence, RI, USA; Comput. Sci. Dept., New York Univ., New York, NY, USA","2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","3 Jul 2017","2017","","","557","566","To minimize the amount of computation, traditional approaches to calculating the distance transform (DT) on a discrete volume propagate distance values in a local neighborhood. This results in recursive dependencies across the volume, requiring the DT to be calculated for all points in the domain en mass and stored as static values in memory. On the other hand, the ability to calculate the distance transform point-wise not only offers the prospect of efficient memory usage and scalability, but also a high degree of flexibility in accommodating the unique requirements of new application domains. However, among the current DT algorithms, the computationally intensive brute-force algorithm is the only one that allows point-wise computation. We demonstrate that the by decomposing it into a map and a reduction pattern on the massively parallel architecture of a modern Graphics Processing Unit (GPU), the brute-force distance transform algorithm achieves the threefold goals of memory efficiency, flexibility, and performance. We discuss a memory constrained implementation in the CUDA parallel programming model. The flexibility of point-wise computation at runtime is demonstrated by presenting an approximate and an anisotropic variant of the standard distance transform algorithm, and using these variants for the rendering of a CT scan image. Our approach allows the distance transform to be calculated for 1024 query points and up to 16 million feature points in 141.25 milliseconds while allowing direct control over the memory working-set size. These results demonstrate the potential of pointwise computation of the DT at runtime and the need for future algorithms to incorporate this capability.","","978-1-5386-3408-0","10.1109/IPDPSW.2017.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965093","Rendering;Medical Imaging;Vision;Map;Reduction;Memory;Patterns;Anisotropic;Approximate","Transforms;Graphics processing units;Approximation algorithms;Memory management;Runtime;Parallel processing;Measurement","graphics processing units;parallel architectures","space-efficient pointwise computation;GPU;discrete volume;memory usage;DT algorithms;point-wise computation;Graphics Processing Unit;brute-force distance transform algorithm;memory efficiency;CUDA parallel programming model;standard distance transform algorithm;rendering;CT scan image;query points;memory working-set size","","","","41","","3 Jul 2017","","","IEEE","IEEE Conferences"
"Pygion: Flexible, Scalable Task-Based Parallelism with Python","E. Slaughter; A. Aiken","SLAC National Accelerator Laboratory, USA; Stanford University, USA","2019 IEEE/ACM Parallel Applications Workshop, Alternatives To MPI (PAW-ATM)","13 Apr 2020","2019","","","58","72","Dynamic languages provide the flexibility needed to implement expressive support for task-based parallel programming constructs. We present Pygion, a Python interface for the Legion task-based programming system, and show that it can provide features comparable to Regent, a statically typed programming language with dedicated support for the Legion programming model. Furthermore, we show that the dynamic nature of Python permits the implementation of several key optimizations (index launches, futures, mapping) currently implemented in the Regent compiler. Together these features enable Pygion code that is comparable in expressiveness but more flexible than Regent, and substantially more concise, less error prone, and easier to use than C++ Legion code. Pygion is designed to interoperate with Regent and can use Regent to generate high- performance CPU and GPU kernel implementations. We show that, in combination with high-performance kernels written in Regent, Pygion is able to achieve efficient, scalable execution on up to 512 nodes of the heterogeneous supercomputer Piz Daint.","","978-1-7281-5979-9","10.1109/PAW-ATM49560.2019.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9062721","task-based parallelism;Pygion;Legion;Python","Task analysis;Runtime;Python;C++ languages;Optimization;Performance analysis","data flow computing;graphics processing units;parallel machines;parallel programming;Python","scalable task-based parallelism;dynamic languages;task-based parallel program;Python interface;statically typed programming language;Legion programming model;Regent compiler;Pygion code;Legion code;GPU kernel implementations;scalable execution;Legion task-based programming","","2","","37","","13 Apr 2020","","","IEEE","IEEE Conferences"
"OpenCL Performance Prediction using Architecture-Independent Features","B. Johnston; G. Falzon; J. Milthorpe","Res. Sch. of Comput. Sci., Australian Nat. Univ., Canberra, ACT, Australia; Sch. of Sci. & Technol., Univ. of New England, Armidale, NSW, Australia; Res. Sch. of Comput. Sci., Australian Nat. Univ., Canberra, ACT, Australia","2018 International Conference on High Performance Computing & Simulation (HPCS)","1 Nov 2018","2018","","","561","569","OpenCL is an attractive programming model for heterogeneous high-performance computing systems, with wide support from hardware vendors and significant performance portability. To support efficient scheduling on HPC systems it is necessary to perform accurate performance predictions for OpenCL workloads on varied compute devices, which is challenging due to diverse computation, communication and memory access characteristics which result in varying performance between devices. The Architecture Independent Workload Characterization (AIWC) tool can be used to characterize OpenCL kernels according to a set of architecture-independent features. This work presents a methodology where AIWC features are used to form a model capable of predicting accelerator execution times. We used this methodology to predict execution times for a set of 37 computational kernels running on 15 different devices representing a broad range of CPU, GPU and MIC architectures. The predictions are highly accurate, differing from the measured experimental run-times by an average of only 1.2%, and correspond to actual execution time mispredictions of 9 ps to 1 sec according to problem size. A previously unencountered code can be instrumented once and the AIWC metrics embedded in the kernel, to allow performance prediction across the full range of modelled devices. The results suggest that this methodology supports correct selection of the most appropriate device for a previously unen- countered code, which is highly relevant to the HPC scheduling setting.","","978-1-5386-7879-4","10.1109/HPCS.2018.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8514400","","Graphics processing units;Computational modeling;Memory management;Predictive models;Kernel;Performance evaluation;Parallel processing","graphics processing units;multiprocessing systems;parallel processing;performance evaluation;scheduling","OpenCL workloads;memory access characteristics;OpenCL kernels;AIWC features;accelerator execution times;AIWC metrics;modelled devices;OpenCL performance prediction;heterogeneous high-performance computing systems;hardware vendors;HPC systems;GPU;CPU;HPC scheduling;MIC architectures;architecture independent workload characterization tool;performance portability;programming model;computational kernels","","4","","24","","1 Nov 2018","","","IEEE","IEEE Conferences"
"A CUDA-enabled Hadoop cluster for fast distributed image processing","R. Malakar; N. Vydyanathan","Corporate Research and Technology, Siemens Technology Services, Bangalore, India; Corporate Research and Technology, Siemens Technology Services, Bangalore, India","2013 National Conference on Parallel Computing Technologies (PARCOMPTECH)","7 Oct 2013","2013","","","1","5","Hadoop is a map-reduce based distributed processing framework, frequently used in the industry today, in areas of big data analysis, particularly text analysis. Graphics processing units (GPUs), on the other hand, are massively parallel platforms with attractive performance to price and power ratios, used extensively in the recent years for acceleration of data parallel computations. CUDA or Compute Unified Device Architecture is a C-based programming model proposed by NVIDIA for leveraging the parallel computing capabilities of the GPU for general purpose computations. This paper attempts to integrate CUDA acceleration into the Hadoop distributed processing framework to create a heterogeneous high performance image processing system. As Hadoop primarily is used for text analysis, this involves facilitating efficient image processing in Hadoop. Our experimental evaluations using a Adaboost based face detection algorithm indicate that CUDA-enabling a Hadoop cluster, even with low-end GPUs, can result in a 25% improvement in data processing throughput, indicating that an integration of these two technologies can help build scalable, high throughput, power and cost-efficient computing platforms.","","978-1-4799-1591-0","10.1109/ParCompTech.2013.6621392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621392","Hadoop;Map-reduce;CUDA;GPGPU","Graphics processing units;Face detection;Java;Acceleration;Throughput;Image processing;Streaming media","C language;data analysis;face recognition;graphics processing units;learning (artificial intelligence);parallel architectures;text analysis","CUDA-enabled Hadoop cluster;distributed image processing;Map-reduce based distributed processing framework;big data analysis;text analysis;graphics processing units;GPU;parallel platforms;data parallel computations;Compute Unified Device Architecture;C-based programming model;NVIDIA;general purpose computations;Hadoop distributed processing framework;heterogeneous high performance image processing system;Adaboost based face detection algorithm","","8","","19","","7 Oct 2013","","","IEEE","IEEE Conferences"
"Accelerating a 3D Finite-Difference Earthquake Simulation with a C-to-CUDA Translator","D. Unat; J. Zhou; Y. Cui; S. B. Baden; X. Cai","University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego; Simula Research Laboratory, Norway","Computing in Science & Engineering","20 Apr 2012","2012","14","3","48","59","GPUs provide impressive computing power, but GPU programming can be challenging. Here, an experience in porting real-world earthquake code to Nvidia GPUs is described. Specifically, an annotation-based programming model, called Mint, and its accompanying source-to-source translator are used to automatically generate CUDA source code and simplify the exploration of performance tradeoffs.","1558-366X","","10.1109/MCSE.2012.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188562","Code generation;optimization;emerging technologies;Earth and atmospheric sciences;scientific computing","Graphics processing unit;Instruction sets;Three dimensional displays;Optimization;Mathematical model","earthquakes;finite difference methods;graphics processing units;parallel architectures","3D finite difference earthquake simulation;GPU;annotation based programming model;source to source translator;CUDA source code","","9","","15","","20 Apr 2012","","","IEEE","IEEE Magazines"
"OpenCL programmable exposed datapath high performance low-power image signal processor","J. Multanen; H. Kultala; M. Koskela; T. Viitanen; P. Jääskelainen; J. Takala; A. Danielyan; C. Cruz","Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland; Noiseless Imaging Ltd, Finland; Noiseless Imaging Ltd, Finland","2016 IEEE Nordic Circuits and Systems Conference (NORCAS)","22 Dec 2016","2016","","","1","6","Sophisticated computational imaging algorithms require both high performance and good energy-efficiency when executed on mobile devices. Recent trend has been to exploit the abundant data-level parallelism found in general purpose programmable GPUs. However, for low-power mobile use cases, generic GPUs consume excessive amounts of power. This paper proposes a programmable computational imaging processor with 16-bit half-precision SIMD floating point vector processing capabilities combined with power efficiency of an exposed datapath. In comparison to traditional VLIW architectures with similar computational resources, the exposed datapath reduces the register file traffic and complexity. These and the specific optimizations enabled by the explicit programming model enable extremely good power-performance. When synthesized on a 28nm ASIC technology, the accelerator consumes 71mW of power while running a state-of-the-art denoising algorithm, and occupies only 0.2mm<sup>2</sup> of chip area. For the algorithm, energy usage per frame is 7mJ, which is 10x less than the best found GPU-based implementation.","","978-1-5090-1095-0","10.1109/NORCHIP.2016.7792906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792906","","Radio frequency;Registers;VLIW;Computer architecture;Signal processing algorithms;Hardware;Ports (Computers)","application specific integrated circuits;floating point arithmetic;graphics processing units;parallel processing","data-level parallelism;general purpose programmable GPU;low-power mobile use cases;programmable computational imaging processor;half-precision SIMD floating point vector processing capabilities;computational resources;exposed datapath;register file traffic;explicit programming model;ASIC technology;state-of-the-art denoising algorithm;word length 16 bit;size 28 nm;power 71 mW;energy 7 mJ","","1","","15","","22 Dec 2016","","","IEEE","IEEE Conferences"
"Automatic Mapping Single-Device OpenCL Program to Heterogeneous Multi-device Platform","D. Chen; C. Xun; D. Huang; M. Wen; C. Zhang","Nat. Key Lab. of Parallel & Distrib. Process., Nat. Univ. of Defense Technol. Changsha, Changsha, China; Nat. Key Lab. of Parallel & Distrib. Process., Nat. Univ. of Defense Technol. Changsha, Changsha, China; Nat. Key Lab. of Parallel & Distrib. Process., Nat. Univ. of Defense Technol. Changsha, Changsha, China; Nat. Key Lab. of Parallel & Distrib. Process., Nat. Univ. of Defense Technol. Changsha, Changsha, China; Nat. Key Lab. of Parallel & Distrib. Process., Nat. Univ. of Defense Technol. Changsha, Changsha, China","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","12 Jun 2014","2013","","","135","142","In this paper, we propose a framework to automatically map single-device OpenCL programs to heterogeneous multi-device platforms with performance concerns. Our framework is based on the independence of work groups which built inside the OpenCL programming model and relies heavily on the knowledge of global memory access regions of work groups. So global memory access patterns of work groups are analyzed and an abstract representation CCRwS is designed to describe the exact memory access regions of each memory access statement in the kernels. A global memory access analyzer is designed to get CCRwSs by performing static program analysis on kernel codes. Based on CCRwSs, data transfer between multiple devices and host can be fully controlled by our framework. Then a kernel code regenerator is designed to distribute the workload and perform architecture specific optimizations by code transformation. Then we tested our framework on a platform with 2 Intel E5-2650 CPUs and 4 NVIDIA Tesla C2050 GPUs. Compared with the performance on single GPU, the kernels running on all the 6 devices can achieve about 4.5x faster.","","978-0-7695-5088-6","10.1109/HPCC.and.EUC.2013.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825555","Automatic;Performance;Code transformation;multi-device","Kernel;Indexes;Performance evaluation;Benchmark testing;Abstracts;Optimization;Computer architecture","graphics processing units;parallel programming;program diagnostics","automatic mapping single-device OpenCL program;heterogeneous multi-device platform;OpenCL programming model;global memory access regions;global memory access patterns;abstract representation CCRwS;static program analysis;kernel codes;data transfer;multiple devices;code transformation;Intel E5-2650 CPU;NVIDIA Tesla C2050 GPU","","1","","10","","12 Jun 2014","","","IEEE","IEEE Conferences"
"Employing Compression Solutions under OpenACC","E. Salehi; A. Lashgar; A. Baniasadi",NA; NA; NA,"2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","4 Aug 2016","2016","","","348","356","For GPUs to achieve their peak performance, effective and efficient usage of memory bandwidth is necessary. To this end, programmers invest extensive development effort to optimize a GPU program, specially its memory bandwidth usage. The OpenACC programming model has been introduced to tackle the accelerators programming complexity. However, this model's coarse-grained control on a program can make the memory bandwidth utilization even worse than the utilization achieved under CUDA. We propose an extension to OpenACC in order to reduce the traffic on the memory interconnection network, using a compression method on floating point numbers. We examine our method on three case studies and achieve up to 1.36X speedup.","","978-1-5090-3682-0","10.1109/IPDPSW.2016.196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529890","OpenACC;Compression;Accelerators","Kernel;Graphics processing units;Bandwidth;Programming;Parallel processing;Standards;Data transfer","graphics processing units;parallel architectures;parallel programming;storage management","compression solution;memory bandwidth;GPU program;OpenACC programming model;accelerator programming complexity;coarse-grained control;CUDA;memory interconnection network;floating point number","","","","17","","4 Aug 2016","","","IEEE","IEEE Conferences"
"Fast 1-itemset frequency count using CUDA","R. L. Uy; N. Marcos","Computer Technology Department, De La Salle University, Manila, Philippines; Software Technology Department, De La Salle University, Manila, Philippines","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","210","213","Frequent itemset mining is one of the main and compute-intensive operations in the field of data mining. The said algorithm is use in finding frequent patterns in transactional databases. The 1-itemset frequent count is used as basis for finding succeeding k-itemset mining. Thus there is a need to speed-up this process. One of the techniques to speed-up the process is using the Single Instruction Multiple Thread (SIMT) architecture. This architecture allows a single instruction to be applied to multiple threads at the same time. Current graphics processing unit (GPU), which contains multiple streaming processing units, uses SIMT architecture. In order to abstract the GPU hardware from the programming model, NVIDIA introduces the compute unified device architecture (CUDA) as an extension to existing programming languages in order to support SIMT. This paper discusses how 1-itemset frequent count is implemented in SIMT using CUDA.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7847991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847991","Frequent itemset mining;CUDA programming;graphics processing unit;data mining;big data","Graphics processing units;Instruction sets;Data mining;Kernel;Itemsets;Hardware;Computer architecture","data mining;graphics processing units;parallel architectures","fast 1-itemset frequency count;CUDA;frequent itemset mining;data mining;k-itemset mining;single-instruction multiple thread architecture;SIMT architecture;graphics processing unit;GPU hardware;NVIDIA;compute unified device architecture","","1","","12","","9 Feb 2017","","","IEEE","IEEE Conferences"
"CUDA: Scalable parallel programming for high-performance scientific computing","D. Luebke","NVIDIA Corporation, U.S.A.","2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro","13 Jun 2008","2008","","","836","838","Graphics processing units (GPUs) originally designed for computer video cards have emerged as the most powerful chip in a high-performance workstation. Unlike multicore CPU architectures, which currently ship with two or four cores, GPU architectures are ""manycore"" with hundreds of cores capable of running thousands of threads in parallel. NVIDIA's CUDA is a co-evolved hardware-software architecture that enables high-performance computing developers to harness the tremendous computational power and memory bandwidth of the GPU in a familiar programming environment - the C programming language. We describe the CUDA programming model and motivate its use in the biomedical imaging community.","1945-8452","978-1-4244-2002-5","10.1109/ISBI.2008.4541126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4541126","","Parallel programming;Scientific computing;Computer architecture;Biomedical computing;Computer graphics;Workstations;Multicore processing;Central Processing Unit;Marine vehicles;Yarn","biomedical imaging;C language;computer graphic equipment;computer graphics;medical computing;parallel programming","CUDA;scalable parallel programming;high-performance scientific computing;graphics processing units;co-evolved hardware-software architecture;computational power;memory bandwidth;C programming language;biomedical imaging;GPU computing","","71","3","13","","13 Jun 2008","","","IEEE","IEEE Conferences"
"AI Drives Domain Specific Processors","Y. Kang","UNISOC Technologies Inc, Beijing, China","2018 IEEE Asian Solid-State Circuits Conference (A-SSCC)","16 Dec 2018","2018","","","13","16","In this paper we first list some of basic requirements for domain specific processors. Then we discuss several commonly used architectures for artificial intelligence domain applications. Their pros and cons are also compared. A new architecture defined as In-Cluster Coprocessor is presented which can best utilize existing memory hierarchy in a general processor and has an easy programming model. CBC has potential advantages of power saving and low cost. Further investigation on CBC is underway.","","978-1-5386-6413-1","10.1109/ASSCC.2018.8579282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579282","Domain-specific;GPU;TPU;SIMD;Vector;Cache Coherence;Cluster;Coprocessors","Computer architecture;Task analysis;Coprocessors;Graphics processing units","artificial intelligence;coprocessors","artificial intelligence domain;in-cluster coprocessor;memory hierarchy;AI drives;domain specific processors","","1","","7","","16 Dec 2018","","","IEEE","IEEE Conferences"
"Design and Experimental Evaluation of Distributed Heterogeneous Graph-Processing Systems","Y. Guo; A. L. Varbanescu; D. Epema; A. Iosup","Tech. Univ. Delft, Delft, Netherlands; Univ. of Amsterdam, Amsterdam, Netherlands; Tech. Univ. Delft, Delft, Netherlands; Tech. Univ. Delft, Delft, Netherlands","2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)","21 Jul 2016","2016","","","203","212","Graph processing is increasingly used in a variety of domains, from engineering to logistics and from scientific computing to online gaming. To process graphs efficiently, GPU-enabled graph-processing systems such as TOTEM and Medusa exploit the GPU or the combined CPU+GPU capabilities of a single machine. Unlike scalable distributed CPU-based systems such as Pregel and GraphX, existing GPU-enabled systems are restricted to the resources of a single machine, including the limited amount of GPU memory, and thus cannot analyze the increasingly large-scale graphs we see in practice. To address this problem, we design and implement three families of distributed heterogeneous graph-processing systems that can use both the CPUs and GPUs of multiple machines. We further focus on graph partitioning, for which we compare existing graph-partitioning policies and a new policy specifically targeted at heterogeneity. We implement all our distributed heterogeneous systems based on the programming model of the single-machine TOTEM, to which we add (1) a new communication layer for CPUs and GPUs across multiple machines to support distributed graphs, and (2) a workload partitioning method that uses offline profiling to distribute the work on the CPUs and the GPUs. We conduct a comprehensive real-world performance evaluation for all three families. To ensure representative results, we select 3 typical algorithms and 5 datasets with different characteristics. Our results include algorithm run time, performance breakdown, scalability, graph partitioning time, and comparison with other graph-processing systems. They demonstrate the feasibility of distributed heterogeneous graph processing and show evidence of the high performance that can be achieved by combining CPUs and GPUs in a distributed environment.","","978-1-5090-2453-7","10.1109/CCGrid.2016.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515690","Graph Processing;Distributed Heterogeneous Systems","Central Processing Unit;Graphics processing units;Programming;Partitioning algorithms;Space exploration;Arrays","computer games;engineering;graph theory;graphics processing units;logistics;parallel processing","distributed heterogeneous graph-processing systems;engineering;logistics;scientific computing;online gaming;GPU-enabled graph-processing systems;scalable distributed CPU-based systems;large-scale graphs;single-machine TOTEM;high performance computing","","2","","45","","21 Jul 2016","","","IEEE","IEEE Conferences"
"Neneta: Heterogeneous computing complex-valued neural network framework","V. Lekić; Z. Babić","Faculty of Electrical Engineering, University of Banja Luka, 78000, Bosnia and Herzegovina; Faculty of Electrical Engineering, University of Banja Luka, 78000, Bosnia and Herzegovina","2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","13 Jul 2017","2017","","","192","196","Due to increased demand for computational efficiency for the training, validation and testing of artificial neural networks, many open source software frameworks have emerged. Almost exclusively GPU programming model of choice in such software frameworks is CUDA. Symptomatic is also lack of the support for complex-valued neural networks. With our research going exactly in that direction, we developed and made publicly available yet another software framework, completely based on C++ and OpenCL standards with which we try to solve problems we identified with already existing solutions.","","978-953-233-090-8","10.23919/MIPRO.2017.7973416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973416","","Graphics processing units;Training;Neural networks;Kernel;Performance evaluation","C++ language;graphics processing units;neural nets;parallel architectures;public domain software","Neneta;heterogeneous computing;complex-valued neural network;artificial neural networks;open source software frameworks;GPU programming;CUDA;C++ language;OpenCL standards","","","","16","","13 Jul 2017","","","IEEE","IEEE Conferences"
"Parallel implementation of the modified subset sum problem in CUDA","Z. Ristovski; I. Mishkovski; S. Gramatikov; S. Filiposka","Faculty of Computer Sciences and Engineering, P.O. Box 393, 1000 Skopje, R. Macedonia; Faculty of Computer Sciences and Engineering, P.O. Box 393, 1000 Skopje, R. Macedonia; Faculty of Computer Sciences and Engineering, P.O. Box 393, 1000 Skopje, R. Macedonia; Faculty of Computer Sciences and Engineering, P.O. Box 393, 1000 Skopje, R. Macedonia","2014 22nd Telecommunications Forum Telfor (TELFOR)","9 Feb 2015","2014","","","923","926","In the recent years, computing is shifting from “central processing” on the CPU to “co-processing” on the CPU and GPU. This computing paradigm shift is due to the development of CUDA (Compute Unified Device Architecture) parallel computing architecture. CUDA is a programming model for parallel computing in Graphics Processing Units (GPUs). In this work, we have implemented parallel solution of the NP-complete modified subset sum algorithm using CUDA. With our implementation, for a certain problem size, we have obtained speedup of 20 times, compared to the CPU version.","","978-1-4799-6191-7","10.1109/TELFOR.2014.7034556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034556","CUDA;Modified subset sum algorithm;GPGPU;Parallel Speedup","Graphics processing units;Vectors;Instruction sets;Central Processing Unit;Programming;Peer-to-peer computing;Computer architecture","graphics processing units;mathematics computing;optimisation;parallel architectures;set theory","CUDA;CPU;GPU;coprocessing;central processing;computing paradigm shift;compute unified device architecture;parallel computing architecture;graphics processing units;NP-complete modified subset sum algorithm","","2","","11","","9 Feb 2015","","","IEEE","IEEE Conferences"
"Throughput-Effective On-Chip Networks for Manycore Accelerators","A. Bakhoda; J. Kim; T. M. Aamodt","ECE Dept., Univ. of British Columbia, Vancouver, BC, Canada; CS Dept., KAIST, Daejeon, South Korea; ECE Dept., Univ. of British Columbia, Vancouver, BC, Canada","2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture","20 Jan 2011","2010","","","421","432","As the number of cores and threads in manycore compute accelerators such as Graphics Processing Units (GPU) increases, so does the importance of on-chip interconnection network design. This paper explores throughput-effective network-on-chips (NoC) for future manycore accelerators that employ bulk-synchronous parallel (BSP) programming models such as CUDA and OpenCL. A hardware optimization is ""throughput-effective"" if it improves parallel application level performance per unit chip area. We evaluate performance of future looking workloads using detailed closed-loop simulations modeling compute nodes, NoC and the DRAM memory system. We start from a mesh design with bisection bandwidth balanced with off-chip demand. Accelerator workloads tend to demand high off-chip memory bandwidth which results in a many-to-few traffic pattern when coupled with expected technology constraints of slow growth in pins-per-chip. Leveraging these observations we reduce NoC area by proposing a ""checkerboard"" NoC which alternates between conventional full-routers and half-routers with limited connectivity. Checkerboard employs a new oblivious routing algorithm that maintains a minimum hop-count for architectures that place L2 cache banks at the half-router nodes. Next, we show that increasing network injection bandwidth for the large amount of read reply traffic at the nodes connected to DRAM controllers alleviates a significant fraction of the remaining imbalance resulting from the many-to-few traffic pattern. The combined effect of the above optimizations with an improved placement of memory controllers in the mesh and channel slicing improves application throughput per unit area by 25.4%.","2379-3155","978-1-4244-9071-4","10.1109/MICRO.2010.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695555","NoC;Compute accelerator;GPGPU","Bandwidth;Throughput;Computational modeling;Instruction sets;Benchmark testing;Computer architecture;Random access memory","circuit optimisation;circuit simulation;multiprocessing systems;multiprocessor interconnection networks;network routing;network-on-chip;parallel programming","manycore compute accelerator;on-chip interconnection network design;throughput-effective network-on-chip;throughput-effective NoC;bulk-synchronous parallel programming model;BSP programming model;hardware optimization;closed-loop simulation;DRAM memory system;off-chip memory bandwidth;pins-per-chip;checkerboard NoC;routing algorithm;L2 cache banks;network injection bandwidth","","94","1","49","","20 Jan 2011","","","IEEE","IEEE Conferences"
"An MDE Approach for Automatic Code Generation from UML/MARTE to OpenCL","A. W. O. Rodrigues; F. Guyomarc'h; J. Dekeyser","Federal Institute of Education, Science, and Technology of Ceará; University of Lille; University of Lille","Computing in Science & Engineering","21 Jan 2013","2013","15","1","46","55","To reduce the design complexity of OpenCL programming, the approach proposed here generates application code automatically, based on model-driven engineering (MDE) and modeling and analysis of real-time and embedded (MARTE) systems. The aim is to provide application-development resources for nonspecialists in parallel programming, exploiting concepts such as reuse and platform independence.","1558-366X","","10.1109/MCSE.2012.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6171148","model-driven engineering;MARTE;OpenCL;GPU;high-performance computing;scientific computing","Unified modeling language;Computational modeling;Computer architecture;Software engineering;Resource management;Parallel programming;Scientific computing","computational complexity;program compilers;software engineering;Unified Modeling Language","MDE approach;automatic code generation;UML/MARTE system;OpenCL programming;model-driven engineering;modeling and analysis of real-time and embedded systems;application-development resources;parallel programming;design complexity","","20","","9","","19 Mar 2012","","","IEEE","IEEE Magazines"
"Fast Motion Estimation on Graphics Hardware for H.264 Video Encoding","M. Schwalb; R. Ewerth; B. Freisleben","Dept. of Math. & Comput. Sci., Univ. of Marburg, Marburg; Dept. of Math. & Comput. Sci., Univ. of Marburg, Marburg; Dept. of Math. & Comput. Sci., Univ. of Marburg, Marburg","IEEE Transactions on Multimedia","13 Jan 2009","2009","11","1","1","10","The video coding standard H.264 supports video compression with a higher coding efficiency than previous standards. However, this comes at the expense of an increased encoding complexity, in particular for motion estimation which becomes a very time consuming task even for today's central processing units (CPU). On the other hand, modern graphics hardware includes a powerful graphics processing unit (GPU) whose computing power remains idle most of the time. In this paper, we present a GPU based approach to motion estimation for the purpose of H.264 video encoding. A small diamond search is adapted to the programming model of modern GPUs to exploit their available parallel computing power and memory bandwidth. Experimental results demonstrate a significant reduction of computation time and a competitive encoding quality compared to a CPU UMHexagonS implementation while enabling the CPU to process other encoding tasks in parallel.","1941-0077","","10.1109/TMM.2008.2008873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721593","Parallel motion estimation;H.264;GPGPU (general purpose computation on GPU);programmable graphics hardware;MPEG-4 part 10/AVC","Motion estimation;Graphics;Hardware;Encoding;Central Processing Unit;Video compression;Video coding;Parallel programming;Parallel processing;Bandwidth","computer graphic equipment;data compression;motion estimation;video coding","fast motion estimation;graphics hardware;H.264 video encoding;video compression;graphics processing unit;parallel computing","","25","2","12","IEEE","22 Dec 2008","","","IEEE","IEEE Journals"
"Porting an explicit time-domain volume-integral-equation solver on gpus with openacc [open problems in cem]","S. Feki; A. Al-Jarro; A. Clo; H. Bagci","KAUST Supercomputing Laboratory; Division of Computer, Electrical and Mathematical Sciences and Engineering, University College London, London WC1E 7JE, UK; Photonics Group, Department of Electronic and Electrical Engineering, University College London, London WC1E 7JE, UK; KAUST Research Computing, King Abdullah University of Science and Technology (KAUST), Thuwal, 23955-6900, KSA; Division of Computer, Electrical and Mathematical Sciences and Engineering, University College London, London WC1E 7JE, UK","IEEE Antennas and Propagation Magazine","17 Jun 2014","2014","56","2","265","277","Graphics processing units (GPUs) are gradually becoming mainstream in high-performance computing, as their capabilities for enhancing performance of a large spectrum of scientific applications to many fold when compared to multi-core CPUs have been clearly identified and proven. In this paper, implementation and performance-tuning details for porting an explicit marching-on-in-time (MOT)-based time-domain volume-integral-equation (TDVIE) solver onto GPUs are described in detail. To this end, a high-level approach, utilizing the OpenACC directive-based parallel programming model, is used to minimize two often-faced challenges in GPU programming: developer productivity and code portability. The MOT-TDVIE solver code, originally developed for CPUs, is annotated with compiler directives to port it to GPUs in a fashion similar to how OpenMP targets multi-core CPUs. In contrast to CUDA and OpenCL, where significant modifications to CPU-based codes are required, this high-level approach therefore requires minimal changes to the codes. In this work, we make use of two available OpenACC compilers, CAPS and PGI. Our experience reveals that different annotations of the code are required for each of the compilers, due to different interpretations of the fairly new standard by the compiler developers. Both versions of the OpenACC accelerated code achieved significant performance improvements, with up to 30× speedup against the sequential CPU code using recent hardware technology. Moreover, we demonstrated that the GPU-accelerated fully explicit MOT-TDVIE solver leveraged energy-consumption gains of the order of 3× against its CPU counterpart.","1558-4143","","10.1109/MAP.2014.6837098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6837098","Time domain volume integral equation;explicit marching-on-in-time scheme;parallel programming;parallel processing;graphics processing unit (GPU);OpenACC","Graphics processing units;Acceleration;Finite element analysis;Convolutional codes;Time-domain analysis;Programming","","","","3","","27","IEEE","17 Jun 2014","","","IEEE","IEEE Magazines"
"Speeding Up Homomorpic Hashing Using GPUs","K. Zhao; X. Chu; M. Z. Wang; Y. Jiang","Dept. of Comput. Sci., Hong Kong Baptist Univ., Hong Kong, China; Dept. of Comput. Sci., Hong Kong Baptist Univ., Hong Kong, China; NA; NA","2009 IEEE International Conference on Communications","11 Aug 2009","2009","","","1","5","Homomorphic hash functions (HHFs) have been applied into peer-to-peer networks with erasure coding or network coding to defend against pollution attacks. Unfortunately HHFs are computationally expensive for contemporary CPUs, This paper to exploit the computing power of graphic processing units (GPUs) for homomorphic hashing. Specifically, we demonstrate how to use NVIDIA GPUs and the computer unified device architecture (CUDA) programming model to achieve 38 times of speedup over the CPU counterpart. We also develop a multi-precision modular arithmetic library on CUDA platform, which is not only key to our specific application, but also very useful for a large number of cryptographic applications.","1938-1883","978-1-4244-3435-0","10.1109/ICC.2009.5199483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5199483","","Application software;Peer to peer computing;Network coding;Pollution;Computer graphics;Computer architecture;Central Processing Unit;Arithmetic;Libraries;Cryptography","computer graphics;coprocessors;cryptography;Internet;peer-to-peer computing;telecommunication security","homomorphic hash functions;NVIDIA GPU;graphic processing units;peer-to-peer networks;erasure coding;network coding;pollution attacks;contemporary CPU;computer unified device architecture programming model;cryptographic applications;Internet applications","","2","","17","","11 Aug 2009","","","IEEE","IEEE Conferences"
"CuMAPz: A tool to analyze memory access patterns in CUDA","Y. Kim; A. Shrivastava","Compiler and Microarchitecture Laboratory, Arizona State University, Tempe 85281, USA; Compiler and Microarchitecture Laboratory, Arizona State University, Tempe 85281, USA","2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)","11 Aug 2011","2011","","","128","133","CUDA programming model provides a simple interface to program on GPUs, but tuning GPGPU applications for high performance is still quite challenging. Programmers need to consider several architectural details, and small changes in source code, especially on memory access pattern, affect performance significantly. This paper presents CuMAPz, a tool to compare the memory performance of a CUDA program. CuMAPz can help programmers explore different ways of using shared and global memories, and optimize their program for memory behavior. CuMAPz models several memory effects, e.g., data reuse, global memory access coalescing, shared memory bank conflict, channel skew, and branch divergence. By using CuMAPz to explore memory access design space, we could improve the performance of our benchmarks by 62% over the naive cases, and 32% over previous approach.","85-644924","978-1-4503-0636-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5981927","GPGPU;CUDA;Memory access pattern;Performance Estimation;Analytical Model","Instruction sets;Graphics processing unit;Kernel;Estimation;Channel estimation;Correlation;Benchmark testing","computer graphic equipment;coprocessors;parallel architectures;parallel programming;storage management","CuMAPz;memory access patterns;CUDA;CUDA programming model;GPU;GPGPU;source code;memory behavior;memory effects;data reuse;global memory access coalescing;shared memory bank conflict;channel skew;branch divergence;graphics processing units","","1","","21","","11 Aug 2011","","","IEEE","IEEE Conferences"
"Legion-based scientific data analytics on heterogeneous processors","L. Yu; H. Yu",University of Nebraska-Lincoln; University of Nebraska-Lincoln,"2016 IEEE International Conference on Big Data (Big Data)","6 Feb 2017","2016","","","2305","2314","We present a study of scientific data analytics on heterogeneous architectures using the Legion runtime system. Legion is a new programming model and runtime system targeting distributed heterogeneous architectures. It introduces logical regions as a new abstraction for describing the structures and usages of program data. We describe how to leverage logical regions to express important properties of program data, such as locality and independence, for scientific data analytics that can consist of multiple operations with different data types. Our approach can help users simplify programming on the data partition, data organization, and data movement for distributed-memory heterogeneous architectures, thereby facilitating a simultaneous execution of multiple analytics operations on modern and future supercomputers. We demonstrate the scalability and the usability of our approach by a hybrid data partitioning and distribution scheme for different data types using both CPUs and GPUs on a heterogeneous system.","","978-1-4673-9005-7","10.1109/BigData.2016.7840863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840863","scientific data analytics;heterogeneous processors;Legion","Program processors;Programming;Computer architecture;Data analysis;Supercomputers;Runtime;Parallel processing","data analysis;parallel programming;scientific information systems","legion-based scientific data analytics;heterogeneous processors;Legion runtime system;Legion programming model;logical regions;program data structures;program data usages;logical region leveraging;program data locality property;program data independence property;data partitioning;data organization;data movement;distributed-memory heterogeneous architectures;hybrid data partitioning-and-distribution scheme;CPU;GPU","","","","33","","6 Feb 2017","","","IEEE","IEEE Conferences"
"Intermediate-Level Synthesis of a Gauss-Jordan Elimination Linear Solver","M. Daigneault; J. P. David","Ecole Polytech. de Montreal, Univ. de Montreal, Montreal, QC, Canada; Ecole Polytech. de Montreal, Univ. de Montreal, Montreal, QC, Canada","2015 IEEE International Parallel and Distributed Processing Symposium Workshop","1 Oct 2015","2015","","","176","181","As the world of computing goes more and more parallel, reconfigurable computing can enable interesting compromises in terms of processing speed and power consumption between CPUs and GPUs. Yet, from a developer's perspective, programming Field-Programmable Gate Arrays to implement application specific processors still represents a significant challenge. In this paper, we present the application of an Intermediate-Level Synthesis methodology to the design of a Gauss-Jordan elimination linear solver on FPGA. The ILS methodology takes for input a language offering an Algorithmic-State Machine programming model. Each ASM handles blocking and non-blocking connections between data-synchronized channels having streaming interfaces with implicit ready-to-send/receive signals. Using our compiler, a scalable linear solver design reaching as much as 46.2 GFLOPS was designed and tested in a matter of days, showing how the ILS methodology can enable an interesting design time/performance compromise between RTL and HLS methodologies.","","978-1-4673-7684-6","10.1109/IPDPSW.2015.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284306","","Hardware;Field programmable gate arrays;Synchronization;Data transfer;Algorithm design and analysis;Coprocessors;Clocks","field programmable gate arrays;graphics processing units","Gauss-Jordan elimination linear solver;parallel computing;reconfigurable computing;processing speed;power consumption;CPU;GPU;programming field-programmable gate arrays;application specific processors;intermediate-level synthesis methodology;FPGA;algorithmic-state machine programming model;nonblocking connections;data-synchronized channels;streaming interfaces;compiler;scalable linear solver design","","1","","15","","1 Oct 2015","","","IEEE","IEEE Conferences"
"Efficient Compilation and Execution of JVM-Based Data Processing Frameworks on Heterogeneous Co-Processors","C. Kotselidis; S. Diamantopoulos; O. Akrivopoulos; V. Rosenfeld; K. Doka; H. Mohammed; G. Mylonas; V. Spitadakis; W. Morgan","The University of Manchester; Exus Ltd.; SparkWorks ITC Ltd.; German Research Center for Artificial Intelligence; National Technical University of Athens; Kaleao Ltd.; Computer Technology Institute & Press Diophantus; Neurocom,Luxembourg; IProov Ltd.","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)","15 Jun 2020","2020","","","175","179","This paper addresses the fundamental question of how modern Big Data frameworks can dynamically and transparently exploit heterogeneous hardware accelerators. After presenting the major challenges that have to be addressed towards this goal, we describe our proposed architecture for automatic and transparent hardware acceleration of Big Data frameworks and applications. Our vision is to retain the uniform programming model of Big Data frameworks and enable automatic, dynamic Just-In-Time compilation of the candidate code segments that benefit from hardware acceleration to the corresponding format. In conjunction with machine learning-based device selection, that respect user-defined constraints (e.g., cost, time, etc.), we enable dynamic code execution on GPUs and FPGAs transparently to the user. In addition, we dynamically re-steer execution at runtime based on the availability of resources. Our preliminary results demonstrate that our approach can accelerate an existing Apache Flink application by up to 16.5x.","1558-1101","978-3-9819263-4-7","10.23919/DATE48585.2020.9116246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116246","","Europe;Automation;Task analysis;Artificial intelligence;Presses;Data processing;Engines","Big Data;coprocessors;field programmable gate arrays;graphics processing units;Java;learning (artificial intelligence);parallel programming;program compilers;virtual machines","modern Big Data frameworks;heterogeneous hardware accelerators;automatic hardware acceleration;transparent hardware acceleration;uniform programming model;just-in-time compilation;candidate code segments;machine learning-based device selection;dynamic code execution;heterogeneous coprocessors;JVM-based data processing frameworks;GPU;FPGA","","","","35","","15 Jun 2020","","","IEEE","IEEE Conferences"
"Optimizing sparse matrix-vector multiplication on CUDA","Zhuowei Wang; Xianbin Xu; Wuqing Zhao; Yuping Zhang; Shuibing He","school of computer, wuhan university, China; school of computer, wuhan university, China; school of computer, wuhan university, China; school of computer, wuhan university, China; school of computer, wuhan university, China","2010 2nd International Conference on Education Technology and Computer","29 Jul 2010","2010","4","","V4-109","V4-113","In recent years, GPUs have attracted the attention of many application developers as powerful massively parallel system. CUDA as a general purpose parallel computing architecture make GPUs an appealing choice to solve many complex computational problems in a more efficient way. In this paper, we discuss implementing optimizing spare matrix-vector multiplication on NVIDIA GPUs using CUDA programming model. We outline three optimizations include: (1) optimized CSR storage format, (2) optimized threads mapping, and (3) avoiding divergence judgment. We experimentally evaluate our optimizations on GeForce 9600 GTX, connect to Windows xp 64-bit system. In comparison with NVIDIA's SpMV library and NVIDIA's CUDDPA library, the results show that optimizing sparse matrix-vector multiplication on CUDA achieves better performance than other SpMV implementations.","2155-1812","978-1-4244-6370-1","10.1109/ICETC.2010.5529724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5529724","GPUs;CUDA;SpMV;NVIDIA's SpMV library;NVIDIA's CUDDPA library","Sparse matrices;Concurrent computing;Libraries;Educational institutions;Graphics processing unit;Parallel processing;Parallel programming;Kernel;Computer science education","microprocessor chips;parallel processing;performance evaluation","sparse matrix-vector multiplication;powerful massively parallel system;general purpose parallel computing architecture;complex computational problem;NVIDIA GPU;CUDA programming model;optimized CSR storage format;optimized thread mapping;GeForce 9600 GTX;Windows xp 64-bit system","","5","","12","","29 Jul 2010","","","IEEE","IEEE Conferences"
"A Comparative Evaluation of Parallel Programming Models for Shared-Memory Architectures","L. M. Sanchez; J. Fernandez; R. Sotomayor; J. D. Garcia","Comput. Sci. Dept., Univ. Carlos III de Madrid, Leganés, Spain; Comput. Sci. Dept., Univ. Carlos III de Madrid, Leganés, Spain; Comput. Sci. Dept., Univ. Carlos III de Madrid, Leganés, Spain; Comput. Sci. Dept., Univ. Carlos III de Madrid, Leganés, Spain","2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications","23 Aug 2012","2012","","","363","370","Nowadays, most computers that are commercially available off-the-shelf (COTS) include hardware features that increase the performance of parallel general-purpose threads (hyper threading, multicore, ccNUMA architectures) or SIMD kernels (CPU vector instructions, GPUs). The purpose of this paper is to perform a compared evaluation of several parallel programming models where each one is fitted to exploit some of these features but also each one requires a different level of programming skills. Four parallel programming models (OpenMP, Intel TBB, Intel ArBB, and CUDA) have been selected. The idea is to cover a wide spectrum of programming models and most of the parallel hardware features included in modern computers. On one hand, OpenMP and TBB platforms, that exploits parallel threads running on multicore systems. On the other hand, ArBB, that combines multicore parallel threads and multicore SIMD features with a simpler programming model, and CUDA that exploits SIMD features of the GPU hardware. Our results obtained with the benchmarks used on this paper suggest that OpenMP and TBB have a lower performance compared to ArBB and CUDA. But also that ArBB performance tends to be comparable with CUDA performance in most cases (although it is normally lower). Thus, there are evidences that a careful designed top range multicore and multisocket architecture, can be comparable in terms of performance with top range GPU cards for many applications, with the advantage of a simpler programming model.","2158-9208","978-1-4673-1631-6","10.1109/ISPA.2012.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6280314","GPGPU;Paralel computing;SIMD;Multicore","Computer architecture;Graphics processing unit;Parallel processing;Instruction sets;Computers;Benchmark testing;Programming","memory architecture;multi-threading;parallel architectures;performance evaluation;shared memory systems","parallel programming models;shared-memory architectures;commercially available off-the-shelf;COTS;parallel general-purpose threads;hyper threading;ccNUMA architectures;SIMD kernels;CPU vector instructions;GPU cards;programming skills;OpenMP;Intel TBB;Intel ArBB;CUDA;multicore SIMD features;multicore parallel threads;multisocket architecture","","","","28","","23 Aug 2012","","","IEEE","IEEE Conferences"
"Enabling an OpenCL Compiler for Embedded Multicore DSP Systems","J. Li; C. Kuan; T. Wu; J. K. Lee","Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan","2012 41st International Conference on Parallel Processing Workshops","25 Oct 2012","2012","","","545","552","OpenCL is an industry's attempt to unify heterogeneous multicore programming. With its programming model defining SPMD kernels, vector types, and address space qualifiers, OpenCL allows programmers to exploit data parallelism with multicore processors and SIMD instructions as well as data locality with memory hierarchy. Recently, OpenCL has gained success on many architectures, including multicore CPUs, GPUs, vector processors, embedded systems with application-specific processors, and even FPGAs. However, how to support OpenCL for embedded multicore DSP systems remains unaddressed. In this paper, we illustrate our OpenCL support for embedded multicore DSP systems. Our target platform consists of one MPU and a DSP subsystem with multiple DSPs. The DSPs we address are VLIW processors with clustered functional units and distributed register files. To generate efficient code for such DSPs, compilers are required to consider irregular register file access in many optimization phases. To utilize the DSPs with distributed register files, we propose a cluster-aware work-item dispatching scheme to vectorize OpenCL kernels and assign independent workload to clusters of a DSP. In addition, we also incorporate several optimizations to enable efficient DSP code generation. In our experiments, we employ a set of OpenCL benchmark programs to evaluate the effectiveness of our OpenCL support. The experiments are conducted on a DSP cycle-accurate simulator and a multicore evaluation board. We report average 29% performance improvement with our vectorization scheme and a near 2-fold speedup with two DSPs compared with a single-MPU setup.","2332-5690","978-1-4673-2509-7","10.1109/ICPPW.2012.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337524","","Digital signal processing;Kernel;Vectors;Registers;Program processors;Multicore processing;VLIW","digital signal processing chips;electronic engineering computing;embedded systems;field programmable gate arrays;graphics processing units;multiprocessing systems;operating system kernels;optimising compilers;parallel processing;program compilers;software performance evaluation","OpenCL compiler;embedded multicore DSP systems;heterogeneous multicore programming;programming model;SPMD kernels;vector types;address space qualifiers;data parallelism;multicore processors;SIMD instructions;data locality;memory hierarchy;multicore CPU;GPU;vector processors;embedded systems;application-specific processors;FPGA;OpenCL support;MPU subsystem;DSP subsystem;VLIW processors;clustered functional units;distributed register files;compilers;irregular register file access;optimization phases;cluster-aware work-item dispatching scheme;OpenCL kernels;independent workload;DSP code generation;OpenCL benchmark programs;DSP cycle-accurate simulator;multicore evaluation board;performance improvement;vectorization scheme;single-MPU setup","","7","","15","","25 Oct 2012","","","IEEE","IEEE Conferences"
"OpenACC Cache Directive: Opportunities and Optimizations","A. Lashgar; A. Baniasadi",NA; NA,"2016 Third Workshop on Accelerator Programming Using Directives (WACCPD)","2 Feb 2017","2016","","","46","56","OpenACC's programming model presents a simple interface to programmers, offering a trade-off between performance and development effort. OpenACC relies on compiler technologies to generate efficient code and optimize for performance. Among the difficult to implement directives, is the cache directive. The cache directive allows the programmer to utilize accelerator's hardware- or software-managed caches by passing hints to the compiler. In this paper, we investigate the implementation aspect of cache directive under NVIDIA-like GPUs and propose optimizations for the CUDA backend. We use CUDA's shared memory as the software-managed cache space. We first show that a straightforward implementation can be very inefficient, and downgrade performance. We investigate the differences between this implementation and hand-written CUDA alternatives and introduce the following optimizations to bridge the performance gap between the two: i) improving occupancy by sharing the cache among several parallel threads and ii) optimizing cache fetch and write routines via parallelization and minimizing control flow. We present compiler passes to apply these optimizations. Investigating three test cases, we show that the best cache directive implementation can perform very close to hand-written CUDA equivalent and improve performance up to 2.18X (compared to the baseline OpenACC.).","","978-1-5090-6152-5","10.1109/WACCPD.2016.009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836580","OpenACC;Cache memory;CUDA;Software-managed cache;Performance","Indexes;Graphics processing units;Arrays;Optimization;Programming;Acceleration;Hardware","cache storage;multi-threading;parallel architectures;program compilers;shared memory systems","open accelerator;control flow minimization;parallelization;write routines;cache fetch;parallel threads;software-managed cache space;CUDA shared memory;CUDA backend;NVIDIA-like GPU;accelerator hardware;code generation;compiler technologies;programmer interface;OpenACC programming model;OpenACC cache directive","","4","","13","","2 Feb 2017","","","IEEE","IEEE Conferences"
"Flexible Linear Algebra Development and Scheduling with Cholesky Factorization","A. Haidar; A. YarKhan; C. Cao; P. Luszczek; S. Tomov; J. Dongarra","Univ. of Tennessee, Knoxville, TN, USA; Univ. of Tennessee, Knoxville, TN, USA; Univ. of Tennessee, Knoxville, TN, USA; Univ. of Tennessee, Knoxville, TN, USA; Univ. of Tennessee, Knoxville, TN, USA; Univ. of Tennessee, Knoxville, TN, USA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","30 Nov 2015","2015","","","861","864","Modern high performance computing environments are composed of networks of compute nodes that often contain a variety of heterogeneous compute resources, such as multicore CPUs and GPUs. One challenge faced by domain scientists ishow to efficiently use all these distributed, heterogeneous resources. Inorder to use the GPUs effectively, the workload parallelism needs to be muchgreater than the parallelism for a multicore-CPU. Additionally, effectivelyusing distributed memory nodes brings out another level of complexity where theworkload must be carefully partitioned over the nodes. In this work we areusing a lightweight runtime environment to handle many of the complexities insuch distributed, heterogeneous systems. The runtime environment usestask-superscalar concepts to enable the developer to write serial code whileproviding parallel execution. The task-programming model allows the developerto write resource-specialization code, so that each resource gets theappropriate sized workload-grain. Our task-programming abstraction enables thedeveloper to write a single algorithm that will execute efficiently across the distributed heterogeneous machine. We demonstrate the effectiveness of ourapproach with performance results for dense linear algebra applications, specifically the Cholesky factorization.","","978-1-4799-8937-9","10.1109/HPCC-CSS-ICESS.2015.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336271","Cholesky factorization;accelerator-based distributed memory computers;superscalar dataflow scheduling;heterogeneous HPC computing","Multicore processing;Runtime;Hardware;Graphics processing units;Scalability;Linear algebra;Parallel processing","distributed memory systems;graphics processing units;mathematics computing;matrix decomposition;parallel processing;resource allocation;scheduling","flexible linear algebra development;flexible linear algebra scheduling;Cholesky factorization;high performance computing environments;compute nodes;heterogeneous compute resources;distributed resources;GPU;workload parallelism;multicore-CPU;distributed memory nodes;task-superscalar concept;serial code;parallel execution;task-programming model;resource-specialization code;task-programming abstraction;distributed heterogeneous machine","","1","","20","","30 Nov 2015","","","IEEE","IEEE Conferences"
"Portable data-parallel visualization and analysis in distributed memory environments","C. Sewell; L. Lo; J. Ahrens","CCS-7, Los Alamos National Laboratory, USA; CCS-7, Los Alamos National Laboratory, USA; CCS-7, Los Alamos National Laboratory, USA","2013 IEEE Symposium on Large-Scale Data Analysis and Visualization (LDAV)","2 Dec 2013","2013","","","25","33","Data-parallelism is a programming model that maps well to architectures with a high degree of concurrency. Algorithms written using data-parallel primitives can be easily ported to any architecture for which an implementation of these primitives exists, making efficient use of the available parallelism on each. We have previously published results demonstrating our ability to compile the same data-parallel code for several visualization algorithms onto different on-node parallel architectures (GPUs and multi-core CPUs) using our extension of NVIDIA's Thrust library. In this paper, we discuss our extension of Thrust to support concurrency in distributed memory environments across multiple nodes. This enables the application developer to write data-parallel algorithms while viewing the data as single, long vectors, essentially without needing to explicitly take into consideration whether the values are actually distributed across nodes. Our distributed wrapper for Thrust handles the communication in the backend using MPI, while still using the standard Thrust library to take advantage of available on-node parallelism. We describe the details of our distributed implementations of several key data-parallel primitives, including scan, scatter/gather, sort, reduce, and upper/lower bound. We also present two higher-level distributed algorithms developed using these primitives: isosurface and KD-tree construction. Finally, we provide timing results demonstrating the ability of these algorithms to take advantage of available parallelism on nodes and across multiple nodes, and discuss scaling limitations for communication-intensive algorithms such as KD-tree construction.","","978-1-4799-1659-7","10.1109/LDAV.2013.6675155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6675155","","Program processors;Vectors;Algorithm design and analysis;Indexes;Isosurfaces;Computer architecture;Parallel processing","concurrency control;data analysis;data visualisation;distributed memory systems;parallel algorithms","portable data-parallel visualization;portable data-parallel analysis;distributed memory environments;programming model;concurrency degree;data-parallel primitives;GPU;graphics processing unit;multicore CPU;on-node parallel architectures;visualization algorithms;NVIDIA Thrust library;data-parallel algorithms;MPI;message passing interface;Thrust library;scan primitives;scatter-gather primitives;sort primitives;reduce primitives;upper-lower bound primitives;isosurface primitives;KD-tree construction primitives;distributed algorithms","","5","","23","","2 Dec 2013","","","IEEE","IEEE Conferences"
"Hybrid MPI/OpenMP/OpenACC Implementations for the Solution of Convection-Diffusion Equations with the HOPMOC Method","F. L. Cabral; C. Osthoff; M. Kischinhevsky; D. Brandão","Laboratοrio Nac. de Computacno Cienc., Centro de Computacno de Alto Desempenho, Petrόpolis, Brazil; Laboratοrio Nac. de Computacno Cienc., Centro de Computacno de Alto Desempenho, Petrόpolis, Brazil; Inst. de Comput., Univ. Fed. Fluminense, Niterόi, Brazil; Centro Fed. de Educacno Tecnolοgica, CEFET, Colegiado de Inf., Nova Iguaçu, Brazil","2014 14th International Conference on Computational Science and Its Applications","6 Dec 2014","2014","","","196","199","The need for fast solution of large scientific and industrial problems has long motivated the quest for improvements both in software as well as in hardware, since the inception of computing tools. In this context, vectorization, parallelization of tasks have been important strategies for the improvement of hardware efficiency during the last decades. Operator splitting techniques for the numerical solution of partial differential equations are also an attempt towards the same goal, on the software side. This work presents two parallel implementations of the Hopmoc method to solve parabolic equations with convective dominance on a cluster with multiple multicore nodes or GPUs. The Hopmoc method is based both on the modified method of characteristics and the Hopscotch method. It is implemented through an explicit-implicit operator splitting technique. Hopmoc has been studied on distributed memory machines under MPI. In this work Hopmoc is implemented on clusters of multiple cores or GPUs in one single programming model. Previous results had shown that Hopmoc is a scalable parallel procedure with respect to distributed memory machines. New numerical results of the technique presented herein show performance improvements of up to 300 times when compared with the sequential version.","","978-1-4799-4264-0","10.1109/ICCSA.2014.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976686","Parallel computing;Hopmoc method;Convection-diffusion equation","Mathematical model;Equations;Multicore processing;Message systems;Graphics processing units;Parallel processing","application program interfaces;convection;distributed memory systems;graphics processing units;mathematics computing;message passing;parabolic equations;partial differential equations;public domain software","Hybrid MPI-OpenMP-OpenACC implementation;HOPMOC Method;convection-diffusion equation solution;software improvement;computing tools;task parallelization;task vectorization;hardware efficiency improvement;numerical solution;partial differential equations;parallel implementations;Hopmoc method;parabolic equations;convective dominance;multiple multicore nodes;GPU;explicit-implicit operator splitting technique;distributed memory machines;multiple core clusters;programming model;scalable parallel procedure;performance improvement;numerical analysis","","6","","13","","6 Dec 2014","","","IEEE","IEEE Conferences"
"A Comparative Study of SYCL, OpenCL, and OpenMP","H. C. da Silva; F. Pisani; E. Borin","Inst. of Comput., Univ. of Campinas, Campinas, Brazil; Inst. of Comput., Univ. of Campinas, Campinas, Brazil; Inst. of Comput., Univ. of Campinas, Campinas, Brazil","2016 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW)","9 Jan 2017","2016","","","61","66","Recent trends indicate that future computing systems will be composed by a group of heterogeneous computing devices, including CPUs, GPUs, and other hardware accelerators. These devices provide increased processing performance, however, creating efficient code for them may require that programmers manage memory assignments and use specialized APIs, compilers, or runtime systems, thus making their programs dependent on specific tools. In this scenario, SYCL is an emerging C++ programming model for OpenCL that allows developers to write code for heterogeneous computing devices that are compatible with standard C++ compilation frameworks. In this paper, we analyze the performance and programming characteristics of SYCL, OpenMP, and OpenCL using both a benchmark and a real-world application. Our performance results indicate that programs that rely on available SYCL runtimes are not on par with the ones based on OpenMP and OpenCL yet. Nonetheless, the gap is getting smaller if we consider the results reported by previous studies. In terms of programmability, SYCL presents itself as a competitive alternative to OpenCL, requiring fewer lines of code to implement kernels and also fewer calls to essential API functions and methods.","","978-1-5090-4844-1","10.1109/SBAC-PADW.2016.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7803697","SYCL;OpenCL;OpenMP;parallel programming;performance evaluation;programmability evaluation","Benchmark testing;Programming;C++ languages;Performance evaluation;Kernel;MOS devices;Program processors","application program interfaces;C++ language;message passing;parallel programming;program compilers","API methods;API functions;programmability;standard C++ compilation frameworks;C++ programming model;runtime systems;compilers;hardware accelerators;GPU;CPU;heterogeneous computing devices;OpenMP;OpenCL;SYCL","","13","","16","","9 Jan 2017","","","IEEE","IEEE Conferences"
"Comparing performance and energy efficiency of FPGAs and GPUs for high productivity computing","B. Betkaoui; D. B. Thomas; W. Luk","Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom","2010 International Conference on Field-Programmable Technology","6 Jan 2011","2010","","","94","101","This paper provides the first comparison of performance and energy efficiency of high productivity computing systems based on FPGA (Field-Programmable Gate Array) and GPU (Graphics Processing Unit) technologies. The search for higher performance compute solutions has recently led to great interest in heterogeneous systems containing FPGA and GPU accelerators. While these accelerators can provide significant performance improvements, they can also require much more design effort than a pure software solution, reducing programmer productivity. The CUDA system has provided a high productivity approach for programming GPUs. This paper evaluates the High-Productivity Reconfigurable Computer (HPRC) approach to FPGA programming, where a commodity CPU instruction set architecture is augmented with instructions which execute on a specialised FPGA co-processor, allowing the CPU and FPGA to co-operate closely while providing a programming model similar to that of traditional software. To compare the GPU and FPGA approaches, we select a set of established benchmarks with different memory access characteristics, and compare their performance and energy efficiency on an FPGA-based Hybrid-Core system with a GPU-based system. Our results show that while GPUs excel at streaming applications, high-productivity reconfigurable computing systems outperform GPUs in applications with poor locality characteristics and low memory bandwidth requirements.","","978-1-4244-8983-1","10.1109/FPT.2010.5681761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681761","","Benchmark testing;Field programmable gate arrays;Coprocessors;Graphics processing unit;Programming;Kernel;Instruction sets","computer graphic equipment;coprocessors;field programmable gate arrays;reconfigurable architectures","high productivity computing system;field-programmable gate array;graphics processing unit;CUDA system;high-productivity reconfigurable computer approach;FPGA programming;CPU instruction set architecture;FPGA-based hybrid-core system;GPU-based system","","37","","27","","6 Jan 2011","","","IEEE","IEEE Conferences"
"A Light-weight API for Portable Multicore Programming","C. G. Baker; M. A. Heroux; H. C. Edwards; A. B. Williams","Comp. Eng. & Energy Sci., Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Scalable Algorithms, Sandia Nat. Labs., Albuquerque, NM, USA; Comput. Simulation Infrastruct., Sandia Nat. Labs., Albuquerque, NM, USA; Comput. Simulation Infrastruct., Sandia Nat. Labs., Albuquerque, NM, USA","2010 18th Euromicro Conference on Parallel, Distributed and Network-based Processing","22 Apr 2010","2010","","","601","606","Multicore nodes have become ubiquitous in just a few years. At the same time, writing portable parallel software for multicore nodes is extremely challenging. Widely available programming models such as OpenMP and Pthreads are not useful for devices such as graphics cards, and more flexible programming models such as RapidMind are only available commercially. OpenCL represents the first truly portable standard, but its availability is limited. In the presence of such transition, we have developed a minimal application programming interface (API) for multicore nodes that allows us to write portable parallel linear algebra software that can use any of the aforementioned programming models and any future standard models. We utilize C++ template meta-programming to enable users to write parallel kernels that can be executed on a variety of node types, including Cell, GPUs and multicore CPUs. The support for a parallel node is provided by implementing a Node object, according to the requirements specified by the API. This ability to provide custom support for particular node types gives developers a level of control not allowed by the current slate of proprietary parallel programming APIs. We demonstrate implementations of the API for a simple vector dot-product on sequential CPU, multicore CPU and GPU nodes.","2377-5750","978-1-4244-5673-4","10.1109/PDP.2010.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5452412","Parallel programming;multicore architectures","Multicore processing;Parallel programming;Linear programming;Writing;Graphics;Application software;Linear algebra;Software standards;Standards development;Kernel","application program interfaces;C++ language;linear algebra;mathematics computing;metacomputing;multiprocessing systems;parallel programming","portable multicore programming;multicore nodes;application programming interface;portable parallel linear algebra software;programming model;C++ template meta-programming;parallel kernels;multicore CPU;node object;parallel programming;vector dot-product;sequential CPU;GPU node","","7","","9","","22 Apr 2010","","","IEEE","IEEE Conferences"
"Implementation of XcalableMP Device Acceleration Extention with OpenCL","T. Nomizu; D. Takahashi; J. Lee; T. Boku; M. Sato","Grad. Sch. of Syst. & Inf. Eng., Univ. of Tsukuba, Tsukuba, Japan; Center for Comput. Sci., Univ. of Tsukuba, Tsukuba, Japan; Grad. Sch. of Syst. & Inf. Eng., Univ. of Tsukuba, Tsukuba, Japan; Fac. of Eng., Inf. & Syst., Univ. of Tsukuba, Tsukuba, Japan; Center for Comput. Sci., Univ. of Tsukuba, Tsukuba, Japan","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","20 Aug 2012","2012","","","2394","2403","Due to their outstanding computational performance, many acceleration devices, such as GPUs, the Cell Broadband Engine (Cell/B.E.), and multi-core computing are attracting a lot of attention in the field of high-performance computing. Although there are many programming models and languages de-signed for programming accelerators, such as CUDA, AMD Accelerated Parallel Processing (AMD APP), and OpenCL, these models remain difficult and complex. Furthermore, when programming for accelerator-enhanced clusters, we have to use an inter-node programming interface, such as MPI to coordinate the nodes. In order to address these problems and reduce complexity, an extension to XcalableMP (XMP), a PGAS language, for use on accelerator-enhanced clusters, called XcalableMP Device Acceleration Extension (XMP-dev), is proposed. In XMP-dev, a global distributed data is mapped onto distributed memory of each accelerator, and a fragment of codes can be of-floaded to execute in a set of accelerators. It eliminates the complex programming between nodes and accelerators and between nodes. In this paper, we present an implementation of the XMP-dev runtime library with the OpenCL APIs, while the previous implementation targets CUDA-only. Since OpenCL is a standardized interface supported for various kinds of accelerators, it improves the portability of XMP-dev and reduces the cost of development. In the result of performance evaluation, we show that the OpenCL implementation of XMP-dev can generate portable programs that can run on not only NVIDIA GPU-enhanced clusters but also various accelerator-enhanced clusters.","","978-1-4673-0974-5","10.1109/IPDPSW.2012.296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270611","Cluster;Accelerator;OpenCL","Graphics processing unit;Acceleration;Kernel;Programming;Synchronization;Arrays","application program interfaces;graphics processing units;message passing;multiprocessing systems;parallel architectures;parallel programming","computational performance;acceleration device;Cell Broadband Engine;Cell/BE;multicore computing;high-performance computing;programming model;programming language;programming accelerator;CUDA;AMD Accelerated Parallel Processing;AMD APP;accelerator-enhanced cluster;internode programming interface;MPI;node coordination;complexity reduction;PGAS language;XcalableMP Device Acceleration Extension;global distributed data mapping;distributed memory;code fragment;XMP-dev runtime library;OpenCL API;standardized interface;XMP-dev portability;performance evaluation;OpenCL implementation;NVIDIA GPU-enhanced cluster","","5","","20","","20 Aug 2012","","","IEEE","IEEE Conferences"
"Automatic Resource Scheduling with Latency Hiding for Parallel Stencil Applications on GPGPU Clusters","K. Maeda; M. Murase; M. Doi; H. Komatsu; S. Noda; R. Himeno","IBM Res. - Tokyo, IBM Japan, Ltd., Tokyo, Japan; IBM Res. - Tokyo, IBM Japan, Ltd., Tokyo, Japan; Syst. & Technol. Group, IBM Japan, Ltd., Tokyo, Japan; IBM Res. - Tokyo, IBM Japan, Ltd., Tokyo, Japan; Adv. Center for Comput. & Commun., RIKEN, Wako, Japan; Adv. Center for Comput. & Commun., RIKEN, Wako, Japan","2012 IEEE 26th International Parallel and Distributed Processing Symposium","16 Aug 2012","2012","","","544","556","Overlapping computations and communication is a key to accelerating stencil applications on parallel computers, especially for GPU clusters. However, such programming is a time-consuming part of the stencil application development. To address this problem, we developed an automatic code generation tool to produce a parallel stencil application with latency hiding automatically from its dataflow model. With this tool, users visually construct the workflows of stencil applications in a dataflow programming model. Our dataflow compiler determines a data decomposition policy for each application, and generates source code that overlaps the stencil computations and communication (MPI and PCIe). We demonstrate two types of overlapping models, a CPU-GPU hybrid execution model and a GPU-only model. We use a CFD benchmark computing 19-point 3D stencils to evaluate our scheduling performance, which results in 1.45 TFLOPS in single precision on a cluster with 64 Tesla C1060 GPUs.","1530-2075","978-1-4673-0975-2","10.1109/IPDPS.2012.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6267857","latency hiding;network embedding;resource scheduling;stencil computations","Kernel;Graphics processing unit;Rivers;Hardware;Peer to peer computing;Computational modeling;Jacobian matrices","graphics processing units;parallel processing;partial differential equations;processor scheduling;search problems","automatic resource scheduling;latency hiding;parallel stencil applications;GPGPU Clusters;parallel computers;automatic code generation tool;dataflow programming model;dataflow compiler;data decomposition;source code generation;PDE;partial differential equations","","","","26","","16 Aug 2012","","","IEEE","IEEE Conferences"
"Coordinated static and dynamic cache bypassing for GPUs","X. Xie; Y. Liang; Y. Wang; G. Sun; T. Wang","Center for Energy-Efficient Computing and Applications, School of EECS, Peking University, China; Center for Energy-Efficient Computing and Applications, School of EECS, Peking University, China; Tsinghua National Laboratory for Information Science and Technology, Department of EE, Tsinghua University, China; Center for Energy-Efficient Computing and Applications, School of EECS, Peking University, China; Center for Energy-Efficient Computing and Applications, School of EECS, Peking University, China","2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)","9 Mar 2015","2015","","","76","88","The massive parallel architecture enables graphics processing units (GPUs) to boost performance for a wide range of applications. Initially, GPUs only employ scratchpad memory as on-chip memory. Recently, to broaden the scope of applications that can be accelerated by GPUs, GPU vendors have used caches in conjunction with scratchpad memory as on-chip memory in the new generations of GPUs. Unfortunately, GPU caches face many performance challenges that arise due to excessive thread contention for cache resource. Cache bypassing, where memory requests can selectively bypass the cache, is one solution that can help to mitigate the cache resource contention problem. In this paper, we propose coordinated static and dynamic cache bypassing to improve application performance. At compile-time, we identify the global loads that indicate strong preferences for caching or bypassing through profiling. For the rest global loads, our dynamic cache bypassing has the flexibility to cache only a fraction of threads. In CUDA programming model, the threads are divided into work units called thread blocks. Our dynamic bypassing technique modulates the ratio of thread blocks that cache or bypass at run-time. We choose to modulate at thread block level in order to avoid the memory divergence problems. Our approach combines compile-time analysis that determines the cache or bypass preferences for global loads with run-time management that adjusts the ratio of thread blocks that cache or bypass. Our coordinated static and dynamic cache bypassing technique achieves up to 2.28X (average I.32X) performance speedup for a variety of GPU applications.","2378-203X","978-1-4799-8930-0","10.1109/HPCA.2015.7056023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056023","","Instruction sets;Graphics processing units;Synchronization;Kernel;Pipelines;Arrays;System-on-chip","cache storage;graphics processing units;multi-threading;parallel architectures","coordinated static cache bypassing;coordinated dynamic cache bypassing;GPUs;parallel architecture;graphics processing units;scratchpad memory;on-chip memory;thread contention;cache resource contention problem;CUDA programming model;thread blocks;dynamic bypassing technique;memory divergence problems;compile-time analysis;bypass preferences;run-time management","","80","1","46","","9 Mar 2015","","","IEEE","IEEE Conferences"
"Performance characterization of the NAS Parallel Benchmarks in OpenCL","S. Seo; G. Jo; J. Lee","Center for Manycore Programming, School of Computer Science and Engineering, Seoul National University, 151-744, Korea; Center for Manycore Programming, School of Computer Science and Engineering, Seoul National University, 151-744, Korea; Center for Manycore Programming, School of Computer Science and Engineering, Seoul National University, 151-744, Korea","2011 IEEE International Symposium on Workload Characterization (IISWC)","29 Dec 2011","2011","","","137","148","Heterogeneous parallel computing platforms, which are composed of different processors (e.g., CPUs, GPUs, FPGAs, and DSPs), are widening their user base in all computing domains. With this trend, parallel programming models need to achieve portability across different processors as well as high performance with reasonable programming effort. OpenCL (Open Computing Language) is an open standard and emerging parallel programming model to write parallel applications for such heterogeneous platforms. In this paper, we characterize the performance of an OpenCL implementation of the NAS Parallel Benchmark suite (NPB) on a heterogeneous parallel platform that consists of general-purpose CPUs and a GPU. We believe that understanding the performance characteristics of conventional workloads, such as the NPB, with an emerging programming model (i.e., OpenCL) is important for developers and researchers to adopt the programming model. We also compare the performance of the NPB in OpenCL to that of the OpenMP version. We describe the process of implementing the NPB in OpenCL and optimizations applied in our implementation. Experimental results and analysis show that the OpenCL version has different characteristics from the OpenMP version on multicore CPUs and exhibits different performance characteristics depending on different OpenCL compute devices. The results also indicate that the application needs to be rewritten or re-optimized for better performance on a different compute device although OpenCL provides source-code portability.","","978-1-4577-2064-2","10.1109/IISWC.2011.6114174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114174","","Kernel;Graphics processing unit;Multicore processing;Indexes;Computational modeling;Optimization","graphics processing units;high level languages;microprocessor chips;parallel programming","NAS parallel benchmarks;OpenCL;heterogeneous parallel computing platforms;CPU;GPU;FPGA;DSP;parallel programming models;Open Computing Language;open standard;parallel applications;heterogeneous platforms;NAS Parallel Benchmark suite;NPB;heterogeneous parallel platform;source-code portability","","99","","36","","29 Dec 2011","","","IEEE","IEEE Conferences"
"The Exploration of Pervasive and Fine-Grained Parallel Model Applied on Intel Xeon Phi Coprocessor","C. Calvin; F. Ye; S. Petiton","DM2S Commissariat a l'Energie Atomique, CEA, France; DM2S Commissariat a l'Energie Atomique, CEA, France; Lab. d'Inf. Fondamentale de Lille, Univ. de Lille 1, Lille, France","2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","12 Dec 2013","2013","","","166","173","In this paper we investigate the dissimilar multithreading programming paradigms on x86 CPU architectures, where the recently released Intel Xeon Phi Coprocessor and commonly used Intel Xeon processors were studied, as well as the NVIDIA K20 GPU, which represents the cutting-edge general purpose graphics processing unit. The relevant numerical algorithm selected to address the problem is power method, which is widely used to compute the dominant eigenvalue of a matrix. This work focuses on dense linear algebra. The frequently used multi-core or many-core processor parallelism techniques include OpenMP, Intel Cilk Plus, Intel Threading Building Blocks, i.e. TBB, along with the optimized computing libraries such as Intel Math Kernel Library(MKL) or the NVIDIA CUDA Basic Linear Algebra Subroutines(cuBLAS) library. Optimized implementations of these techniques were separately applied to the aforementioned architectures. For the reason that a unitary programming model may not satisfy the growing performance demand, we also explored some possible mix of these languages. The study shows that the hybrid pattern of multithreading and data parallelism via explicit vectorization maximizes the performance on x86 architectures, which allows us to obtain 80% of the sustainable peak performance in double precision on the Intel Many Integrated Core(MIC) Architecture. In the case of single precision, this number reaches even 96%. In addition, this approach enables a reasonable performance by requiring least developing time. The numbers of iterations till convergence are roughly the same in both architectures of CPU and GPU. The GPU performs better in small matrix sizes. However, the Intel Xeon Phi coprocessor excels for large sizes with a better scalability.","","978-0-7695-5094-7","10.1109/3PGCIC.2013.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681224","","Coprocessors;Vectors;Kernel;Graphics processing units;Arrays;Computational modeling","graphics processing units;mathematics computing;matrix algebra;multiprocessing systems;multi-threading;ubiquitous computing","fine-grained parallel model;pervasive model;dissimilar multithreading programming paradigms;Intel Xeon Phi coprocessor;NVIDIA K20 GPU;dense linear algebra;many-core processor parallelism techniques;OpenMP;Building Blocks;Intel Cilk Plus;Intel Threading;optimized computing libraries;Intel Math Kernel Library;NVIDIA CUDA basic linear algebra subroutines library;multithreading;data parallelism;iteration method;multicore processor parallelism techniques;general purpose graphics processing unit","","3","","18","","12 Dec 2013","","","IEEE","IEEE Conferences"
"TARCAD: A template architecture for reconfigurable accelerator designs","M. Shafiq; M. Pericàs; N. Navarro; E. Ayguadé","Computer Sciences, Barcelona Supercomputing Center, Spain; Computer Sciences, Barcelona Supercomputing Center, Spain; Dept. Arquitectura de Computadors, Universitat Politècnica de Catalunya, Barcelona, Spain; Computer Sciences, Barcelona Supercomputing Center, Spain","2011 IEEE 9th Symposium on Application Specific Processors (SASP)","7 Jul 2011","2011","","","8","15","In the race towards computational efficiency, accelerators are achieving prominence. Among the different types, accelerators built using reconfigurable fabric, such as FPGAs, have a tremendous potential due to the ability to customize the hardware to the application. However, the lack of a standard design methodology hinders the adoption of such devices and makes the portability and reusability across designs difficult. In addition, generation of highly customized circuits does not integrate nicely with high level synthesis tools. In this work, we introduce TARCAD, a template architecture to design reconfigurable accelerators. TARCAD enables high customization in the data management and compute engines while retaining a programming model based on generic programming principles. The template provides generality and scalable performance over a range of FPGAs. We describe the template architecture in detail and show how to implement five important scientific kernels: MxM, Acoustic Wave Equation, FFT, SpMV and Smith Waterman. TARCAD is compared with other High Level Synthesis models and is evaluated against GPUs, a well-known architecture that is far less customizable and, therefore, also easier to target from a simple and portable programming model. We analyze the TARCAD template and compare its efficiency on a large Xilinx Virtex-6 device to that of several recent GPU studies.","","978-1-4577-1213-5","10.1109/SASP.2011.5941071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941071","","Layout;Kernel;Computer architecture;Field programmable gate arrays;Hardware design languages;Monitoring;Registers","field programmable gate arrays;high level synthesis;reconfigurable architectures","TARCAD;template architecture;reconfigurable accelerator design;reconflgurable fabric;data management;programming model;generic programming;FPGA;acoustic wave equation;Xilinx Virtex-6 device;high level synthesis model","","1","","25","","7 Jul 2011","","","IEEE","IEEE Conferences"
"Meta-programming and Multi-stage Programming for GPGPUs","I. Masliah; M. Baboulin; J. Falcou","Univ. Paris Sud, Orsay, France; Univ. Paris Sud, Orsay, France; Univ. Paris Sud, Orsay, France","2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)","8 Dec 2016","2016","","","369","376","GPGPUs and other accelerators are becoming a mainstream asset for high-performance computing. Raising the programmability of such hardware is essential to enable users to discover, master and subsequently use accelerators in day-to-day simulations. Furthermore, tools for high-level programming of parallel architectures are becoming a great way to simplify the exploitation of such systems. For this reason, we have extended NT2 - the Numerical Template Toolbox - a C++ scientific computing library which can generate code for SIMD and multithreading systems in a transparent way using a MATLAB like syntax. In this paper, we study how to introduce an accelerator based programming model into this library to allow developers to reap the benefits of such an architecture. After a brief description of the NT2 framework, we explain how our accelerator programming model has been designed and integrated in a pure C++ library. We conclude by showing the applicability and performance of this tool on some practical applications.","","978-1-5090-3531-1","10.1109/MCSoC.2016.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774460","GPU;Multi-stage;C++","Libraries;C++ languages;Graphics processing units;Programming;Computational modeling;MATLAB;Mathematical model","C++ language;graphics processing units;high level languages;parallel architectures","meta programming;multistage programming;GPGPUs;high-performance computing;accelerators;high-level programming;parallel architectures;numerical template toolbox;C++ scientific library computing;SIMD;multithreading systems;MATLAB","","","","28","","8 Dec 2016","","","IEEE","IEEE Conferences"
"SOLAR: Services-Oriented Deep Learning Architectures-Deep Learning as a Service","C. Wang; L. Gong; X. Li; Q. Yu; A. Wang; P. Hung; X. Zhou","University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; Business and Information Technology, University of Ontario Institute of Technology, Oshawa, ON, Canada; University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Services Computing","4 Feb 2021","2021","14","1","262","273","Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space exploration. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability.","1939-1374","","10.1109/TSC.2017.2777478","NSFC(grant numbers:61379040); Anhui Provincial NSF(grant numbers:1608085QF12); Suzhou Research Foundation(grant numbers:SYG201625); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2017497); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119814","Services-oriented architecture;deep learning;neural network;accelerator","Machine learning;Hardware;Service-oriented architecture;Computer architecture;Field programmable gate arrays;Training","field programmable gate arrays;learning (artificial intelligence);service-oriented architecture","hardware implementation;hardware accelerators;FPGA board;SOLAR;machine learning;large scale data size;flexible performance implementations;high performance implementations;deep learning neural networks;services-oriented deep learning architecture;uniform programming model;deep learning as a service","","2","","35","IEEE","24 Nov 2017","","","IEEE","IEEE Journals"
"Design consideration of Network Intrusion detection system using Hadoop and GPGPU","S. R. Bandre; J. N. Nandimath","Department of Computer Engineering, Smt. Kashibai Navale College of Engineering, Affiliated to Savitribai Phule Pune University, India; Department of Computer Engineering, Smt.Kashibai Navale College of Engineering, Affiliated to Savitribai Phule Pune University, India","2015 International Conference on Pervasive Computing (ICPC)","16 Apr 2015","2015","","","1","6","Modern computing has primarily shifted towards the distributed environment using commodity resources which results in increase in data and its security concern. This paper deals with design consideration of Network Intrusion Detection System (NIDS) based on the Hadoop framework and acceleration of its performance by using General Purpose Graphical Processing Unit (GPGPU). The large volume of data from an entire infrastructure is assigned to Hadoop framework and intrusion detections are carried out on GPGPU. This approach improves NIDS performance and it enables to provide quick response to various attacks on the network. In order to perform the general purposed computation on the GPU, NVidia provides the Compute Unified Device Architecture (CUDA) which is a parallel programming model which performs high-end complex operations using GPU. In order to process large volumes of data in distributed networks, Hadoop framework has to configure with various supporting ecosystems like Flume, Pig, Hive and HBase. These ecosystems enable the Hadoop framework to handle streaming data on the network and large log files on servers. The proposed system is capable of performing analytics over intrusion pattern and their behavior on the network, which helps a network administrator to configure network security policy and settings. Analytics over intrusion is done by using a Score-Weight approach called as Pattern Frequency Inverse Cluster Frequency (PF-ICF). The design consideration of accelerated NIDS is a solution towards the performance issues of various NIDS that faces due to the large volumes of the network traffic.","","978-1-4799-6272-3","10.1109/PERVASIVE.2015.7087201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087201","CUDA;GPGPU;Hadoop;Network Security;NIDS","Graphics processing units;Intrusion detection;Servers;Ecosystems;Algorithm design and analysis;Telecommunication traffic","data analysis;graphics processing units;parallel architectures;parallel programming;security of data","network intrusion detection system;GPGPU;Hadoop framework;general purpose graphical processing unit;NIDS;NVidia;Compute Unified Device Architecture;CUDA;parallel programming model;Flume;Pig;Hive;HBase;streaming data handling;log files;intrusion pattern analytics;network security policy;score-weight approach;pattern frequency inverse cluster frequency;PF-ICF;network traffic","","8","","15","","16 Apr 2015","","","IEEE","IEEE Conferences"
"An Efficient Acceleration of Symmetric Key Cryptography Using General Purpose Graphics Processing Unit","F. Wu; C. -h. Chen; H. Narang","Comput. Sci. Dept., Tuskegee Univ., Tuskegee, AL, USA; Comput. Sci. Dept., Tuskegee Univ., Tuskegee, AL, USA; Comput. Sci. Dept., Tuskegee Univ., Tuskegee, AL, USA","2010 Fourth International Conference on Emerging Security Information, Systems and Technologies","11 Nov 2010","2010","","","228","233","Graphics Processing Units (GPU) have been the extensive research topic in recent years and have been successfully applied to general purpose applications other than computer graphical area. The nVidia CUDA programming model provides a straightforward means of describing inherently parallel computations. In this paper, we present a study of the efficiency of emerging technology in applying General Purpose Graphics Processing Units (GPGPU) in high performance symmetric key cryptographic solutions. We implemented symmetric key cryptography algorithm using the novel CUDA platform on nVidia Geforce 280 GTX and compared its performance with an optimized CPU implementation on a high-end AMD Opteron Dual Core CPU. Our experimental results show that GPGPU can perform as an efficient cryptographic accelerator and the developed GPU based implementation achieve a significant performance improvement over CPU based implementation and the maximum observed speedups are about 100 times.","2162-2116","978-1-4244-7517-9","10.1109/SECURWARE.2010.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633744","Symmetric Key Cryptography;High Performance Computation;Purpose Graphics Processing Unit;CUDA","Graphics processing unit;Cryptography;Kernel;Instruction sets;Computer architecture;Central Processing Unit","computer graphic equipment;coprocessors;cryptography","efficient acceleration;symmetric key cryptography;general purpose graphics processing unit;computer graphical area;nVidia CUDA programming model;parallel computations;GPGPU;performance symmetric key cryptographic solutions;nVidia Geforce 280 GTX","","1","","12","","11 Nov 2010","","","IEEE","IEEE Conferences"
"A Sparse Matrix Personality for the Convey HC-1","K. K. Nagar; J. D. Bakos","Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA; Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA","2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines","19 May 2011","2011","","","1","8","In this paper we describe a double precision floating point sparse matrix-vector multiplier (SpMV) and its performance as implemented on a Convey HC-1 reconfigurable computer. The primary contributions of this work are a novel streaming reduction architecture for floating point accumulation, a novel on-chip cache optimized for streaming compressed sparse row (CSR) matrices, and end-to-end integration with the HC-1's system, programming model, and runtime environment. The design is composed of 32 parallel processing elements, each connected to the HC-1's coprocessor memory and each containing a streaming multiply-accumulator and local vector cache. When used on the HC-1, each PE has a peak throughput of 300 double precision MFLOP/s, giving a total peak throughput of 9.6 GFLOPS/s. For our test matrices, we demonstrate up to 40% of the peak performance and compare these results with results obtained using the CUSparse library on an NVIDIA Tesla S1070 GPU. In most cases our implementation exceeds the performance of the GPU.","","978-1-61284-277-6","10.1109/FCCM.2011.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771239","floating point accumulation;reduction;reconfigurable computing;sparse matrix;SpMV","Adders;Coprocessors;Arrays;Sparse matrices;Field programmable gate arrays;Pipelines","cache storage;coprocessors;floating point arithmetic;matrix multiplication;multiplying circuits;parallel processing;reconfigurable architectures;sparse matrices","sparse matrix personality;double precision floating point sparse matrix-vector multiplier;Convey HC-1 reconfigurable computer;streaming reduction architecture;floating point accumulation;on-chip cache;compressed sparse row matrix;end-to-end integration;HC-1 system;programming model;runtime environment;parallel processing;HC-1 coprocessor memory;multiply-accumulator;local vector cache;CUSparse library","","29","","21","","19 May 2011","","","IEEE","IEEE Conferences"
"Extending OpenACC for Efficient Stencil Code Generation and Execution by Skeleton Frameworks","A. D. Pereira; M. Castro; M. A. R. Dantas; R. C. O. Rocha; L. F. W. Góes","Fed. Univ. of Santa Catarina, Florianopolis, Brazil; Fed. Univ. of Santa Catarina, Florianopolis, Brazil; Fed. Univ. of Santa Catarina, Florianopolis, Brazil; Univ. of Edinburgh, Edinburgh, UK; Pontifical Catholic Univ. of Minas Gerais, Belo Horizonte, Brazil","2017 International Conference on High Performance Computing & Simulation (HPCS)","14 Sep 2017","2017","","","719","726","The OpenACC programming model simplifies the programming for accelerator devices such as GPUs. Its abstract accelerator model defines a least common denominator for accelerator devices, thus it cannot represent architectural specifics of these devices without losing portability. Therefore, this general- purpose approach delivers good performance on average, but it misses optimization opportunities for code generation and execution of specific classes of applications. In this paper, we propose OpenACC extensions to enable efficient code generation and execution of stencil applications by parallel skeleton frameworks such as PSkel. Our results show that our stencil extensions may improve the performance of OpenACC in up to 28% and 45% on GPU and CPU, respectively. Moreover, we show that the work-partitioning mechanism offered by the skeleton framework, which splits the computation across CPU and GPU, may improve even further the performance of the applications in up to 18%.","","978-1-5386-3250-5","10.1109/HPCS.2017.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8035149","stencil;skeleton frameworks;source-to-source compilation;CUDA;OpenACC","Graphics processing units;Skeleton;Programming;Jacobian matrices;Optimization;Computer architecture;Runtime","graphics processing units;parallel programming;program compilers;software performance evaluation","OpenACC programming model;accelerator devices;abstract accelerator model;OpenACC extensions;stencil applications;parallel skeleton frameworks;stencil extensions;efficient stencil code generation;least common denominator;general-purpose approach;optimization;code execution;performance improvement;work-partitioning","","4","","27","","14 Sep 2017","","","IEEE","IEEE Conferences"
"Processing of synthetic Aperture Radar data with GPGPU","C. Clemente; M. di Bisceglie; M. Di Santo; N. Ranaldo; M. Spinelli","Università degli Studi del Sannio, Piazza Roma 21, 82100 Benevento, Italy; Università degli Studi del Sannio, Piazza Roma 21, 82100 Benevento, Italy; Università degli Studi del Sannio, Piazza Roma 21, 82100 Benevento, Italy; Università degli Studi del Sannio, Piazza Roma 21, 82100 Benevento, Italy; Università degli Studi del Sannio, Piazza Roma 21, 82100 Benevento, Italy","2009 IEEE Workshop on Signal Processing Systems","17 Nov 2009","2009","","","309","314","Synthetic aperture radar processing is a complex task that involves advanced signal processing techniques and intense computational effort. While the first issue has now reached a mature stage, the question of how to produce accurately focused images in real-time, without mainframe facilities, is still under debate. The recent introduction of general-purpose graphic processing units seems to be quite promising in this view, especially for the decreased per-core cost barrier and for the affordable programming complexity. The authors explain, in this work, the main computational features of a range-Doppler Synthetic Aperture Radar (SAR) processor, trying to disclose the degree of parallelism in the operations at the light of the CUDA programming model. Given the extremely flexible structure of the Single Instruction Multiple Threads (SIMT) model, the authors show that the optimization of a SAR processing unit cannot reduce to an FFT optimization, although this is a quite extensively used kernel. Actually, it is noticeable that the most significant advantage is obtained in the range cell migration correction kernel where a complex interpolation stage is performed very efficiently exploiting the SIMT model. Performance show that, using a single Nvidia Tesla-C1060 GPU board, the obtained processing time is more than fifteen time better than our test workstation.","2162-3570","978-1-4244-4335-2","10.1109/SIPS.2009.5336272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336272","Synthetic Aperture Radar;parallel processing;GPU;CUDA","Synthetic aperture radar;Kernel;Radar signal processing;Focusing;Graphics;Costs;Concurrent computing;Parallel processing;Parallel programming;Flexible structures","computer graphic equipment;Doppler radar;interpolation;synthetic aperture radar","synthetic aperture radar;GPGPU;general-purpose graphic processing unit;range-Doppler SAR;single instruction multiple thread model","","15","1","13","","17 Nov 2009","","","IEEE","IEEE Conferences"
"Accelerated solution of stiffness matrix for isoparametric elements based on CUDA","H. Binxing; L. Xinguo; Q. Hao; L. Zenghao","Shaanxi Aerospace Flight Vehicle Design Key Laboratory, Northwestern Polytechnical University, Xi'an, PR China; Shaanxi Aerospace Flight Vehicle Design Key Laboratory, Northwestern Polytechnical University, Xi'an, PR China; Shaanxi Aerospace Flight Vehicle Design Key Laboratory, Northwestern Polytechnical University, Xi'an, PR China; School of Astronautics, Northwestern Polytechnical University, Xi'an, PR China","2017 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","1 Jan 2018","2017","","","1","4","High precision results in structural with the shortest time consumption are expected when methods are introduced to solve FEM(Finite element method). Solving of stiffness matrix assembled by isoparametric elements and solving the assembled stiffness matrix are the most time-consuming. In the previous serial algorithms, there is always a time limitation for some applications and it is hard to achieve. However, break-through in programming and feasibility of general-purpose applications executed on GPU (Graph Processing Unit), such as the parallel computing platform and programming model CUDA (Compute Unified Device Architecture) released by NVIDIA corporation, make it possible for some large scale FEM explicit dynamic simulation in real time. Authors present an approach to accelerate calculation in element stiffness matrices, taking the three-dimensional hexahedral isoparametric element in different scale as an example. A speedup of about 15 times is achieved with respect to parallel algorithms using boost on CPU. The results show that the parallel algorithm based on CUDA can satisfy the fast simulation of finite element model of structural problems with certain computational scale.","","978-1-5386-3142-3","10.1109/ICSPCC.2017.8242497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8242497","FEM;CUDA;Conjugate gradient;Parallel Computing;GPU acceleration","Finite element analysis;Computers;Graphics processing units;Indexes;Shape;Hardware","finite element analysis;graphics processing units;mathematics computing;matrix algebra;parallel algorithms;parallel architectures","general-purpose applications;finite element method;graph processing unit;large scale FEM explicit dynamic simulation;assembled stiffness matrix;shortest time consumption;isoparametric elements;accelerated solution;finite element model;CUDA;parallel algorithm;three-dimensional hexahedral isoparametric element;element stiffness matrices;Compute Unified Device Architecture;parallel computing platform","","2","","7","","1 Jan 2018","","","IEEE","IEEE Conferences"
"HeTM: Transactional Memory for Heterogeneous Systems","D. Castro; P. Romano; A. Ilic; A. M. Khan","Universidade de Lisboa, Portugal; Universidade de Lisboa, Portugal; Universidade de Lisboa, Portugal; UiT The Arctic University of Norway, Norway","2019 28th International Conference on Parallel Architectures and Compilation Techniques (PACT)","7 Nov 2019","2019","","","232","244","Modern heterogeneous computing architectures, which couple multi-core CPUs with discrete many-core GPUs (or other specialized hardware accelerators), enable unprecedented peak performance and energy efficiency levels. However, developing applications that can take full advantage of the potential of heterogeneous systems is a notoriously hard task. This work takes a step towards reducing the complexity of programming heterogeneous systems by introducing the abstraction of Heterogeneous Transactional Memory (HeTM). HeTM provides programmers with the illusion of a single memory region, shared among the CPUs and the (discrete) GPU(s) of a heterogeneous system, with support for atomic transactions. Besides introducing the abstract semantics and programming model of HeTM, we present the design and evaluation of a concrete implementation of the proposed abstraction, referred herein as Speculative HeTM (SHeTM). SHeTM makes use of a novel design that leverages speculative techniques, which aims at hiding the inherently large communication latency between CPUs and discrete GPUs and at minimizing inter-device synchronization overhead. We demonstrate the efficiency of the SHeTM via an extensive quantitative study based both on synthetic benchmarks and on a popular object caching system.","2641-7936","978-1-7281-3613-4","10.1109/PACT.2019.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8891063","transaction;memory;CPU;GPU;heterogeneous;computing;system","Synchronization;Graphics processing units;Programming;Performance evaluation;Computer architecture;Task analysis","cache storage;graphics processing units;multiprocessing systems;parallel programming;power aware computing","Heterogeneous Transactional Memory;single memory region;Speculative HeTM;modern heterogeneous computing architectures;couple multicore CPUs;many-core GPUs;energy efficiency levels;heterogeneous systems programming;object caching system;SHeTM","","","","59","","7 Nov 2019","","","IEEE","IEEE Conferences"
"A Comparison of Performance Tunabilities between OpenCL and OpenACC","M. Sugawara; S. Hirasawa; K. Komatsu; H. Takizawa; H. Kobayashi","Tohoku Univ., Sendai, Japan; Tohoku Univ., Sendai, Japan; Tohoku Univ., Sendai, Japan; Tohoku Univ., Sendai, Japan; Tohoku Univ., Sendai, Japan","2013 IEEE 7th International Symposium on Embedded Multicore Socs","11 Nov 2013","2013","","","147","152","To design and develop any auto tuning mechanisms for OpenACC, it is important to clarify the differences between conventional GPU programming models and OpenACC in terms of available programming and tuning techniques, called performance tunabilities. This paper hence discusses the performance tunabilities of OpenACC and OpenCL. As OpenACC cannot synchronize threads running on GPUs, some important techniques are not available to OpenACC. Therefore, we also design an additional compiler directive for thread synchronization. Evaluation results show that both OpenCL and OpenACC need architecture-aware optimizations, and similar approaches to performance optimization are effective for both OpenCL and OpenACC. The additional directive can allow OpenACC to describe more tuning techniques in the same approach as OpenCL. As it is obvious that OpenACC is more productive than OpenCL especially for legacy application migration, OpenACC is a very promising programming model if it can achieve the same performance as the conventional GPU programming models such as CUDA and OpenCL.","","978-0-7695-5086-2","10.1109/MCSoC.2013.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657921","Autotuning;OpenCL;OpenACC","Instruction sets;Kernel;Graphics processing units;Optimization;Programming;Synchronization;Data transfer","graphics processing units;parallel architectures;software maintenance","performance tunabilities;OpenCL;OpenACC;auto tuning mechanisms;GPU programming models;compiler directive;thread synchronization;architecture-aware optimizations;performance optimization;legacy application migration;CUDA","","8","","15","","11 Nov 2013","","","IEEE","IEEE Conferences"
"Exploring Programming Multi-GPUs Using OpenMP and OpenACC-Based Hybrid Model","R. Xu; S. Chandrasekaran; B. Chapman","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum","31 Oct 2013","2013","","","1169","1176","Heterogeneous computing come with tremendous potential and is a leading candidate for scientific applications that are becoming more and more complex. Accelerators such as GPUs whose computing momentum is growing faster than ever offer application performance when compute intensive portions of an application are offloaded to them. It is quite evident that future computing architectures are moving towards hybrid systems consisting of multi-GPUs and multi-core CPUs. A variety of high-level languages and software tools can simplify programming these systems. Directive-based programming models are being embraced since they not only ease programming complex systems but also abstract low-level details from the programmer. We already know that OpenMP has been making programming CPUs easy and portable. Similarly, a directive-based programming model for accelerators is OpenACC that is gaining popularity since the directives play an important role in developing portable software for GPUs. A combination of OpenMP and OpenACC, a hybrid model, is a plausible solution to port scientific applications to heterogeneous architectures especially when there is more than one GPU on a single node to port an application to. However OpenACC meant for accelerators is yet to provide support for multi-GPUs. But using OpenMP we could conveniently exploit features such as for and section to distribute compute intensive kernels to more than one GPU. We demonstrate the effectiveness of this hybrid approach with some case studies in this paper.","","978-0-7695-4979-8","10.1109/IPDPSW.2013.263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651003","GPUs;OpenACC;OpenMP","Graphics processing units;Kernel;Programming;Computational modeling;Computer architecture;Instruction sets;Data transfer","application program interfaces;graphics processing units;high level languages;multiprocessing systems;parallel programming;software tools","heterogeneous architectures;portable software;directive-based programming models;software tools;high-level languages;multicore CPU;heterogeneous computing;OpenMP;OpenACC-based hybrid model;multi-GPU","","11","","20","","31 Oct 2013","","","IEEE","IEEE Conferences"
"A Comprehensive Performance Comparison of CUDA and OpenCL","J. Fang; A. L. Varbanescu; H. Sips","Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands; Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands; Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands","2011 International Conference on Parallel Processing","17 Oct 2011","2011","","","216","225","This paper presents a comprehensive performance comparison between CUDA and OpenCL. We have selected 16 benchmarks ranging from synthetic applications to real-world ones. We make an extensive analysis of the performance gaps taking into account programming models, ptimization strategies, architectural details, and underlying compilers. Our results show that, for most applications, CUDA performs at most 30% better than OpenCL. We also show that this difference is due to unfair comparisons: in fact, OpenCL can achieve similar performance to CUDA under a fair comparison. Therefore, we define a fair comparison of the two types of applications, providing guidelines for more potential analyses. We also investigate OpenCL's portability by running the benchmarks on other prevailing platforms with minor modifications. Overall, we conclude that OpenCL's portability does not fundamentally affect its performance, and OpenCL can be a good alternative to CUDA.","2332-5690","978-1-4577-1336-1","10.1109/ICPP.2011.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047190","Performance Comparison;CUDA;OpenCL","Graphics processing unit;Benchmark testing;Programming;Performance evaluation;Computational modeling;Kernel","benchmark testing;computer graphic equipment;coprocessors;multiprocessing systems;parallel architectures;parallel programming;parallelising compilers","performance comparison;CUDA;performance gap;programming model;optimization strategy;architectural detail;compilers;OpenCL portability;NVIDIA GPU","","121","","30","","17 Oct 2011","","","IEEE","IEEE Conferences"
"Hippogriff: Efficiently moving data in heterogeneous computing systems","Y. Liu; H. -W. Tseng; M. Gahagan; J. Li; Y. Jin; S. Swanson","Department of Computer Science and Engineering, University of California, San Diego, La Jolla, U.S.A.; Department of Computer Science and Engineering, University of California, San Diego, La Jolla, U.S.A.; Department of Computer Science and Engineering, University of California, San Diego, La Jolla, U.S.A.; Department of Computer Science and Engineering, University of California, San Diego, La Jolla, U.S.A.; Department of Computer Science and Engineering, University of California, San Diego, La Jolla, U.S.A.; Department of Computer Science and Engineering, University of California, San Diego, La Jolla, U.S.A.","2016 IEEE 34th International Conference on Computer Design (ICCD)","24 Nov 2016","2016","","","376","379","Data movement between the compute and the storage (e.g., GPU and SSD) has been a long-neglected problem in heterogeneous systems, while the inefficiency in existing systems does cause significant loss in both performance and energy efficiency. This paper presents Hippogriff to provide a high-level programming model to simplify data movement between the compute and the storage, and to dynamically schedule data transfers based on system load. By eliminating unnecessary data movement, Hippogriff can speedup single program workloads by 1.17×, and save 17% energy. For multi-program workloads, Hippogriff shows 1.25× speedup. Hippogriff also improves the performance of a GPU-based MapReduce framework by 27%.","","978-1-5090-5142-7","10.1109/ICCD.2016.7753307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7753307","","Nonvolatile memory;Graphics processing units;Data transfer;Runtime;Discrete wavelet transforms;Peer-to-peer computing;Benchmark testing","data handling;graphics processing units;parallel processing;storage management","Hippogriff;heterogeneous computing systems;data movement;data storage;high-level programming;GPU-based MapReduce","","8","","17","","24 Nov 2016","","","IEEE","IEEE Conferences"
"Performance Analysis of a Quantum Monte Carlo Application on Multiple Hardware Architectures Using the HPX Runtime","W. Wei; A. Chatterjee; K. Huck; O. Hernandez; H. Kaiser",Louisiana State University; Oak Ridge National Laboratory; University of Oregon; Oak Ridge National Laboratory; Louisiana State University,"2020 IEEE/ACM 11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA)","31 Dec 2020","2020","","","77","84","This paper describes how we successfully used the HPX programming model to port the DCA++ application on multiple architectures that include POWER9, x86, ARM v8, and NVIDIA GPUs. We describe the lessons we can learn from this experience as well as the benefits of enabling the HPX in the application to improve the CPU threading part of the code, which led to an overall 21% improvement across architectures. We also describe how we used HPX-APEX to raise the level of abstraction to understand performance issues and to identify tasking optimization opportunities in the code, and how these relate to CPU/GPU utilization counters, device memory allocation over time, and CPU kernel level context switches on a given architecture.","","978-1-6654-2270-3","10.1109/ScalA51936.2020.00015","U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; Basic Energy Sciences; Office of Science; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308704","Quantum Monte Carlo (QMC);Dynamical Cluster Approximation (DCA);Autonomic Performance Environment for eXascale (APEX);HPX runtime system","Instruction sets;C++ languages;Runtime;Task analysis;Computer architecture;Libraries;Standards","graphics processing units;Monte Carlo methods;multi-threading;parallel architectures;quantum computing;software performance evaluation","x86;POWER9;tasking optimization;CPU threading part;NVIDIA GPUs;ARM v8;DCA++ application;HPX programming model;HPX runtime;hardware architectures;quantum Monte Carlo application;performance analysis;CPU kernel level context;HPX-APEX","","","","33","","31 Dec 2020","","","IEEE","IEEE Conferences"
"Optimal Bidding Strategies for Thermal and Generic Programming Units in the Day-Ahead Electricity Market","F. -. Heredia; M. J. Rider; C. Corchero","Statistics and Operations Research Department, Universitat Politècnica de Catalunya, Barcelona, Spain; Statistics and Operations Research Department, Universitat Politècnica de Catalunya, Barcelona, Spain; Statistics and Operations Research Department, Universitat Politècnica de Catalunya, Barcelona, Spain","IEEE Transactions on Power Systems","19 Jul 2010","2010","25","3","1504","1518","This study has developed a stochastic programming model that integrates the day-ahead optimal bidding problem with the most recent regulation rules of the Iberian Electricity Market (MIBEL) for bilateral contracts (BC), with a special consideration for the new mechanism to balance the competition of the production market, namely virtual power plant (VPP) auctions. The model allows a price-taking generation company (GenCo) to decide on the unit commitment of the thermal units, the economic dispatch of the BCs between the thermal units and the generic programming unit (GPU), and the optimal sale/purchase bids for all units (thermal and generic), by observing the MIBEL regulation. The uncertainty of the spot prices has been represented through scenario sets built from the most recent real data using scenario reduction techniques. The model has been solved using real data from a Spanish generation company and spot prices, and the results have been reported and analyzed.","1558-0679","","10.1109/TPWRS.2009.2038269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5409525","Bilateral contracts;electricity spot market;optimal bidding strategies;short-term electricity generation planning;stochastic programming;virtual power plant auctions","Electricity supply industry;Power generation;Costs;Marketing and sales;Procurement;Stochastic processes;Contracts;Production;Power system economics;Power generation economics","power markets;pricing;stochastic programming","optimal bidding strategies;generic programming units;thermal programming units;day-ahead electricity market;stochastic programming model;Iberian Electricity Market;bilateral contracts;virtual power plant auctions;production market;price-taking generation company;optimal sale-purchase bids;Spanish generation company;spot prices;scenario reduction techniques","","42","","30","IEEE","8 Feb 2010","","","IEEE","IEEE Journals"
"Parallelizing Back Propagation Neural Network on Speculative Multicores","Y. Wang; H. An; Z. Liu; T. Liu; D. Zhao","Dept. of Comput. Sci. & Technol., Southwest Univ. of Sci. & Technol., Mianyang, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Southwest Univ. of Sci. & Technol., Mianyang, China; Dept. of Comput. Sci. & Technol., Southwest Univ. of Sci. & Technol., Mianyang, China; Dept. of Comput. Sci. & Technol., Southwest Univ. of Sci. & Technol., Mianyang, China","2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)","19 Jan 2017","2016","","","902","907","Applications typically exhibit extremely different performance characteristics depending on the accelerator. Back propagation neural network (BPNN) has been parallelized into different platforms. However, it has not yet been explored on speculative multicore architecture thoroughly. This paper presents a study of parallelizing BPNN on a speculative multicore architecture, including its speculative execution model, hardware design and programming model. The implementation was analyzed with seven well-known benchmark data sets. Furthermore, it trades off several important design factors in coming speculative multicore architecture. The experimental results show that: (1) the BPNN performs well on speculative multicore platform. It can achieve similar speedup (17.7x to 57.4x) compared with graphics processors (GPU) while provides a more friendly programmability. (2) 64 cores' computing resources can be used efficiently and 4k is the proper speculative buffer capacity in the model.","1521-9097","978-1-5090-4457-3","10.1109/ICPADS.2016.0121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823836","thread level speculation;parallel programming;back propagation;multicore","Multicore processing;Instruction sets;Graphics processing units;Programming;Neural networks;Hardware","backpropagation;multiprocessing systems;neural nets","backpropagation neural network;multicore architecture;speculative execution model;hardware design;programming model;speculative multicore platform;parallelized BPNN","","","","27","","19 Jan 2017","","","IEEE","IEEE Conferences"
"Task Mapping and Scheduling for OpenVX Applications on Heterogeneous Multi/Many-Core Architectures","F. Lumpp; S. Aldegheri; H. D. Patel; N. Bombieri","Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Computer Science, University of Verona, Verona, Italy","IEEE Transactions on Computers","8 Jul 2021","2021","70","8","1148","1159","Computer vision applications have stringent performance constraints that must be satisfied when they are run at the edge on programmable low-power embedded devices. OpenVX has emerged as the de-facto reference standard to develop such applications. OpenVX uses a primitive-based programming model that results in a directed-acyclic graph (DAG) representation of the application, which can then be used for automatic system-level optimizations and synthesis to heterogeneous multi- and many-core platforms. Although OpenVX has been standardized, its state-of-the-art algorithm for task mapping and scheduling does not deliver the performance necessary for such applications to be deployed on heterogeneous multi-/many-core platforms. This article focuses on addressing this challenge with three main contributions: First, we implemented a static task scheduling and mapping approach for OpenVX using the heterogeneous earliest finish time (HEFT) heuristic. We show that HEFT allows us to improve the system performance up to 70 percent on one of the most widespread smart systems for applying computer vision and intelligent video analytics in general at the edge (i.e., NVIDIA VisionWorks on NVIDIA Jetson TX2). Second, we show that HEFT, in the context of a vision application for edge computing where some primitives may have multiple implementations (e.g., for CPU and GPU), can lead to load imbalance amongst heterogeneous computing elements (CEs), thus suffering from degraded performance. Third, we present an algorithm called exclusive earliest finish time (XEFT) that introduces the notion of exclusive overlap between single implementation primitives to improve the load balancing. We show that XEFT can further improve the system performance up to 33 percent over HEFT, and 82 percent over the native OpenVX scheduler. We present the results on a large set of benchmarks, including a real-world localization and mapping application (ORB-SLAM) combined with an NVIDIA inference application based on convolutional neural networks (CNNs) for object detection.","1557-9956","","10.1109/TC.2021.3059528","Italian Ministry of Education, Univ. and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354946","Embedded vision applications;static mapping and scheduling;OpenVX;heterogeneous architectures","Task analysis;Computer architecture;Computer vision;Graphics processing units;Optimization;System performance;Kernel","computer vision;convolutional neural nets;directed graphs;embedded systems;low-power electronics;metaheuristics;multiprocessing systems;object detection;parallel architectures;processor scheduling;SLAM (robots);video signal processing","automatic system-level optimizations;static task scheduling;heterogeneous earliest finish time heuristic;NVIDIA Jetson TX2;computer vision;edge computing;heterogeneous computing elements;exclusive earliest finish time;native OpenVX scheduler;primitive-based programming model;directed-acyclic graph representation;HEFT heuristic;programmable low-power embedded devices;XEFT;real-world localization and mapping application;ORB-SLAM;convolutional neural networks;CNN;object detection;NVIDIA VisionWorks;DAG representation;heterogeneous multicore architecture;heterogeneous many-core architecture;intelligent video analytics","","","","30","IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"Enhancing Performance of Computer Vision Applications on Low-Power Embedded Systems Through Heterogeneous Parallel Programming","S. Aldegheri; S. Manzato; N. Bombieri","Department of Computer Science, University of Verona; Department of Computer Science, University of Verona; Department of Computer Science, University of Verona","2018 IFIP/IEEE International Conference on Very Large Scale Integration (VLSI-SoC)","21 Feb 2019","2018","","","119","124","Enabling computer vision applications on low-power embedded systems gives rise to new challenges for embedded SW developers. Such applications implement different functionalities, like image recognition based on deep learning, simultaneous localization and mapping tasks. They are characterized by stringent performance constraints to guarantee real-time behaviors and, at the same time, energy constraints to save battery on the mobile platform. Even though heterogeneous embedded boards are getting pervasive for their high computational power at low power costs, they need a time consuming customization of the whole application (i.e., mapping of application blocks to CPU-GPU processing elements and their synchronization) to efficiently exploit their potentiality. Different languages and environments have been proposed for such an embedded SW customization. Nevertheless, they often find limitations on complex real cases, as their application is mutual exclusive. This paper presents a comprehensive framework that relies on a heterogeneous parallel programming model, which combines OpenMP, PThreads, OpenVX, OpenCV, and CUDA to best exploit different levels of parallelism while guaranteeing a semi-automatic customization. The paper shows how such languages and API platforms have been interfaced, synchronized, and applied to customize an ORB-SLAM application for an NVIDIA Jetson TX2 board.","2324-8440","978-1-5386-4756-1","10.1109/VLSI-SoC.2018.8644937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8644937","","Graphics processing units;Computer vision;Kernel;Task analysis;Computational modeling;Power demand;Libraries","application program interfaces;computer vision;coprocessors;embedded systems;graphics processing units;hardware-software codesign;image recognition;microprocessor chips;multiprocessing systems;parallel architectures;parallel programming;SLAM (robots)","low-power embedded systems;embedded SW developers;real-time behaviors;heterogeneous embedded boards;high computational power;low power costs;application blocks;embedded SW customization;heterogeneous parallel programming model;ORB-SLAM application;computer vision applications;performance constraints;OpenMP;PThreads;OpenVX;OpenCV;CUDA","","3","","12","","21 Feb 2019","","","IEEE","IEEE Conferences"
"GasCL: A vertex-centric graph model for GPUs","S. Che","Advanced Micro Devices, USA","2014 IEEE High Performance Extreme Computing Conference (HPEC)","12 Feb 2015","2014","","","1","6","There are increasing research efforts of using GPUs for graph processing. Most prior work on accelerating GPGPU graph algorithms has been focused on algorithm and device-specific optimizations. There is little research on studying high-level programming models and associate run-time systems for graph processing on GPUs, which will be useful to solve diverse real-world problems flexibly. This paper presents a preliminary implementation of a graph framework, GasCL, supporting the well-known “think-like-a-vertex” programming model. The system is built on top of OpenCL and portable across diverse accelerators. We describe our design and use two applications as case studies. The initial performance result shows an average of 2.5× speedup on a GPU compared with a CPU.","","978-1-4799-6233-4","10.1109/HPEC.2014.7040962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040962","","Graphics processing units;Kernel;Arrays;Runtime;Computational modeling;Programming","graph theory;graphics processing units;optimisation;parallel programming","GasCL;vertex-centric graph model;graph processing;GPGPU graph algorithms;device-specific optimizations;graph framework;think-like-a-vertex programming model;OpenCL;gather-apply-scatter;general purpose graphics processing unit","","15","","25","","12 Feb 2015","","","IEEE","IEEE Conferences"
"Performance portability of a fluidized bed solver","V. M. Krushnarao Kotteda; V. Kumar; W. Spotz; D. Sunderland","Mechanical Engineering, University of Texas at El Paso, El Paso, USA; Mechanical Engineering, University of Texas at El Paso, El Paso, USA; Multiphysics Applications, Sandia National Laboratories, Albuquerque, USA; Scalable Algorithms, Sandia National Laboratories, Albuquerque, USA","2018 IEEE High Performance extreme Computing Conference (HPEC)","29 Nov 2018","2018","","","1","7","Performance portability is a challenge for application developers as the source code needs to be executed and performant on various hybrid computing architectures. The linear iterative solvers implemented in most applications consume more than 70% of the runtime. This paper presents the results of a linear solver in Trilinos for fluidized bed applications. The linear solver implemented in our code is based on the Kokkos programming model in Trilinos, which uses a library approach to provide performance portability across diverse devices with different memory models. For large scale problems, the numerical experiments on Xeon Phi and Kepler GPU architectures show good performance over the results on Xeon (Haswell) computing architectures.","2377-6943","978-1-5386-5989-2","10.1109/HPEC.2018.8547775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8547775","performance;portability;fluidized bed;Kokkos;Trilinos;MFiX","Mathematical model;Programming;Graphics processing units;Computational modeling;Computer architecture;Message systems;Libraries","fluidised beds;graphics processing units;iterative methods;mathematics computing;multiprocessing systems;parallel architectures","Kokkos programming model;linear iterative solver;source code;application developers;fluidized bed solver;Xeon computing architectures;performance portability;fluidized bed applications;hybrid computing architectures","","","","31","","29 Nov 2018","","","IEEE","IEEE Conferences"
"STOIC: Serverless Teleoperable Hybrid Cloud for Machine Learning Applications on Edge Device","M. Zhang; C. Krintz; R. Wolski","University of California,Dept. of Computer Science,Santa Barbara; University of California,Dept. of Computer Science,Santa Barbara; University of California,Dept. of Computer Science,Santa Barbara","2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)","4 Aug 2020","2020","","","1","6","Serverless computing is a promising new event-driven programming model that was designed by cloud vendors to expedite the development and deployment of scalable web services on cloud computing systems. Using the model, developers write applications that consist of simple, independent, stateless functions that the cloud invokes on-demand (i.e. elastically), in response to system-wide events (data arrival, messages, web requests, etc.). In this work, we present STOIC (Serverless TeleOperable HybrId Cloud), an application scheduling and deployment system that extends the serverless model in two ways. First, it uses the model in a distributed setting and schedules application functions across multiple cloud systems. Second, STOIC supports serverless function execution using hardware acceleration (e.g. GPU resources) when available from the underlying cloud system. We overview the design and implementation of STOIC and empirically evaluate it using real-world machine learning applications and multi-tier (e.g. edge-cloud) deployments. We find that STOIC's combined use of edge and cloud resources is able to outperform using either cloud in isolation for the applications and datasets that we consider.","","978-1-7281-4716-1","10.1109/PerComWorkshops48775.2020.9156239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156239","Serverless computing;Edge computing;Image Processing;Internet of Things","Cloud computing;Image edge detection;Runtime;Machine learning;Task analysis;Computer architecture;Graphics processing units","cloud computing;Internet;learning (artificial intelligence);Web services","edge-cloud;STOIC;Serverless teleoperable hybrid Cloud;Serverless computing;event-driven programming model;cloud vendors;scalable web services;cloud computing systems;simple functions;independent functions;stateless functions;system-wide events;Serverless TeleOperable HybrId Cloud;application scheduling;deployment system;serverless model;distributed setting;schedules application functions;multiple cloud systems;serverless function execution;underlying cloud system;real-world machine learning applications","","1","","36","","4 Aug 2020","","","IEEE","IEEE Conferences"
