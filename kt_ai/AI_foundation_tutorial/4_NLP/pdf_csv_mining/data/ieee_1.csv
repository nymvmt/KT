"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Exploring a multi-resolution GPU programming model for Chapel","A. Hayashi; S. Raj Paul; V. Sarkar","Georgia Institute of Technology Atlanta,Georgia,USA; Georgia Institute of Technology Atlanta,Georgia,USA; Georgia Institute of Technology Atlanta,Georgia,USA","2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","28 Jul 2020","2020","","","675","675","There is a growing need to support accelerators, especially GPU accelerators, since they are a common source of performance improvement in HPC clusters. As for GPU programming with Chapel, typically programmers first start with writing forall loops and run these loops on CPUs as a proof-of-concept. If the resulting CPU performance is not sufficient for their needs, their next step could be to try the automatic compiler-based GPU code generation techniques [1], [2]. For portions that remain as performance bottlenecks, even after automatic compilation approaches, the next step is to consider writing GPU kernels using CUDA/HIP/OpenCL and invoking these kernels from the Chapel program using the GPUIterator [3], [4] and Chapel's C interoperability feature.","","978-1-7281-7445-7","10.1109/IPDPSW50202.2020.00117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150427","","Graphics processing units;Programming;Kernel;Data transfer;Optimization;Conferences;Writing","parallel architectures;parallel programming;program compilers;public domain software","automatic compilation;GPU kernels;Chapel program;multiresolution GPU programming model;GPU accelerators;HPC clusters;forall loops;Chapel C interoperability feature;automatic compiler-based GPU code generation;CUDA;OpenCL;HIP","","","","6","","28 Jul 2020","","","IEEE","IEEE Conferences"
"GPU Assist using DSP Pre-processor","M. Mody; H. Hariyani; A. Balagopalakrishnan; J. Jones; A. Jayaraj; Y. A. Prithvishankar","Texas Instruments Inc,Embedded Automotive Processor Business; Texas Instruments Inc,Embedded Automotive Processor Business; Texas Instruments Inc,Embedded Automotive Processor Business; Texas Instruments Inc,Embedded Automotive Processor Business; Texas Instruments Inc,Embedded Automotive Processor Business; Texas Instruments Inc,Embedded Automotive Processor Business","2020 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)","16 Sep 2020","2020","","","1","4","There is an ever increasing need for higher GPU performance to render sophisticated User Interface, latest high end 3D games and general purpose compute (GPGPU) applications. GPU SW programming models such as OpenGL have evolved over decades to cater to the unique mixed pipeline 3D GPU architectures. Due to the sticky nature of GPU SW programming model, leveraging other HW blocks to enhance graphics performance has been a most challenging task for SW architects. System designers have usually responded to the GFLOPS demand by increasing the GPU HW specifications. This paper proposes enhancing GPU performance by leveraging DSP transparently in background without impacting GPU software programming model. The proposed solution consists of multiple novel techniques namely ability to offload vertex shader to DSP, 3 stage pipelined execution and ability to re-use GPU internal pipeline. The proposed solution is prototyped in Jacinoto6 Platform from Texas Instruments. The default GPU spec performance is increased by up-to 41% by leveraging dual core C66x DSP in Jacinto6 Platform using proposed solution for different use-cases. The proposed solution is fully transparent to application software stack. In addition, the solution is directly applicable to any GPU + DSP architecture making it attractive approach for cost optimized solutions.","","978-1-7281-6828-9","10.1109/CONECCT50063.2020.9198650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9198650","GPU;GPGPU;GLOPS;Shader;OpenGL;Vulkan DSP;Jacinto","Graphics processing units;Pipeline processing;Pipelines;Programming;Random access memory;Context","digital signal processing chips;graphics processing units;pipeline processing","3D games;general purpose compute applications;GPU SW programming model;graphics performance;GPU HW specifications;GPU software programming model;3 stage pipelined execution;default GPU spec performance;DSP architecture;GPU performance;user interface;GPU internal pipeline;3D GPU architectures;DSP preprocessor;OpenGL;GFLOPS;vertex shader;Jacinoto6 platform;dual core C66x DSP;Jacinto6 platform","","1","","8","","16 Sep 2020","","","IEEE","IEEE Conferences"
"GPU Computing Pipeline Inefficiencies and Optimization Opportunities in Heterogeneous CPU-GPU Processors","J. Hestness; S. W. Keckler; D. A. Wood",NA; NA; NA,"2015 IEEE International Symposium on Workload Characterization","2 Nov 2015","2015","","","87","97","Emerging heterogeneous CPU-GPU processors have introduced unified memory spaces and cache coherence. CPU and GPU cores will be able to concurrently access the same memories, eliminating memory copy overheads and potentially changing the application-level optimization targets. To date, little is known about how developers may organize new applications to leverage the available, finer-grained communication in these processors. However, understanding potential application optimizations and adaptations is critical for directing heterogeneous processor programming model and architectural development. This paper quantifies opportunities for applications and architectures to evolve to leverage the new capabilities of heterogeneous processors. To identify these opportunities, we ported and simulated a broad set of benchmarks originally developed for discrete GPUs to remove memory copies, and applied analytical models to quantify their application-level pipeline inefficiencies. For existing benchmarks, GPU bulk-synchronous software pipelines result in considerable core and cache utilization inefficiency. For heterogeneous processors, the results indicate increased opportunity for techniques that provide flexible compute and data granularities, and support for efficient producer-consumer data handling and synchronization within caches.","","978-1-5090-0088-3","10.1109/IISWC.2015.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314150","GPGPU;GPU computing;heterogeneous processors;benchmarking","Graphics processing units;Benchmark testing;Kernel;Optimization;Pipelines;Synchronization","graphics processing units;memory architecture;optimisation;pipeline processing","GPU computing pipeline inefficiency;optimization opportunity;heterogeneous CPU-GPU processor;unified memory space;cache coherence;CPU core;GPU core;memory copy overhead;application-level optimization;finer-grained communication;heterogeneous processor programming model;architectural development;application-level pipeline inefficiency;GPU bulk-synchronous software pipeline","","19","2","38","","2 Nov 2015","","","IEEE","IEEE Conferences"
"Poster: Acceleration of the BLAST Hydro Code on GPU","T. Dong; T. Kolev; R. Rieben; V. Dobrev","Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, USA; Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, USA; Lawrence Livermore Nat. Lab., Livermore, USA; Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, USA","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","11 Apr 2013","2012","","","1337","1337","The BLAST code implements a high-order numerical algorithm that solves the equations of compressible hydrodynamics using the Finite Element Method in a moving Lagrangian frame. BLAST is coded in C++ and parallelized by MPI. We accelerate the most computationally intensive parts (80%-95%) of BLAST on an NVIDIA GPU with the CUDA programming model. Several 2D and 3D problems were tested and a maximum speedup of 4.3x was delivered. Our results demonstrate the validity and capability of GPU computing.","","978-0-7695-4956-9","10.1109/SC.Companion.2012.172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495955","GPU;CFD;FEM","","application program interfaces;C++ language;compressible flow;computational fluid dynamics;finite element analysis;graphics processing units;hydrodynamics;message passing;parallel programming","GPU computing;NVIDIA GPU;compute unified device architecture;CUDA programming model;parallelization;message passing interface;MPI;C++ language;moving Lagrangian frame;finite element method;compressible hydrodynamics;high-order numerical algorithm;graphics processing unit;BLAST hydro code","","","","","","11 Apr 2013","","","IEEE","IEEE Conferences"
"Accelerating Kirchhoff Migration on GPU Using Directives","R. Xu; M. Hugues; H. Calandra; S. Chandrasekaran; B. Chapman","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; TOTAL E&P Res. & Technol. USA, Houston, TX, USA; TOTAL E&P Res. & Technol. USA, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA; Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA","2014 First Workshop on Accelerator Programming using Directives","9 Apr 2015","2014","","","37","46","Accelerators offer the potential to significantly improve the performance of scientific applications when offloading compute intensive portions of programs to the accelerators. However, effectively tapping their full potential is difficult owing to the programmability challenges faced by the users when mapping computation algorithms to the massively parallel architectures such as GPUs.Directive-based programming models offer programmers an option to rapidly create prototype applications by annotating region of code for offloading with hints to the compiler. This is critical to improve the productivity in the production code. In this paper, we study the effectiveness of a high-level directivebased programming model, OpenACC, for parallelizing a seismic migration application called Kirchhoff Migration on GPU architecture. Kirchhoff Migration is a real-world production code in the Oil & Gas industry. Because of its compute intensive property, we focus on the computation part and explore different mechanisms to effectively harness GPU's computation capabilities and memory hierarchy. We also analyze different loop transformation techniques in different OpenACC compilers and compare their performance differences. Compared toone socket (10 CPU cores) on the experimental platform, one GPU achieved a maximum speedup of 20.54x and 6.72x for interpolation and extrapolation kernel functions.","","978-1-4673-6753-0","10.1109/WACCPD.2014.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081676","OpenACC; Kirchhoff Migration; GPU; Directives; Programming Model","Graphics processing units;Interpolation;Kernel;Programming;Extrapolation;Computer architecture;Computational modeling","extrapolation;geophysics computing;graphics processing units;interpolation;parallel architectures;program compilers;seismology","Kirchhoff migration;accelerator;compute intensive portion;programmability challenges;computation algorithm;parallel architecture;directive-based programming model;productivity;high-level directive based programming model;seismic migration application;GPU architecture;real-world production code;oil & gas industry;compute intensive property;GPU computation capability;memory hierarchy;loop transformation technique;OpenACC compiler;interpolation kernel function;extrapolation kernel function","","5","","18","","9 Apr 2015","","","IEEE","IEEE Conferences"
"GPU parallel computing architecture and CUDA programming model","J. Nickolls",NA,"2007 IEEE Hot Chips 19 Symposium (HCS)","4 Jul 2016","2007","","","1","12","This article consists of a collection of slides from the author's conference presentation on NVIDIA's CUDA programming model (parallel computing platform and application programming interface) via graphical processing units (GPU). Some of the specific topics discussed include: the special features of GPUs; the importance of GPU computing; system specifications and architectures; processing capabilities; parallel memory sharing; CUDA programming models; transparent scalability; and major applications supported.","","978-1-4673-8869-6","10.1109/HOTCHIPS.2007.7482491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482491","","Instruction sets;Graphics processing units;Parallel processing;Computer architecture;Programming;Computational modeling","application program interfaces;graphics processing units;parallel architectures","GPU parallel computing architecture;NVIDIA CUDA programming models;parallel computing platform;application programming interface;graphical processing units;parallel memory sharing;transparent scalability","","14","1","","","4 Jul 2016","","","IEEE","IEEE Conferences"
"Kernel Fusion/Decomposition for Automatic GPU-Offloading","A. Mishra; M. Kong; B. Chapman","Stony Brook University, USA; Brookhaven National Laboratory, USA; Stony Brook University, USA","2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","7 Mar 2019","2019","","","283","284","The massively parallel architecture of GPU accelerators are being harnessed to expedite computational workloads in cutting edge scientific research. Unfortunately writing applications for GPUs requires extensive knowledge of the underlying architecture, the application and the interfacing programming model. Moreover, (re-)writing kernels using lower-level programming models such as CUDA and OpenCL is a burden for application scientists. A more appealing strategy is to leverage a programming model layered on directive-based optimization: OpenMP, whose recent specification significantly extends its accelerator functionalities. Despite this, it is still quite challenging to optimize large scale applications, since “pragmatizing” each kernel is a repetitive and complex task. In large scale applications most of the operations could be small, don't have enough computational work to justify a GPU execution, deeply buried in the library specification, or evenly spread throughout the application. Thus, we seek to design and build a compiler framework that can automatically and profitably offload regions of code with these characteristics. The driving principle of our work resides in generating numerous kernel variants that result from fusing and/or decomposing existing function bodies. We analyze the program's call graph to determine the “proximity” of kernel calls and evaluate the degree of data reuse among adjacent or “close-enough” calls. When such patterns are detected we generate several scenarios, until producing a single variant whose footprint is near the capacity of the GPU. To compare the potential performance among the various kernel variants generated, we are designing an adaptive cost model. The precision of this cost model will depend upon the analyzability of the program. We are also building upon existing cost models like Baghsorkhi et al.'s model which proposed a work flow graph based analytical model and a recent Hong et al.'s model which propose the use of abstract kernel emulations to help identify the performance bottlenecks of a GPU program execution. Along with these we introduce GPU initialization and data transfer cost to the model. Once the profitable kernel variants are detected, we automatically insert pertinent OpenMP directives and provide a newly generated code supporting GPU offloading.","","978-1-7281-1436-1","10.1109/CGO.2019.8661188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661188","","Kernel;Graphics processing units;Adaptation models;Computational modeling;Analytical models;Programming;Libraries","application program interfaces;coprocessors;graphics processing units;message passing;optimisation;parallel architectures;parallel programming;program compilers","lower-level programming models;application scientists;directive-based optimization;accelerator functionalities;repetitive task;complex task;GPU execution;library specification;kernel calls;adaptive cost model;work flow graph;abstract kernel emulations;GPU program execution;data transfer cost;profitable kernel variants;pertinent OpenMP directives;kernel fusion/decomposition;automatic GPU-offloading;massively parallel architecture;GPU accelerators;computational workloads;edge scientific research;interfacing programming model;writing kernels;generated code;automatic GPU offloading","","3","","8","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Research on Tool Path Planning Method of NURBS Surface Based on CPU-GPU Parallel Computing","W. Yu; Y. Bi; Z. Li","Sch. of Autom., Hangzhou Univ. of Electron. Sci. & Technol., Hangzhou, China; Sch. of Autom., Hangzhou Univ. of Electron. Sci. & Technol., Hangzhou, China; Sch. of Autom., Hangzhou Univ. of Electron. Sci. & Technol., Hangzhou, China","2017 International Conference on Computer Network, Electronic and Automation (ICCNEA)","7 Dec 2017","2017","","","85","88","In order to deal with the inefficiency of trational serial tool path algorithms and incompatibility issues on the heterogeneous hardware platforms, this paper suggests a tool path planning method based on CPU-GPU(Central Processing Unit-Graphic Processing Unit) heterogeneous parallel computing. The method contra poses NURBS(Non-Uniform Rational B-Splines) surface which is abstracted as a matrix multiplication on the principle of isoparametric line tool path planning method. Then a parallel algorithm in accordance with Open CL(Open Computing Language) specification is proposed. Adopting data parallel programming model, the method executes multiple work-items of the GPU on the core under control of the CPU logic, and reconstructs the isoparametric line method as parallel execution instead of traditional serial execution. Simulation results show that this algorithm takes less time to generate tool paths on the CPU-GPU heterogeneous platforms, reduced by 1.5 to 15.9 times compared with traditional serial algorithm and it is of great significance to the tool path planning's real-time or quasi real-time generation.","","978-1-5386-3981-8","10.1109/ICCNEA.2017.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128535","Component;Nurbs surfaces;OpenCL;Parallel computing;Tool path planning;CPU-GPU","Tools;Graphics processing units;Path planning;Parallel processing;Splines (mathematics);Surface topography;Surface reconstruction","graphics processing units;parallel programming;path planning;splines (mathematics)","CPU logic;CPU-GPU heterogeneous platforms;traditional serial algorithm;Nurbs surface;CPU-GPU parallel computing;heterogeneous hardware platforms;parallel algorithm;serial tool path algorithms;Central Processing Unit-Graphic Processing Unit;NonUniform Rational B-Splines surface;isoparametric line tool path planning;Open CL;Open Computing Language;data parallel programming model","","1","","11","","7 Dec 2017","","","IEEE","IEEE Conferences"
"GPU programming for EDA with OpenCL","R. O. Topaloglu; B. Gaster","GLOBALFOUNDRIES, 840 N McCarthy Blvd, Milpitas CA 95035; Advanced Micro Devices, 1 AMD Pl, Sunnyvale CA 94085","2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","15 Dec 2011","2011","","","63","66","Graphical processing unit (GPU) computing has been an interesting area of research in the last few years. While initial adapters of the technology have been from image processing domain due to difficulties in programming the GPUs, research on programming languages made it possible for people without the knowledge of low-level programming languages such as OpenGL develop code on GPUs. Two main GPU architectures from AMD (former ATI) and NVIDIA acquired grounds. AMD adapted Stanford's Brook language and made it into an architecture-agnostic programming model. NVIDIA, on the other hand, brought CUDA framework to a wide audience. While the two languages have their pros and cons, such as Brook not being able to scale as well and CUDA having to account for architectural-level decisions, it has not been possible to compile one code on another architecture or across platforms. Another opportunity came with the introduction of the idea of combining one or more CPUs and GPUs on the same die. Eliminating some of the interconnection bandwidth issues, this combination makes it possible to offload tasks with high parallelism to the GPU. The technological direction towards multicores for CPU-only architectures also require a programming methodology change and act as a catalyst for suitable programming languages. Hence, a unified language that can be used both on multiple core CPUs as well as GPUs and their combinations has gained interest. Open Computing Language (OpenCL), developed originally by the Khronos Group of Apple and supported by both AMD and NVIDIA, is seen as the programming language of choice for parallel programming. In this paper, we provide a motivation for our tutorial talk on usage of OpenCL for GPUs and highlight key features of the language. We provide research directions on OpenCL for EDA. In our tutorial talk, we use EDA as our application domain to get the readers started with programming the rising language of parallelism, OpenCL.","1558-2434","978-1-4577-1400-9","10.1109/ICCAD.2011.6105306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6105306","GPU;GPGPU;OpenCL;EDA;CAD;algorithms","Graphics processing unit;Programming;Kernel;Multicore processing;Design automation;Algorithm design and analysis","graphics processing units;parallel programming;programming languages","GPU programming;EDA;OpenCL;graphical processing unit computing;image processing domain;research on programming language;low-level programming languages;OpenGL;GPU architecture;AMD;NVIDIA;Stanford Brook language;architecture-agnostic programming model;CUDA framework;architectural-level decisions;unified language;open computing language;parallel programming","","1","","17","","15 Dec 2011","","","IEEE","IEEE Conferences"
"Reflection removal using ghosting cues based on GPU via CUDA","M. Liao; C. Lv; G. Li; J. Lin; X. Gao","Software School, Xiamen University, Xiamen, Fujian, P.R. China; Software School, Xiamen University, Xiamen, Fujian, P.R. China; Software School, Xiamen University, Xiamen, Fujian, P.R. China; Software School, Xiamen University, Xiamen, Fujian, P.R. China; Software School, Xiamen University, Xiamen, Fujian, P.R. China","2016 11th International Conference on Computer Science & Education (ICCSE)","6 Oct 2016","2016","","","814","817","Reflections of windows or glass panes always destroy the photograph that we want to take by camera or phone. The purpose of this paper is to study and put forward a reflection removal algorithm based on GPU. The algorithm is not simply transplanted from CPU onto GPU. In order to make it adapted to the GPU architecture and programming model, we improve on the original method and the improved algorithm is more efficient by using GPU resources sufficiently. Experimental results show that the algorithm of this paper is effective and efficient. The GPU-based implementation is faster (up to 18 times) than the CPU-based implementation.","","978-1-5090-2218-2","10.1109/ICCSE.2016.7581687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581687","reflection removal;parallel computation;GPU;CUDA","Reflection;Graphics processing units;Algorithm design and analysis;Windows;Glass;Parallel processing","graphics processing units;image restoration;parallel architectures;reflection","reflection removal algorithm;ghosting cues;GPU architecture;CUDA;window reflections;glass pane reflections;photographs;programming model;image restoration","","1","","22","","6 Oct 2016","","","IEEE","IEEE Conferences"
"Hybrid Parallel Programming on GPU Clusters","C. Yang; C. Huang; C. Lin; T. Chang","Dept. of Comput. Sci., Tunghai Univ., Taichung, Taiwan; Dept. of Comput. Sci., Tunghai Univ., Taichung, Taiwan; Dept. of Comput. Sci., Tunghai Univ., Taichung, Taiwan; Dept. of Comput. Sci., Tunghai Univ., Taichung, Taiwan","International Symposium on Parallel and Distributed Processing with Applications","11 Nov 2010","2010","","","142","147","Nowadays, NVIDIA's CUDA is a general purpose scalable parallel programming model for writing highly parallel applications. It provides several key abstractions - a hierarchy of thread blocks, shared memory, and barrier synchronization. This model has proven quite successful at programming multithreaded many core GPUs and scales transparently to hundreds of cores: scientists throughout industry and academia are already using CUDA to achieve dramatic speedups on production and research codes. In this paper, we propose a hybrid parallel programming approach using hybrid CUDA and MPI programming, which partition loop iterations according to the number of C1060 GPU nodes in a GPU cluster which consists of one C1060 and one S1070. Loop iterations assigned to one MPI process are processed in parallel by CUDA run by the processor cores in the same computational node.","2158-9208","978-1-4244-8095-1","10.1109/ISPA.2010.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634329","CUDA;GPU;MPI;OpenMP;hybrid;parallel programming","Graphics processing unit;Instruction sets;Parallel processing;Parallel programming;Computational modeling;Linux","computer graphic equipment;coprocessors;message passing;parallel programming","hybrid parallel programming;NVIDIA CUDA programming model;graphics processing units;multithreaded programming;message passing interface;loop iterations;C1060 GPU cluster;S1070 GPU cluster","","4","","18","","11 Nov 2010","","","IEEE","IEEE Conferences"
"GPU-accelerated parallel algorithms for map algebra","Jianbo Zhang; Wenxin Yang; Jing Sun; Yonghong Lv","Department of Software Engineering, Faculty of Information Engineering, China University of Geoscience, Wuhan, China; Department of Software Engineering, Faculty of Information Engineering, China University of Geoscience, Wuhan, China; Department of Software Engineering, Faculty of Information Engineering, China University of Geoscience, Wuhan, China; Department of Software Engineering, Faculty of Information Engineering, China University of Geoscience, Wuhan, China","2010 The 2nd Conference on Environmental Science and Information Application Technology","9 Sep 2010","2010","1","","882","885","Aiming at the low efficiency when traditional realization methods of map algebra apply to calculations for gigantic raster data, this paper maps the traditional serial algorithms to GPU parallel processing architecture on a new parallel programming model of GPU named Compute Unified Device Architecture. The paper also aims to discuss the realization mechanism surrounding parallel mapping methods from traditional serial algorithms to parallel algorithms and adaptive-parameter adjustments on computer graphic processor resources.","","978-1-4244-7390-8","10.1109/ESIAT.2010.5567202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5567202","Map Algebra;GPU;CUDA;Parallel Computation","Graphics processing unit;Algebra;Instruction sets;Computer architecture;Computational modeling;Hardware;Parallel processing","geographic information systems;mathematics computing;parallel algorithms;parallel architectures;parallel programming","GPU-accelerated parallel algorithms;map algebra;serial algorithms;GPU parallel processing architecture;parallel programming model;Compute Unified Device Architecture;adaptive-parameter adjustments;computer graphic processor","","2","","10","","9 Sep 2010","","","IEEE","IEEE Conferences"
"Abstract: GPU Accelerated Ultrasonic Tomography Using Propagation and Backpropagation Method","P. D. Bello; Y. Jin; E. Lu","Dept. of Electr. & Comput. Eng., Florida Int. Univ., Miami, FL, USA; Dept. of Eng. & Aviation Sci., Univ. of Maryland Eastern Shore, Princess Anne, MD, USA; Dept. of Math. & Comput. Sci., Salisbury Univ., Salisbury, MD, USA","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","11 Apr 2013","2012","","","1445","1446","This paper develops implementation strategy and method to accelerate the propagation and backpropagation (PBP) tomographic imaging algorithm using Graphic Processing Units (GPUs). The Compute Unified Device Architecture (CUDA) programming model is used to develop our parallelized algorithm since the CUDA model allows the user to interact with the GPU resources more efficiently than traditional shader methods. The results show an improvement of more than 80x when compared to the C/C++ version of the algorithm, and 515x when compared to the MATLAB version while achieving high quality imaging for both cases. We test different CUDA kernel configurations in order to measure changes in the processing-time of our algorithm. By examining the acceleration rate and the image quality, we develop an optimal kernel configuration that maximizes the throughput of CUDA implementation for the PBP method.","","978-0-7695-4956-9","10.1109/SC.Companion.2012.248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496031","Medical Imaging;Ultrasonic Tomography;GPU;CUDA;Parallel Computing","","acoustic tomography;backpropagation;computerised tomography;graphics processing units;medical image processing;parallel algorithms;parallel architectures;ultrasonic imaging","parallel computing;medical imaging;PBP method;optimal kernel configuration;image quality;acceleration rate;CUDA kernel configuration;MATLAB version;shader method;GPU resource;parallelized algorithm;CUDA programming model;compute unified device architecture;graphic processing unit;PBP tomographic imaging algorithm;propagation and backpropagation tomographic imaging algorithm;GPU accelerated ultrasonic tomography","","","","5","","11 Apr 2013","","","IEEE","IEEE Conferences"
"Extending OpenSHMEM for GPU Computing","S. Potluri; D. Bureddy; H. Wang; H. Subramoni; D. K. Panda","Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","2013 IEEE 27th International Symposium on Parallel and Distributed Processing","29 Jul 2013","2013","","","1001","1012","Graphics Processing Units (GPUs) are becoming an integral part of modern supercomputer architectures due to their high compute density and performance per watt. In order to maximize utilization, it is imperative that applications running on these clusters have low synchronization and communication overheads. Partitioned Global Address Space (PGAS) models provide an attractive approach for developing parallel scientific applications. Such models simplify programming through the abstraction of a shared memory address space while their one-sided communication primitives allow for efficient implementation of applications with minimum synchronization. OpenSHMEM is a library-based programming model that is gaining popularity. However, the current OpenSHMEM standard does not support direct communication from GPU device buffers. It requires data to be copied to the host memory before OpenSHMEM calls can be made. Similarly, data has to moved to the GPU explicitly by remote processes. This severely limits the programmability and performance of GPU applications. In this paper we provide extensions to the OpenSHMEM model which allow communication calls to be made directly on the GPU memory. The proposed extensions are interoperable with the two most popular GPU programming frameworks: CUDA and OpenCL. We present designs for an efficient OpenSHMEM runtime which transparently provide high-performance communication between GPUs in different inter-node and intra-node configurations. To the best of our knowledge this is the first work that enables GPU-GPU communication using the OpenSHMEM model for both CUDA and OpenCL computing frameworks. The proposed extensions to OpenSHMEM, coupled with the high-performance runtime, improve the latency of GPU-GPU shmem getmem operation by 90%, 40% and 17%, for intra-IOH (I/O Hub), inter-IOH and inter-node configurations. It improves the performance of OpenSHMEM atomics by up to 55% and 52%, for intra-IOH and inter-node GPU configurations respectively. The proposed enhancements improve the performance of Stencil2D kernel by 65% on a cluster of 192 GPUs and the performance of BFS kernel by 12% on a cluster of 96 GPUs.","1530-2075","978-1-4673-6066-1","10.1109/IPDPS.2013.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569880","PGAS;OpenSHMEM;GPU;CUDA;OpenCL","Graphics processing units;Runtime;Context;Computational modeling;Electronics packaging;Programming;Kernel","graphics processing units;parallel architectures;parallel machines;shared memory systems","OpenSHMEM runtime;high-performance communication;inter-node configurations;intra-node configurations;OpenCL computing frameworks;high-performance runtime;GPU-GPU shmem getmem operation;intra-IOH;I/O hub;OpenSHMEM atomics;GPU computing;graphics processing units;modern supercomputer architectures;high compute density;performance per watt;communication overheads;partitioned global address space models;PGAS models;parallel scientific applications;abstraction;shared memory address space;one-sided communication primitives;minimum synchronization;library-based programming model;direct communication;GPU device buffers;host memory;programmability;OpenSHMEM model;GPU memory;interoperable;GPU programming frameworks;CUDA;OpenCL;Stencil2D kernel","","21","","32","","29 Jul 2013","","","IEEE","IEEE Conferences"
"Tartan: Evaluating Modern GPU Interconnect via a Multi-GPU Benchmark Suite","A. Li; S. L. Song; J. Chen; X. Liu; N. Tallent; K. Barker",Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory,"2018 IEEE International Symposium on Workload Characterization (IISWC)","13 Dec 2018","2018","","","191","202","High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale applications. However, the lack of deep understanding on how modern GPUs can be connected and the actual impact of state-of-the-art interconnect on multiGPU application performance becomes a hurdle. Additionally, the absence of a practical multi-GPU benchmark suite poses further obstacles for conducting research in multi-GPU era. In this paper, we fill the gap by proposing a multi-GPU benchmark suite named Tartan, which contains microbenchmarks, scale-up and scale-out applications. We then apply Tartan to evaluate the four latest types of modern GPU interconnects, i.e., PCI-e, NVLink-V1, NVLink-V2 and InfiniBand with GPUDirect-RDMA from two recently released NVIDIA super AI platforms as well as ORNL's exascale prototype system. Based on empirical evaluation, we observe four new types of NUMA effects: three types are triggered by NVLink's topology, connectivity and routing, while one type is caused by PCI-e (i.e., anti-locality). They are very important for performance tuning in multi-GPU environment. Our evaluation results show that, unless the current CPU-GPU master-slave programming model can be replaced, it is difficult for scale-up multi-GPU applications to really benefit from faster intra-node interconnects such as NVLinks; while for inter-node scale-out applications, although interconnect is more crucial to the overall performance, GPUDirect-RDMA appears to be not always the optimal choice. The Tartan benchmark suite including the microbenchmarks are opensource and available athttp://github.com/uuudown/Tartan.","","978-1-5386-6780-4","10.1109/IISWC.2018.8573483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573483","","Graphics processing units;Bandwidth;Peer-to-peer computing;Benchmark testing;Network topology;Topology;Routing","benchmark testing;computer graphic equipment;graphics processing units;multiprocessing systems","Tartan benchmark suite;modern GPU interconnect;high performance multiGPU computing;planet-scale applications;multiGPU application performance;multiGPU era;NVLink-V1;NVLink-V2;multiGPU environment;scale-up multiGPU applications;modern GPU;CPU-GPU master-slave programming model;multiGPU benchmark suite;intra-node interconnects","","14","","72","","13 Dec 2018","","","IEEE","IEEE Conferences"
"Integrating Multi-GPU Execution in an OpenACC Compiler","T. Komoda; S. Miwa; H. Nakamura; N. Maruyama","Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan; Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan; Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan; Adv. Inst. for Comput. Sci., RIKEN, Kobe, Japan","2013 42nd International Conference on Parallel Processing","19 Dec 2013","2013","","","260","269","GPUs have become promising computing devices in current and future computer systems due to its high performance, high energy efficiency, and low price. However, lack of high level GPU programming models hinders the wide spread of GPU applications. To resolve this issue, OpenACC is developed as the first industry standard of a directive-based GPU programming model and several implementations are now available. Although early evaluations of the OpenACC systems showed significant performance improvement with modest programming efforts, they also revealed the limitations of the systems. One of the biggest limitations is that the current OpenACC compilers do not automate the utilization of multiple GPUs. In this paper, we present an OpenACC compiler with the capability to execute single GPU OpenACC programs on multiple GPUs. By orchestrating the compiler and the runtime system, the proposed system can efficiently manage the necessary data movements among multiple GPUs memories. To enable advanced communication optimizations in the proposed system, we propose a small set of directives as extensions of OpenACC API. The directives allow programmers to express the patterns of memory accesses in the parallel loops to be offloaded. Inserting a few directives into an OpenACC program can reduce a large amount of unnecessary data movements and thus helps the proposed system drawing great performance from multi-GPU systems. We implemented and evaluated the prototype system on top of CUDA with three data parallel applications. The proposed system achieves up to 6.75x of the performance compared to OpenMP in the 1CPU with 2GPU machine, and up to 2.95x of the performance compared to OpenMP in the 2CPU with 3GPU machine. In addition, in two of the three applications, the multi-GPU OpenACC compiler outperforms the single GPU system where hand-written CUDA programs run.","2332-5690","978-0-7695-5117-3","10.1109/ICPP.2013.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687359","OpenACC;Multi-GPU","Graphics processing units;Arrays;Kernel;Programming;Runtime;Distributed databases","application program interfaces;graphics processing units;multiprocessing systems;program compilers","multiGPU execution;graphics processing unit;OpenACC compiler;high level GPU programming models;GPU applications;directive-based GPU programming model;GPU OpenACC programs;runtime system;data movements;GPU memories;OpenACC API extension;application program interface;data parallel applications;hand-written CUDA programs;compute unified device architecture","","11","","23","","19 Dec 2013","","","IEEE","IEEE Conferences"
"Integral image computation on GPU","M. Chouchene; F. E. Sayadi; M. Atri; R. Tourki","Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences Monastir, Tunisia; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences Monastir, Tunisia; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences Monastir, Tunisia; Laboratory of Electronics and Microelectronics (EμE), Faculty of Sciences Monastir, Tunisia","10th International Multi-Conferences on Systems, Signals & Devices 2013 (SSD13)","22 Jul 2013","2013","","","1","4","In this paper we present an integral image algorithm that can run in real-time on a Graphics Processing Unit (GPU). Our system exploits the parallelisms in computation via the NVIDA CUDA programming model, which is a software platform for solving non-graphics problems in a massively parallel high performance fashion. We compare the performance of the parallel approach running on the GPU with the sequential CPU implementation across a range of image sizes.","","978-1-4673-6457-7","10.1109/SSD.2013.6564007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564007","Integral image;GPU;CPU;NVIDIA CUDA","Graphics processing units;Instruction sets;Programming;Computer architecture;Kernel;Computer vision;Laboratories","graphics processing units;parallel architectures;parallel programming","integral image computation;GPU;graphics processing unit;NVIDA;CUDA programming model;software platform;nongraphics problem solving;parallel high performance fashion","","2","","12","","22 Jul 2013","","","IEEE","IEEE Conferences"
"GPU-based non-binary LDPC decoder with weighted bit-reliability based algorithm","Z. Liu; R. Liu; L. Zhao","School of Electrical and Information Engineering, Beihang University, Beijing 100191, China; School of Electrical and Information Engineering, Beihang University, Beijing 100191, China; School of Electrical and Information Engineering, Beihang University, Beijing 100191, China","China Communications","29 May 2020","2020","17","5","78","88","In this paper, we present a graphics processing unit (GPU)-based implementation of a weighted bit-reliability based (wBRB) decoder for non-binary LDPC (NB-LDPC) codes. To achieve coalesced memory accesses, an efficient data structure for the wBRB algorithm is proposed. Based on the Single-Instruction Multiple-Threads (SIMT) programming model, a novel mapping strategy with high intra-frame parallelism is presented to improve the latency and throughput performance. Moreover, by using Single-Instruction Multiple-Data (SIMD) intrinsics, four 8-bit message elements are packed into a 32-bit unit and simultaneously processed. Experimental results show that the proposed wBRB decoder provides good tradeoff between error performance and throughput for the codes with relatively large column degrees or high rates.","1673-5447","","10.23919/JCC.2020.05.008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103919","non-binary LDPC;bit-reliability;GPU;SIMT;SIMD","Decoding;Iterative decoding;Complexity theory;Graphics processing units;Memory management;Data structures;Throughput","data structures;decoding;graphics processing units;multi-threading;parity check codes","GPU-based nonbinary LDPC decoder;graphics processing unit-based implementation;weighted bit-reliability based decoder;nonbinary LDPC codes;NB-LDPC;coalesced memory accesses;efficient data structure;wBRB algorithm;high intra-frame parallelism;8-bit message elements;32-bit unit;wBRB decoder;single-instruction multiple-data intrinsics;single-instruction multiple-threads programming model","","","","","","29 May 2020","","","IEEE","IEEE Magazines"
"GPU implementation of Hertzian Potential Formulation for simulation of nanosensors","D. Tartarini; A. Massaro","Scuola Superiore ISUFI, University of Salento, via Arnesano 16, Lecce 73100, Italy; Center for Bio-Molecular Nanotechnologies (CBN) of IIT, via Barsanti 1, 73010, Arnesano (LE), Italy","2011 Numerical Simulation of Optoelectronic Devices","10 Oct 2011","2011","","","109","110","The time domain modeling and simulation of electromagnetic (EM) waves interaction with nanodevices, at high spatial and time resolution, requires high computational power. In this paper we present an implementation of the Hertzian Potential Formulation (HPF) on the Graphics Processing Units (GPUs) through the NVIDIA's CUDA (Compute Unified Device Architecture)programming model. The results demonstrate that this GPU tool outperforms the CPU based HPF implementation, reaching a speedup from 30× to 70×.","2158-3242","978-1-61284-878-5","10.1109/NUSOD.2011.6041164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041164","GPU Implementation;Nanodevices;Hertzian Potential Formulation","Graphics processing unit;Computational modeling;Time domain analysis;Electromagnetic scattering;Optical waveguides;Nanoscale devices","computer graphics;nanophotonics;nanosensors;optical engineering computing;optical sensors;time-domain analysis","GPU;Hertzian potential formulation;nanosensors;time domain modeling;electromagnetic waves interaction;nanodevices;spatial resolution;time resolution;graphics processing units;compute unified device architecture programming model","","","","14","","10 Oct 2011","","","IEEE","IEEE Conferences"
"Physis: An implicitly parallel programming model for stencil computations on large-scale GPU-accelerated supercomputers","N. Maruyama; K. Sato; T. Nomura; S. Matsuoka","Tokyo Institute of Technology, JST, CREST 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan; Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan; Google, Inc. 6-10-1 Roppongi, Minato-ku, Tokyo, Japan; Tokyo Institute of Technology, JST, CREST & NII, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan","SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis","29 Dec 2011","2011","","","1","12","This paper proposes a compiler-based programming framework that automatically translates user-written structured grid code into scalable parallel implementation code for GPU-equipped clusters. To enable such automatic translations, we design a small set of declarative constructs that allow the user to express stencil computations in a portable and implicitly parallel manner. Our framework translates the user-written code into actual implementation code in CUDA for GPU acceleration and MPI for node-level parallelization with automatic optimizations such as computation and communication overlapping. We demonstrate the feasibility of such automatic translations by implementing several structured grid applications in our framework. Experimental results on the TSUBAME2.0 GPU-based supercomputer show that the performance is comparable as hand-written code and good strong and weak scalability up to 256 GPUs.","2167-4337","978-1-4503-0771-0","10.1145/2063384.2063398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114468","Domain Specific Languages;Application Framework;High Perforamnce Computing","Programming;Computational modeling;DSL;Graphics processing unit;Runtime;Optimization;Indexes","application program interfaces;graphics processing units;grid computing;message passing;parallel architectures;parallel machines;program compilers;program interpreters","Physis;parallel programming model;stencil computations;TSUBAME2.0 GPU-based supercomputer;compiler-based programming framework;user-written structured grid code translation;parallel implementation code;GPU-equipped clusters;CUDA;GPU acceleration;MPI;node-level parallelization;computation overlapping;communication overlapping;structured grid applications","","53","1","30","","29 Dec 2011","","","IEEE","IEEE Conferences"
"Infiniband-Verbs on GPU: A Case Study of Controlling an Infiniband Network Device from the GPU","L. Oden; H. Fröning; F. Pfreundt","Competence Center High Perfomance Comput., Fraunhofer Inst. for Ind. Math., Kaiserslautern, Germany; Inst. of Comput. Eng., Univ. of Heidelberg, Heidelberg, Germany; Competence Center High Perfomance Comput., Fraunhofer Inst. for Ind. Math., Kaiserslautern, Germany","2014 IEEE International Parallel & Distributed Processing Symposium Workshops","4 Dec 2014","2014","","","976","983","Due to their massive parallelism and high performance per watt GPUs gain high popularity in high performance computing and are a strong candidate for future exacscale systems. But communication and data transfer in GPU accelerated systems remain a challenging problem. Since the GPU normally is not able to control a network device, today a hybrid-programming model is preferred, whereby the GPU is used for calculation and the CPU handles the communication. As a result, communication between distributed GPUs suffers from unnecessary overhead, introduced by switching control flow from GPUs to CPUs and vice versa. In this work, we modify user space libraries and device drivers of GPUs and the Infiniband network device in a way to enable the GPU to control an Infiniband network device to independently source and sink communication requests without any involvements of the CPU. Our performance analysis shows the differences to hybrid communication models in detail, in particular that the CPU's advantage in generating work requests outshines the overhead associated with context switching. In other terms, our results show that complex networking protocols like IBVERBS are better handled by CPUs in spite of time penalties due to context switching, since overhead of work request generation cannot be parallelized and is not suitable with the high parallel programming model of GPUs.","","978-1-4799-4116-2","10.1109/IPDPSW.2014.111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969487","GPUs;Communication;Heterogeneous Clusters;Infiniband;RDMA","Graphics processing units;Data transfer;Registers;Performance evaluation;Context;Libraries;Instruction sets","graphics processing units;parallel programming","infiniband-verbs;GPU;infiniband network device;massive parallelism;high performance per watt;high performance computing;exacscale systems;data transfer;hybrid-programming model;IBVERBS;high parallel programming model","","9","2","24","","4 Dec 2014","","","IEEE","IEEE Conferences"
"Performance Analysis of Benchmarks for GPU-based Linear Programming Problem Solvers","U. A. Shah; S. Yousaf","Department of Computer Science and IT, University of Engineering and Technology, Peshawar, Pakistan; Department of Computer Science and IT, University of Engineering and Technology, Peshawar, Pakistan","2019 2nd International Conference on Communication, Computing and Digital systems (C-CODE)","4 Apr 2019","2019","","","132","136","The single instruction multiple threads (SIMT) architecture of modern graphics processing units (GPUs) shows great potential for efficiently solving compute-intensive linear programming (LP) problems. For benchmarking these GPU-based solutions, speedup is used to measure their relative performance gains with respect to CPU-based implementations. However, a methodological flaw has been observed in benchmarking these GPU-based LP problem solvers, namely, the magnitude of their speedup varies with the choice of CPU-based benchmark. In this paper, we analyze the performance of CPU-based LP problem solvers used to benchmark their GPU-based counterparts. More specifically, we consider benchmarks of two established GPU-based solutions for revised simplex method. The first solution is based on general-purpose computing on GPUs (GPGPU) programming model, and uses a custom-built CPU-based solution as benchmark. The second solution is an OpenGL-based solver, and uses the LP utility in GNU linear programming kit (GLPK) as benchmark. Further, we performed experiments to compare the efficiency of these CPU-based benchmarks with an LP tool present in a commercially available CPU-based software package called CPLEX. Our study reveals that the use of nonstandard benchmarks makes it unfair to quantitatively compare the claims made about speedups achieved by various GPU-based solvers. In order to facilitate decision making process for potential users of GPU-based solutions, we recommend that standard sequential benchmarks be used during any future attempts at developing GPU-based linear optimization tools.","","978-1-5386-9609-5","10.1109/C-CODE.2019.8680981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8680981","Graphics processing unit (GPU);General-purpose computing on GPUs (GPGPU);Linear programming (LP);Revised simplex method (RSM)","Benchmark testing;Linear programming;Graphics processing units;Standards;Libraries;Tools;Optimization","decision making;graphics processing units;linear programming;optimisation","compute-intensive linear programming problems;CPU-based implementations;CPU-based benchmark;CPU-based LP problem solvers;GPU-based counterparts;established GPU-based solutions;GPUs programming model;custom-built CPU-based solution;GNU linear programming kit;nonstandard benchmarks;GPU-based solvers;standard sequential benchmarks;GPU-based linear programming problem solvers;single instruction multiple threads architecture;graphics processing units;general-purpose computing;CPU-based software package","","1","","18","","4 Apr 2019","","","IEEE","IEEE Conferences"
"Accelerating frequency-domain simulations using small shared-memory CPU/GPU cluster","T. Topa; A. Noga; A. Karwowski","Silesian University of Technology, Gliwice, Poland; Silesian University of Technology, Gliwice, Poland; Silesian University of Technology, Gliwice, Poland","2016 21st International Conference on Microwave, Radar and Wireless Communications (MIKON)","16 Jun 2016","2016","","","1","4","Numerical approach to frequency response problems usually requires that the system governing equation is solved repeatedly at many frequencies. The computational efficiency of the overall process can be increased by departing from traditional sequential computing model in favor of utilizing the parallel processing capability commonly offered by modern hardware. In this paper, we consider a hybrid programming pattern, OpenMP + CUDA, from the perspective of a user of a rather typical low-cost multi-core CPU-based workstation that can accommodate up to four GPUs. Such the small-scale heterogeneous platforms have recently gained wide popularity in scientific computing as an inexpensive massively parallel architecture. The relevant programming model issues and performance questions are addressed. Experimental results for the example physics problem, that is, the electromagnetic scattering from perfectly electrically conducting body, show that significant performance improvement can be attained with the OpenMP + CUDA programming model.","","978-1-5090-2214-4","10.1109/MIKON.2016.7492098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492098","GPU;CUDA;milticore CPU;OpenMP;MoM","Graphics processing units;Method of moments;Mathematical model;Kernel;Instruction sets;Computational modeling;Programming","electromagnetic wave scattering;frequency response;frequency-domain analysis;graphics processing units;microprocessor chips;multiprocessing systems;parallel architectures","frequency-domain simulations;graphics processing units;small shared-memory CPU-GPU cluster;frequency response problems;computational efficiency;sequential computing model;parallel processing capability;hybrid programming pattern;multicore CPU-based workstation;electromagnetic scattering;OpenMP + CUDA programming model","","","","11","","16 Jun 2016","","","IEEE","IEEE Conferences"
"Time-Domain Computational Electromagnetics Algorithms for GPU Based Computers","P. P. M. So","Department of Electrical and Computer Engineering, University of Victoria, Victoria, BC, Canada. email: Poman.So@ECE.UVic.CA","EUROCON 2007 - The International Conference on "Computer as a Tool"","26 Dec 2007","2007","","","1","4","Time-domain computational electromagnetic algorithms such as FDTD and TLM require computers with superb processing power and large memory capacity. Grid computing network, cluster computer and massively parallel supercomputers have been the hardware of choices for running powerful modelling tools based on these methods. As a result, high performance modelling tools are only available to elite groups of researchers and big corporations. Stream computing, a new technology that harnesses the tremendous numerical processing power of advanced graphics processing units for general purpose numerical computation, is going to bring high performance time-domain modelling tools to the EM community. This paper reviews the emerging GPU technologies and programming models. Two modelling examples are also used to illustrate the suitability of GPU computing for time-domain electromagnetics.","","978-1-4244-0812-2","10.1109/EURCON.2007.4400480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400480","CEM;TLM;Time Domain;GPU Computing;Stream Computing","Time domain analysis;Computational electromagnetics;Clustering algorithms;Grid computing;High performance computing;Finite difference methods;Computer networks;Concurrent computing;Supercomputers;Hardware","computational electromagnetics;computer graphic equipment;finite difference time-domain analysis;grid computing;parallel processing","time-domain computational electromagnetics algorithm;GPU based computer;grid computing network;cluster computer;parallel supercomputer;high performance modelling tool;stream computing technology;general purpose numerical computation;graphics processing unit;programming model","","3","","8","","26 Dec 2007","","","IEEE","IEEE Conferences"
"Scalability of Higher-Order Discontinuous Galerkin FEM Computations for Solving Electromagnetic Wave Propagation Problems on GPU Clusters","N. Gödel; N. Nunn; T. Warburton; M. Clemens","Faculty of Electrical Engineering, Chair for Theory of Electrical Engineering and Computational Electromagnetics, $^{1}$ Helmut-Schmidt-University, University of the Federal Armed Forces Hamburg, Hamburg, Germany; NA; $^{2}$ Computational and Applied Mathematics, Rice University, Houston, TX 77005 , USA; FB E, Chair for Electromagnetic Theory, $^{3}$Bergische Universität Wuppertal, Wuppertal, Germany","IEEE Transactions on Magnetics","19 Jul 2010","2010","46","8","3469","3472","A highly parallel implementation of Maxwell's equations in the time domain using a cluster of Graphics Processing Units (GPUs) is presented. The higher-order Discontinuous Galerkin Finite Element Method (DG-FEM) is used for spatial discretization since its characteristics are matching the parallelization design aspects of the NVIDIA Compute Unified Device Architecture (CUDA) programming model. Asynchronous data transfer is introduced to minimize parallelization overhead and improve parallel efficiency. The implementation is benchmarked with help of a realistic 3-D geometry of an electromagnetic compatibility problem.","1941-0069","","10.1109/TMAG.2010.2046022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5513006","CUDA;discontinuous Galerkin;FEM;GPGPU;GPU-computing;high order","Scalability;Electromagnetic propagation;Maxwell equations;Graphics;Moment methods;Finite element methods;Concurrent computing;Computer architecture;Parallel programming;Geometry","electromagnetic wave propagation;finite element analysis;Galerkin method;Maxwell equations","high-order discontinuous Galerkin FEM computation;electromagnetic wave propagation;GPU clusters;graphics processing units;finite element method;spatial discretization;NVIDIA compute unified device architecture programming model;asynchronous data transfer;realistic 3-D geometry;Maxwell equations","","36","","8","IEEE","19 Jul 2010","","","IEEE","IEEE Journals"
"Improving Utility of GPU in Accelerating Industrial Applications With User-Centered Automatic Code Translation","P. Yang; F. Dong; V. Codreanu; D. Williams; J. B. T. M. Roerdink; B. Liu; A. Anvari-Moghaddam; G. Min","Department of Computer Science, Liverpool John Moores University, Liverpool, U.K.; University of Bedfordshire, Luton, U.K.; SURFsara, Amsterdam, The Netherlands; Faculteit Wiskunde en Natuurwetenschappen, Rijksuniversiteit Groningen, Groningen, The Netherlands; Johann Bernoulli Institute for Mathematics and Computer Science, University of Groningen, Groningen, The Netherlands; Department of Computer Science and Technology, University of Bedfordshire, Luton, U.K.; Department of Energy Technology, Aalborg University, Aalborg, Denmark; Department of Mathematics and Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.","IEEE Transactions on Industrial Informatics","4 Apr 2018","2018","14","4","1347","1360","Small to medium enterprises (SMEs), particularly those whose business is focused on developing innovative produces, are limited by a major bottleneck in the speed of computation in many applications. The recent developments in GPUs have been the marked increase in their versatility in many computational areas. But due to the lack of specialist GPUprogramming skills, the explosion of GPU power has not been fully utilized in general SME applications by inexperienced users. Also, the existing automatic CPU-to-GPU code translators are mainly designed for research purposes with poor user interface design and are hard to use. Little attentions have been paid to the applicability, usability, and learnability of these tools for normal users. In this paper, we present an online automated CPU-to-GPU source translation system (GPSME) for inexperienced users to utilize the GPU capability in accelerating general SME applications. This system designs and implements a directive programming model with a new kernel generation scheme and memory management hierarchy to optimize its performance. A web service interface is designed for inexperienced users to easily and flexibly invoke the automatic resource translator. Our experiments with nonexpert GPU users in four SMEs reflect that a GPSME system can efficiently accelerate real-world applications with at least 4× and have a better applicability, usability, and learnability than the existing automatic CPU-to-GPU source translators.","1941-0050","","10.1109/TII.2017.2731362","European Commission and the Engineering and Physical Sciences Research Council within the iManageCancer(grant numbers:611140); MyHealthAvatar(grant numbers:600929); MyLifeHub(grant numbers:EP/L023830); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990251","Automatic translation;graphics processing unit (GPU);parallel computing;usability","Graphics processing units;Tools;Acceleration;Programming;Usability;C++ languages;Linux","C language;graphics processing units;performance evaluation;service-oriented architecture;small-to-medium enterprises;storage management;user interfaces;Web services","industrial applications;automatic code translation;medium enterprises;SMEs;computational areas;GPU power;general SME applications;inexperienced users;existing automatic CPU-to-GPU code translators;poor user interface design;normal users;CPU-to-GPU source translation system;GPU capability;automatic resource translator;nonexpert GPU users;GPSME system;real-world applications;CPU-to-GPU source translators;specialist GPU programming skills","","","","41","IEEE","24 Jul 2017","","","IEEE","IEEE Journals"
"GPU Accelerated Tensor Computation of Hadamard Product for Machine Learning Applications","K. M. A. Hasan; S. Chakraborty","Khulna University of Engineering & Technology,Computer Science and Engineering Department,Khulna,Bangladesh; Khulna University of Engineering & Technology,Computer Science and Engineering Department,Khulna,Bangladesh","2021 International Conference on Information and Communication Technology for Sustainable Development (ICICT4SD)","12 Apr 2021","2021","","","1","5","The computation on Graphics Processing Unit (GPU) has come out as a new cost-effective parallel computing paradigm for high performance computing that makes possible to process large scale data in parallel. GPU is designed to perform complex mathematical and geometric tasks which are primarily used for 3D graphics related functions. It is also possible to use GPU for non-graphics or general-purpose computation, called General Purpose Computing on GPU (GPGPU), a sub-discipline of High-Performance Computing (HPC). The use of GPU, along with CPU to accelerate more complex scientific, engineering and mathematical tasks is known as GPU Accelerated Computing. In this paper, we propose an efficient tensor computation for Hadamard Product (HP) which is directly applied in machine learning applications especially in Long Short-Term Memory (LSTM). The HP computation becomes complex when higher order tensors with millions of data is considered. Therefore, the only CPU-based traditional serial operation becomes tedious and inefficient. The contribution of this paper is in two fold; first we have developed efficient algorithms for higher order tensors by dimension conversion. Then we apply the algorithm in GPU to speed up the computation. To apply in GPU, we develop efficient partitioning scheme of higher order tensors. We have used CUDA (Compute Unified Device Architecture) C programming model developed by NVIDIA to implement the algorithm. We compared these algorithms with Traditional Multidimensional Array (TMA) based algorithm and found improved results.","","978-1-6654-1460-9","10.1109/ICICT4SD50815.2021.9396980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396980","GPU Computing;HPC;Tensor computation;Hadamard Product;CUDA C","Machine learning algorithms;Tensors;Graphics processing units;Machine learning;Partitioning algorithms;Acceleration;Task analysis","C language;graphics processing units;learning (artificial intelligence);mathematics computing;parallel architectures;parallel programming;recurrent neural nets;tensors","higher order tensor partitioning scheme;dimension conversion;LSTM;long short-term memory;HPC;GPGPU;general-purpose computing on GPU;GPU accelerated tensor computation;compute unified device architecture;CUDA C programming model;HP computation;GPU accelerated computing;geometric tasks;complex mathematical tasks;high performance computing;cost-effective parallel computing paradigm;graphics processing unit;machine learning applications;Hadamard Product","","","","17","","12 Apr 2021","","","IEEE","IEEE Conferences"
"Distributed Interactive Visualization Using GPU-Optimized Spark","S. Hong; J. Choi; W. -K. Jeong","Ulsan National Institute of Science and Technology, Ulsan, South Korea; Ulsan National Institute of Science and Technology, Ulsan, South Korea; Korea University, Seoul, South Korea","IEEE Transactions on Visualization and Computer Graphics","29 Jul 2021","2021","27","9","3670","3684","With the advent of advances in imaging and computing technologies, large-scale data acquisition and processing have become commonplace in many science and engineering disciplines. Conventional workflows for large-scale data processing usually rely on in-house or commercial software that are designed for domain-specific computing tasks. Recent advances in MapReduce, which was originally developed for batch processing textual data via a simplified programming model of the map and reduce functions, have expanded its applications to more general tasks in big-data processing, such as scientific computing, and biomedical image processing. However, as shown in previous work, volume rendering and visualization using MapReduce is still considered challenging and impractical owing to the disk-based, batch-processing nature of its computing model. In this article, contrary to this common belief, we show that the MapReduce computing model can be effectively used for interactive visualization. Our proposed system is a novel extension of Spark, one of the most popular open-source MapReduce frameworks, which offers GPU-accelerated MapReduce computing. To minimize CPU-GPU communication and overcome slow, disk-based shuffle performance, the proposed system supports GPU in-memory caching and MPI-based direct communication between compute nodes. To allow for GPU-accelerated in-situ visualization using raster graphics in Spark, we leveraged the CUDA-OpenGL interoperability, resulting in faster processing speeds by several orders of magnitude compared to conventional MapReduce systems. We demonstrate the performance of our system via several volume processing and visualization tasks, such as direct volume rendering, iso-surface extraction, and numerical simulations with in-situ visualization.","1941-0506","","10.1109/TVCG.2020.2990894","National Research Foundation of Korea(grant numbers:NRF-2017R1D1A1A09000841); Ministry of Science and ICT(grant numbers:NRF-2019M3E5D2A01063819,NRF-2014K1A3A1A05034557); Korea Health Industry Development Institute(grant numbers:HI18C0316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079657","MapReduce;spark;GPU;distributed rendering;in-situ visualization","Data visualization;Sparks;Graphics processing units;Computational modeling;Rendering (computer graphics);Programming;Task analysis","computer graphic equipment;computer graphics;data acquisition;data handling;data visualisation;graphics processing units;medical image processing;message passing;open systems;parallel architectures;parallel processing;public domain software;rendering (computer graphics)","CPU-GPU communication;disk-based shuffle performance;GPU in-memory caching;MPI-based direct communication;compute nodes;GPU-accelerated in-situ visualization;faster processing speeds;conventional MapReduce systems;volume processing;visualization tasks;direct volume rendering;distributed interactive visualization;GPU-optimized Spark;imaging computing technologies;large-scale data acquisition;conventional workflows;large-scale data processing;commercial software;domain-specific computing tasks;batch processing textual data;simplified programming model;general tasks;big-data processing;scientific computing;biomedical image processing;batch-processing nature;MapReduce computing model;popular open-source MapReduce frameworks;GPU-accelerated MapReduce computing","","2","","45","IEEE","27 Apr 2020","","","IEEE","IEEE Journals"
"A GPU-based Fast Solution for Riesz Space Fractional Reaction-Diffusion Equation","Q. Wang; J. Liu; C. Gong; Y. Zhang; Z. Xing","Sci. & Technol. on Parallel & Distrib. Process. Lab., Nat. Univ. of Defense Technol., Changsha, China; Sci. & Technol. on Parallel & Distrib. Process. Lab., Nat. Univ. of Defense Technol., Changsha, China; Sci. & Technol. on Parallel & Distrib. Process. Lab., Nat. Univ. of Defense Technol., Changsha, China; Sci. & Technol. on Parallel & Distrib. Process. Lab., Nat. Univ. of Defense Technol., Changsha, China; Sci. & Technol. on Parallel & Distrib. Process. Lab., Nat. Univ. of Defense Technol., Changsha, China","2015 18th International Conference on Network-Based Information Systems","10 Dec 2015","2015","","","317","323","The fast numerical solutions of Riesz fractional equation have computational cost of O(NMlogM), where M, N are the number of grid points and time steps. In this paper, we present a GPU-based fast solution for Riesz space fractional equation. The GPU-based fast solution, which is based on the fast method using FFT and implemented with CUDA programming model, consists of parallel FFT, vector-vector addition and vector-vector multiplication on GPU. The experimental results show that the GPU-based fast solution compares well with the exact solution. Compared to the known parallel fast solution on 8-core Intel E5-2670 CPU, the overall performance speedup on NVIDIA GTX650 GPU reaches 2.12 times and that on NVIDIA K20C GPU achieves 10.93 times.","","978-1-4799-9942-2","10.1109/NBiS.2015.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350638","Parallel computing;GPU;fractional differential equation;FFT;fast solution","Graphics processing units;Instruction sets;Mathematical model;Arrays;Yttrium;Approximation methods;Parallel processing","differential equations;fast Fourier transforms;graphics processing units;mathematics computing;parallel architectures;reaction-diffusion systems;vectors","GPU-based fast solution;Riesz space fractional reaction-diffusion equation;numerical solutions;Riesz fractional equation;computational cost;CUDA programming model;parallel FFT;vector-vector addition;vector-vector multiplication;NVIDIA GTX650 GPU;NVIDIA K20C GPU;fractional differential equation","","2","","30","","10 Dec 2015","","","IEEE","IEEE Conferences"
"GCMR: A GPU Cluster-Based MapReduce Framework for Large-Scale Data Processing","Yiru Guo; Weiguo Liu; B. Gong; G. Voss; W. Muller-Wittig","Sch. of Comput. Sci. & Technol., Shandong Univ., Jinan, China; Sch. of Comput. Sci. & Technol., Shandong Univ., Jinan, China; Sch. of Comput. Sci. & Technol., Shandong Univ., Jinan, China; Fraunhofer IDM@NTU, Nanyang Technol. Univ., Singapore, Singapore; Fraunhofer IDM@NTU, Nanyang Technol. Univ., Singapore, Singapore","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","12 Jun 2014","2013","","","580","586","MapReduce is a very popular programming model to support parallel and distributed large-scale data processing. There have been a lot of efforts to implement this model on commodity GPU-based systems. However, most of these implementations can only work on a single GPU. And they can not be used to process large-scale datasets. In this paper, we present a new approach to design the MapReduce framework on GPU clusters for handling large-scale data processing. We have used Compute Unified Device Architectures (CUDA) and MPI parallel programming models to implement this framework. To derive an efficient mapping onto GPU clusters, we introduce a two-level parallelization approach: the inter node level and intra node level parallelization. Furthermore in order to improve the overall MapReduce efficiency, a multi-threading scheme is used to overlap the communication and computation on a multi-GPU node. Compared to previous GPU-based MapReduce implementations, our implementation, called GCMR, achieves speedups up to 2.6 on a single node and up to 9.1 on 4 nodes of a Tesla S1060 quad-GPU cluster system for processing small datasets. It also shows very good scalability for processing large-scale datasets on the cluster system.","","978-0-7695-5088-6","10.1109/HPCC.and.EUC.2013.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6831970","MapReduce;CUDA;MPI;GPU Cluster","Graphics processing units;Instruction sets;Computational modeling;Computer architecture;Acceleration;Writing;Data processing","application program interfaces;graphics processing units;message passing;parallel programming","GCMR;GPU cluster-based MapReduce framework;distributed large-scale data processing;commodity GPU-based systems;compute unified device architectures;CUDA;MPI parallel programming models;inter node level parallelization;intra node level parallelization;MapReduce efficiency;multithreading scheme;multi-GPU node;GPU-based MapReduce;single node;Tesla S1060 quad-GPU cluster system;large-scale datasets","","2","","15","","12 Jun 2014","","","IEEE","IEEE Conferences"
"NVIDIA A100 Tensor Core GPU: Performance and Innovation","J. Choquette; W. Gandhi; O. Giroux; N. Stam; R. Krashinsky","NVIDIA, Singapore; NVIDIA, Singapore; NVIDIA, Singapore; NVIDIA, Singapore; NVIDIA, Singapore","IEEE Micro","29 Mar 2021","2021","41","2","29","35","NVIDIA A100 Tensor Core GPU is NVIDIA's latest flagship GPU. It has been designed with many new innovative features to provide performance and capabilities for HPC, AI, and data analytics workloads. Feature enhancements include a Third-Generation Tensor Core, new asynchronous data movement and programming model, enhanced L2 cache, HBM2 DRAM, and third-generation NVIDIA NVLink I/O.","1937-4143","","10.1109/MM.2021.3061394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361255","GPU;A100;NVLink;Deep Learning;Tensor Core;CUDA;C++20","Graphics processing units;Tensors;Bandwidth;Throughput;Parallel processing;Benchmark testing;Artificial intelligence","cache storage;data analysis;DRAM chips;graphics processing units;multiprocessing systems;parallel architectures;parallel processing;tensors","third-generation NVIDIA NVLink;Third-Generation Tensor Core;innovative features;NVIDIA's latest flagship GPU;NVIDIA A100 Tensor Core GPU","","4","","5","IEEE","23 Feb 2021","","","IEEE","IEEE Magazines"
"Automatic Parallelization of GPU Applications Using OpenCL","L. D. Solano-Quinde; B. M. Bode; A. K. Somani","Dept. of Electr., Univ. of Cuenca, Cuenca, Ecuador; Nat. Center for Supercomput. Applic., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Electr. & Comput. Eng., Iowa State Univ., Ames, IA, USA","2015 Asia-Pacific Conference on Computer Aided System Engineering","5 Oct 2015","2015","","","276","283","Graphics Processing Units (GPUs) have been successfully used to accelerate scientific applications due to their computation power and the availability of programming languages that make more approachable writing scientific applications for GPUs. However, since the programming model of GPUs requires offloading all the data to the GPU memory, the memory footprint of the application is limited to the size of the GPU memory. Multi-GPU systems can make memory limited problems tractable by parallelizing the computation and data among the available GPUs. Parallelizing applications written for running on single-GPU systems can be done (i) at runtime through an environment that captures the memory operations and kernel calls and distributes among the available GPUs, and (ii) at compile time through a pre-compiler that transforms the application for decomposing the data and computation among the available GPUs. In this paper we propose a framework and implement a tool that transforms an OpenCL application written to run on single-GPU systems into one that runs on multi-GPU systems. Based on data dependencies and data usage analysis, the application is transformed to decompose data and computation among the available GPUs. To reduce the data transfer overhead, computation-communication overlapping techniques are utilized. We tested our tool using two applications with different data transfer requirements, for the application with no data transfer requirements, a linear speedup is achieved, while for the application with data transfers, the computation-communication overlapping reduces the communication overhead by 40%.","","978-1-4799-7588-4","10.1109/APCASE.2015.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7287032","GPU;OpenCL;Program Transformation","Kernel;Arrays;Graphics processing units;Data transfer;Algorithms;XML;Memory management","graphics processing units;parallel processing","automatic parallelization;GPU applications;OpenCL;graphics processing units;scientific applications;computation power;programming languages;programming model;data offloading;GPU memory;memory footprint;multiGPU systems;parallelizing applications;single-GPU systems;memory operations;kernel calls;precompiler;data decomposition;data dependencies;data usage analysis;data transfer overhead;computation-communication overlapping techniques;data transfer requirements;linear speedup;communication overhead","","1","","10","","5 Oct 2015","","","IEEE","IEEE Conferences"
"An OpenMP-CUDA Implementation of Multilevel Fast Multipole Algorithm for Electromagnetic Simulation on Multi-GPU Computing Systems","J. Guan; S. Yan; J. Jin","Center for Computational Electromagnetics, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Center for Computational Electromagnetics, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Center for Computational Electromagnetics, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA","IEEE Transactions on Antennas and Propagation","2 Jul 2013","2013","61","7","3607","3616","A multi-GPU implementation of the multilevel fast multipole algorithm (MLFMA) based on the hybrid OpenMPCUDA parallel programming model (OpenMP-CUDA-MLFMA) is presented for computing electromagnetic scattering of a three-dimensional conducting object. The proposed hierarchical parallelization strategy ensures a high computational throughput for the GPU calculation. The resulting OpenMP-based multi-GPU implementation is capable of solving real-life problems with over one million unknowns with a remarkable speed-up. The radar cross sections of a few benchmark objects are calculated to demonstrate the accuracy of the solution. The results are compared with those from the CPU-based MLFMA and measurements. The capability and efficiency of the presented method are analyzed through the examples of a sphere, an aerocraft, and a missile-like object. Compared with the 8-threaded CPU-based MLFMA, the OpenMP-CUDA-MLFMA method can achieve from 5 to 20 total speed-up ratios.","1558-2221","","10.1109/TAP.2013.2258882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504730","CUDA;electromagnetic scattering;hybrid parallel programming model;multi-GPU;multilevel fast multipole algorithm;OpenMP;radar cross section","Graphics processing units;Instruction sets;Method of moments;Computer architecture;Antenna radiation patterns;Computational modeling","computational electromagnetics;electromagnetic wave scattering;graphics processing units;parallel architectures","multilevel fast multipole algorithm;electromagnetic simulation;multi-GPU computing systems;OpenMP-CUDA parallel programming model;electromagnetic scattering;three-dimensional conducting object;radar cross sections;aerocraft;missile-like object","","49","","27","","18 Apr 2013","","","IEEE","IEEE Journals"
"Enhancing Blowfish file encryption algorithm through parallel computing on GPU","T. Mahajan; S. Masih","Devi Ahilya University, Indore, India; Devi Ahilya University, Indore, India","2015 International Conference on Computer, Communication and Control (IC4)","11 Jan 2016","2015","","","1","4","Parallel computing can provide fast execution of the program as compared to sequential computing. Graphical Processing Unit can be used for parallel computing as it gives the advantage of multiple cores. Purpose of parallel implementation of Blowfish cryptography algorithm is to improve the speed up of encryption and decryption so that large files also can be communicated on the network in secure and efficient way. This paper demonstrates the way of implementing Blowfish cryptography algorithm on GPU for improving performance. This implementation uses GPGPU and CUDA. CUDA is used as a programming model for implementing on the GPU. The experiment shows multifold difference in performance of CPU and GPU in encryption-decryption of large files.","","978-1-4799-8164-9","10.1109/IC4.2015.7375604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375604","GPU-Graphical Processing Unit;CPU-Central Processing Unit;CUDA-Computer Unified Device Architecture;GPGPU-General Purpose computation on Graphical Processing Unit","Graphics processing units;Encryption;Programming;Algorithm design and analysis;Conferences","cryptography;graphics processing units;parallel architectures;parallel programming","Blowfish file encryption algorithm;parallel computing;program execution;sequential computing;graphical processing unit;parallel implementation;Blowfish cryptography algorithm;GPGPU;CUDA;programming model;large file encryption-decryption;computer unified device architecture;general purpose computation on graphical processing unit","","2","","10","","11 Jan 2016","","","IEEE","IEEE Conferences"
"HPGA: A High-Performance Graph Analytics Framework on the GPU","H. Yang; H. Su; M. Wen; C. Zhang","Department of Computer, National University of Defense Technology, Changsha, 410000, China; Department of Computer, National University of Defense Technology, Changsha, 410000, China; Department of Computer, National University of Defense Technology, Changsha, 410000, China; Department of Computer, National University of Defense Technology, Changsha, 410000, China","2018 International Conference on Information Systems and Computer Aided Education (ICISCAE)","14 Mar 2019","2018","","","488","492","In recent years, the rapidly growing use of graphs has sparked parallel graph analytics frameworks for leveraging the massive hardware resources, specifically graphics processing units (GPUs). However, the issues of the unpredictable control flows, memory divergence, and the complexity of programming have restricted high-level GPU graph libraries. In this work, we present HPGA, a high performance parallel graph analytics framework targeting the GPU. HPGA implements an abstraction which maps vertex programs to generalized sparse matrix operations on GPUs for delivering high performance. HPGA incorporates high-performance GPU computing primitives and optimization strategies with a high-level programming model. We evaluate the performance of HPGA for three graph primitives (BFS, SSSP, PageRank) with large-scale datasets. The experimental results show that HPGA matches or even exceeds the performance of MapGraph and nvGRAPH, two state-of-the-art GPU graph libraries.","","978-1-5386-5738-6","10.1109/ICISCAE.2018.8666877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666877","Graph Analytics;High-performance Computing;GPU","Graphics processing units;Sparse matrices;Programming;Computational modeling;Arrays;Optimization","graph theory;graphics processing units;sparse matrices","HPGA;high-performance graph analytics framework;parallel graph analytics frameworks;massive hardware resources;high-level GPU graph libraries;high performance;generalized sparse matrix operations;high-performance GPU;high-level programming model;GPU graph libraries;vertex programs","","","","20","","14 Mar 2019","","","IEEE","IEEE Conferences"
"Compiling and Optimizing Java 8 Programs for GPU Execution","K. Ishizaki; A. Hayashi; G. Koblents; V. Sarkar","IBM Res., Tokyo, Japan; NA; IBM Canada, Canada; NA","2015 International Conference on Parallel Architecture and Compilation (PACT)","10 Mar 2016","2015","","","419","431","GPUs can enable significant performance improvements for certain classes of data parallel applications and are widely used in recent computer systems. However, GPU execution currently requires explicit low-level operations such as 1) managing memory allocations and transfers between the host system and the GPU, 2) writing GPU kernels in a low-level programming model such as CUDA or OpenCL, and 3) optimizing the kernels by utilizing appropriate memory types on the GPU. Because of this complexity, in many cases, only expert programmers can exploit the computational capabilities of GPUs through the CUDA/OpenCL languages. This is unfortunate since a large number of programmers use high-level languages, such as Java, due to their advantages of productivity, safety, and platform portability, but would still like to exploit the performance benefits of GPUs. Thus, one challenging problem is how to utilize GPUs while allowing programmers to continue to benefit from the productivity advantages of languages like Java. This paper presents a just-in-time (JIT) compiler that can generate and optimize GPU code from a pure Java program written using lambda expressions with the new parallel streams APIs in Java 8. These APIs allow Java programmers to express data parallelism at a higher level than threads and tasks. Our approach translates lambda expressions with parallel streams APIs in Java 8 into GPU code and automatically generates runtime calls that handle the low-level operations mentioned above. Additionally, our optimization techniques 1) allocate and align the starting address of the Java array body in the GPUs with the memory transaction boundary to increase memory bandwidth, 2) utilize read-only cache for array accesses to increase memory efficiency in GPUs, and 3) eliminate redundant data transfer between the host and the GPU. The compiler also performs loop versioning for eliminating redundant exception checks and for supporting virtual method invocations within GPU kernels. These features and optimizations are supported and automatically performed by a JIT compiler that is built on top of a production version of the IBM Java 8 runtime environment. Our experimental results on an NVIDIA Tesla GPU show significant performance improvements over sequential execution (127.9 × geometric mean) and parallel execution (3.3 × geometric mean) for eight Java 8 benchmark programs running on a 160-thread POWER8 machine. This paper also includes an in-depth analysis of GPU execution to show the impact of our optimization techniques by selectively disabling each optimization. Our experimental results show a geometric-mean speed-up of 1.15 × in the GPU kernel over state-of-the-art approaches. Overall, our JIT compiler can improve the performance of Java 8 programs by automatically leveraging the computational capability of GPUs.","1089-795X","978-1-4673-9524-3","10.1109/PACT.2015.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429325","JIT compiler;GPU;Java 8;Parallel streams","Graphics processing units;Java;Optimization;Kernel;Arrays;Semantics","application program interfaces;cache storage;configuration management;graphics processing units;Java;parallel processing;program compilers","Java 8 programs;GPU execution;data parallel applications;CUDA/OpenCL languages;Java language;just-in-time compiler;JIT compiler;GPU code optimization;lambda expressions;parallel streams API;data parallelism;memory transaction boundary;memory bandwidth;read-only cache;memory efficiency;loop versioning;virtual method invocations;GPU kernels;IBM Java 8 runtime environment;NVIDIA Tesla GPU;thread POWER8 machine;graphics processing unit","","24","3","37","","10 Mar 2016","","","IEEE","IEEE Conferences"
"OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future NUMA-Based Multi-GPU Systems","C. Xie; F. Xin; M. Chen; S. L. Song","Pacific Northwest National Lab (PNNL), University of Houston; ECMOS Lab, University of Houston,ECE Department; School of Computer Science and Software Engineering, East China Normal University; Pacific Northwest National Lab (PNNL), University of Houston","2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)","6 Feb 2020","2019","","","53","65","With the strong computation capability, NUMA-based multi-GPU system is a promising candidate to provide sustainable and scalable performance for Virtual Reality (VR) applications and deliver the excellent user experience. However, the entire multi-GPU system is viewed as a single GPU under the single programming model which greatly ignores the data locality among VR rendering tasks during the workload distribution, leading to tremendous remote memory accesses among GPU models (GPMs). The limited inter- GPM link bandwidth (e.g., 64GB/s for NVlink) becomes the major obstacle when executing VR applications in the multi-GPU system. By conducting comprehensive characterizations on different kinds of parallel rendering frameworks, we observe that distributing the rendering object along with its required data per GPM can reduce the inter-GPM memory accesses. However, this object-level rendering still faces two major challenges in NUMA-based multi- GPU system: (1) the large data locality between the left and right views of the same object and the data sharing among different objects and (2) the unbalanced workloads induced by the software- level distribution and composition mechanisms. To tackle these challenges, we propose object-oriented VR rendering framework (OO-VR) that conducts the software and hardware co-optimization to provide a NUMA friendly solution for VR multi-view rendering in NUMA-based multi-GPU systems. We first propose an object-oriented VR programming model to exploit the data sharing between two views of the same object and group objects into batches based on their texture sharing levels. Then, we design an object aware runtime batch distribution engine and distributed hardware composition unit to achieve the balanced workloads among GPMs and further improve the performance of VR rendering. Finally, evaluations on our VR featured simulator show that OO-VR provides 1.58x overall performance improvement and 76% inter-GPM memory traffic reduction over the state-of- the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly performance scalability for the future larger multi-GPU scenarios with ever increasing asymmetric bandwidth between local and remote memory.","2575-713X","978-1-4503-6669-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8980302","","","graphics processing units;multiprocessing systems;object-oriented programming;parallel processing;rendering (computer graphics);virtual reality","VR multiview rendering;NUMA-based multiGPU system;object-oriented VR programming model;data sharing;object aware runtime batch distribution engine;NUMA friendly performance scalability;NUMA friendly object-oriented VR rendering framework;virtual reality applications;single GPU;single programming model;data locality;VR rendering tasks;GPU models;VR applications;parallel rendering frameworks;inter-GPM memory accesses;object-level rendering;composition mechanisms;software-level distribution;inter-GPM link bandwidth;inter-GPM memory traffic reduction;OO-VR framework","","","","44","","6 Feb 2020","","","IEEE","IEEE Conferences"
"Efficient implementation of apriori algorithm on HDFS using GPU","M. Tiwary; A. K. Sahoo; R. Misra","Department of Information Technology, C.V. Raman College of Engineering, Bhubaneswar, India; Department of Information Technology, C.V. Raman College of Engineering, Bhubaneswar, India; Department of Information Technology, C.V. Raman College of Engineering, Bhubaneswar, India","2014 International Conference on High Performance Computing and Applications (ICHPCA)","19 Feb 2015","2014","","","1","7","A very efficient distributed processing framework is provided by Hadoop. For processing big data, Hadoop uses map-reduce programming model. The proposed technique uses parallel apriori mapreduce algorithm using high performance GPU. The computationally intensive operations of mapping phase are offloaded to GPU. Apriori is a very basic data mining algorithm which is used to determine the frequent item sets in the transactional database. In Hadoop, big transactional database are stored in structured form. When the size of transactional database is big, very fast apriori technique is required to solve the problem. Past researches show a clear view of solving data mining operations in heterogeneous environment which increase the performance with a very high rate than older serial execution techniques. This paper introduces integration of GPU in mapreduce programming model to solve the apriori data mining technique in a very time efficient manner. For our experimental implementation, we use NVIDIA's GPU and for the integration process, we use JCUDA and JNI.","","978-1-4799-5958-7","10.1109/ICHPCA.2014.7045323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045323","Hadoop;Map-reduce;CUDA;GPU;Apriori","Graphics processing units;Acceleration;Computer architecture;Databases;Integrated circuits;Kernel","Big Data;data integration;data mining;graphics processing units;parallel architectures;parallel programming;transaction processing","distributed processing framework;Hadoop;big data processing;Mapreduce programming model;parallel apriori Mapreduce algorithm;high performance GPU;mapping phase;data mining algorithm;big transactional database;data mining operations;heterogeneous environment;apriori data mining technique;NVIDIA's GPU;JCUDA;JNI","","7","","10","","19 Feb 2015","","","IEEE","IEEE Conferences"
"Accelerating FDTD algorithm using GPU computing","Z. Bo; X. Zheng-hui; R. Wu; L. Wei-ming; S. Xin-qing","School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China","2011 IEEE International Conference on Microwave Technology & Computational Electromagnetics","23 Jun 2011","2011","","","410","413","Hardware acceleration of Finite-Difference Time-Domain (FDTD) algorithm has always been an important part of FDTD research. In this essay, we discussed the advantage and feasibility of accelerate FDTD algorithm using Graphics Processing Unit (GPU). With the implement of lattice-threads mapping and other techniques, we proposed a GPU FDTD programming model that is both efficient and accurate. Then we simulated two projects using proposed model, the result shows good agreement with CPU computing while simulation speedup is 20 at minimum, thus proves the efficiency and great potential of GPU accelerating in FDTD research.","","978-1-4244-8559-8","10.1109/ICMTCE.2011.5915546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5915546","FDTD;GPU;Hardware Accelerating","Graphics processing unit;Finite difference methods;Time domain analysis;Acceleration;Instruction sets;Lattices;Computer architecture","coprocessors;finite difference time-domain analysis;mathematics computing","GPU computing;hardware acceleration;finite-difference time-domain algorithm;lattice-threads mapping;FDTD research;FDTD algorithm","","9","","5","","23 Jun 2011","","","IEEE","IEEE Conferences"
"GPU based parallel matrix exponential algorithm for large scale power system electromagnetic transient simulation","J. Zhao; J. Liu; P. Li; X. Fu; G. Song; C. Wang","Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China","2016 IEEE Innovative Smart Grid Technologies - Asia (ISGT-Asia)","26 Dec 2016","2016","","","110","114","In order to meet the demand of fast electromagnetic transient simulation for large-scale interconnected power systems, a new method of GPU based parallel matrix exponential algorithm for power system electromagnetic transient simulation is presented in this paper. Firstly, the hardware structure and programming model of GPU are introduced. Then the high data parallelism based on matrix exponential integration algorithm is proposed. Then, a simulation test is carried out for a wind farm system including 17 WTGs. The accuracy and efficiency of the proposed method are verified. The results show that the speed of parallel computing based on GPU is about 2 times faster than that of CPU and Matlab/SimPowerSystems.","2378-8542","978-1-5090-4303-3","10.1109/ISGT-Asia.2016.7796370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796370","matrix exponential;electromagnetic transient simulation;GPU;parallel computation","Graphics processing units;Computational modeling;Parallel processing;Mathematical model;Algorithm design and analysis;Sparse matrices;Power systems","graphics processing units;power engineering computing;power system interconnection;power system simulation;wind power plants","GPU-based parallel matrix exponential integration algorithm;large-scale interconnected power system electromagnetic transient simulation;GPU hardware structure;GPU programming model;data parallelism;wind farm system;WTG;parallel computing;Matlab-SimPower system","","3","","14","","26 Dec 2016","","","IEEE","IEEE Conferences"
"GPU Peer-to-Peer Techniques Applied to a Cluster Interconnect","R. Ammendola; M. Bernaschi; A. Biagioni; M. Bisson; M. Fatica; O. Frezza; F. Lo Cicero; A. Lonardo; E. Mastrostefano; P. S. Paolucci; D. Rossetti; F. Simula; L. Tosoratto; P. Vicini","Sezione Roma Tor Vergata, Ist. Naz. di Fis. Nucleare, Rome, Italy; Ist. Applicazioni Calcolo, Consiglio Naz. delle Ric., Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Ist. Applicazioni Calcolo, Consiglio Naz. delle Ric., Rome, Italy; NVIDIA Corp., CA, USA; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Dept. of Comput. Sci., Sapienza Univ., Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy; Sezione Roma, Ist. Naz. di Fis. Nucleare, Rome, Italy","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum","31 Oct 2013","2013","","","806","815","Modern GPUs support special protocols to exchange data directly across the PCI Express bus. While these protocols could be used to reduce GPU data transmission times, basically by avoiding staging to host memory, they require specific hardware features which are not available on current generation network adapters. In this paper we describe the architectural modifications required to implement peer-to-peer access to NVIDIA Fermi- and Kepler-class GPUs on an FPGA-based cluster interconnect. Besides, the current software implementation, which integrates this feature by minimally extending the RDMA programming model, is discussed, as well as some issues raised while employing it in a higher level API like MPI. Finally, the current limits of the technique are studied by analyzing the performance improvements on low-level benchmarks and on two GPU-accelerated applications, showing when and how they seem to benefit from the GPU peer-to-peer method.","","978-0-7695-4979-8","10.1109/IPDPSW.2013.128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650959","peer-to-peer;GPU;interconnection network;parallel computing","Graphics processing units;Peer-to-peer computing;Bandwidth;Protocols;Hardware;Benchmark testing;Switches","field programmable gate arrays;graphics processing units;peer-to-peer computing;peripheral interfaces","RDMA programming model;FPGA based cluster interconnect;NVIDIA Fermi- and Kepler class GPU;peer-to-peer access;architectural modifications;hardware features;host memory;GPU data transmission times;PCI express bus;exchange data;GPUs support;GPU peer-to-peer techniques","","24","","17","","31 Oct 2013","","","IEEE","IEEE Conferences"
"Performance Portability of Molecular Docking Miniapp On Leadership Computing Platforms","M. Thavappiragasam; A. Scheinberg; W. Elwasif; O. Hernandez; A. Sedova","Oak Ridge National Laboratory,Oak Ridge,Tennessee; Jubilee Development,Cambridge,Massachusetts; Oak Ridge National Laboratory,Oak Ridge,Tennessee; Oak Ridge National Laboratory,Oak Ridge,Tennessee; Oak Ridge National Laboratory,Oak Ridge,Tennessee","2020 IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC (P3HPC)","1 Jan 2021","2020","","","36","44","Rapidly changing computer architectures, such as those found at high-performance computing (HPC) facilities, present the need for mini-applications (miniapps) that capture essential algorithms used in large applications to test program performance and portability, aiding transitions to new systems. The COVID-19 pandemic has fueled a flurry of activity in computational drug discovery, including the use of supercomputers and GPU acceleration for massive virtual screens for therapeutics. Recent work targeting COVID-19 at the Oak Ridge Leadership Computing Facility (OLCF) used the GPU-accelerated program AutoDock-GPU to screen billions of compounds on the Summit supercomputer. In this paper we present the development of a new miniapp, miniAutoDock-GPU, that can be used to evaluate the performance and portability of GPU-accelerated protein-ligand docking programs on different computer architectures. These tests are especially relevant as facilities transition from petascale systems and prepare for upcoming exascale systems that will use a variety of GPU vendors. The key calculations, namely, the Lamarckian genetic algorithm combined with a local search using a Solis-Wets based random optimization algorithm, are implemented. We developed versions of the miniapp using several different programming models for GPU acceleration, including a version using the CUDA runtime API for NVIDIA GPUs, and the Kokkos middle-ware API which is facilitated by C++ template libraries. A third version, currently in progress, uses the HIP programming model. These efforts will help facilitate the transition to exascale systems for this important emerging HPC application, as well as its use on a wide range of heterogeneous platforms.","","978-1-6654-2287-1","10.1109/P3HPC51967.2020.00009","Battelle; U.S. Department of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309044","heterogeneous system;high-performance computing;performance portability;hybrid parallel programming model;molecular docking;drug discovery","Graphics processing units;Proteins;Optimization;Programming;Leadership;Hip;COVID-19","application program interfaces;biology computing;coprocessors;drugs;genetic algorithms;graphics processing units;mainframes;molecular biophysics;parallel architectures;parallel machines;parallel processing;proteins","molecular docking miniapp;Leadership Computing platforms;high-performance computing facilities;COVID-19 pandemic;computational drug discovery;supercomputers;GPU acceleration;Oak Ridge Leadership Computing Facility;GPU-accelerated program AutoDock-GPU;Summit supercomputer;miniAutoDock-GPU;GPU-accelerated protein-ligand docking programs;computer architectures;petascale systems;exascale systems;GPU vendors;Lamarckian genetic algorithm;random optimization algorithm;HIP programming model;HPC application;NVIDIA GPU;Kokkos middleware API","","1","","14","","1 Jan 2021","","","IEEE","IEEE Conferences"
"A Formal Instruction-level GPU Model for Scalable Verification","Y. Xing; B. Huang; A. Gupta; S. Malik",Princeton University; Princeton University; Princeton University; Princeton University,"2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","3 Jan 2019","2018","","","1","8","GPUs have been widely used to accelerate big-data inference applications and scientific computing through their parallelized hardware resources and programming model. Their extreme parallelism increases the possibility of bugs such as data races and un-coalesced memory accesses, and thus verifying program correctness is critical. State-of-the-art GPU program verification efforts mainly focus on analyzing application-level programs, e.g., in C, and suffer from the following limitations: (1) high false-positive rate due to coarse-grained abstraction of synchronization primitives, (2) high complexity of reasoning about pointer arithmetic, and (3) keeping up with an evolving API for developing application-level programs. In this paper, we address these limitations by modeling GPUs and reasoning about programs at the instruction level. We formally model the Nvidia GPU at the parallel execution thread (PTX) level using the recently proposed Instruction-Level Abstraction (ILA) model for accelerators. PTX is analogous to the Instruction-Set Architecture (ISA) of a general-purpose processor. Our formal ILA model of the GPU includes non-synchronization instructions as well as all synchronization primitives, enabling us to verify multithreaded programs. We demonstrate the applicability of our ILA model in scalable GPU program verification of data-race checking. The evaluation shows that our checker outperforms state-of-the-art GPU data race checkers with fewer false-positives and improved scalability.","1558-2434","978-1-4503-5950-4","10.1145/3240765.3240771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8587727","","Graphics processing units;Instruction sets;Synchronization;Computational modeling;Registers;Programming;Hardware","application program interfaces;graphics processing units;instruction sets;multiprocessing systems;multi-threading;program diagnostics;program verification;reasoning about programs","formal Instruction-level GPU model;scalable verification;big-data inference applications;parallelized hardware resources;programming model;data races;program correctness;application-level programs;coarse-grained abstraction;synchronization primitives;Nvidia GPU;parallel execution thread level;formal ILA model;nonsynchronization instructions;multithreaded programs;scalable GPU program verification;data-race checking;instruction-set architecture;GPU data race checkers;instruction-level abstraction model;GPU program verification efforts","","","","20","","3 Jan 2019","","","IEEE","IEEE Conferences"
"Unified Execution Mode in a GPU-Style Softcore","P. Thontirawong; P. Chongstitvatana","Dept. of Comput. Eng., Chulalongkorn Univ., Bangkok, Thailand; Dept. of Comput. Eng., Chulalongkorn Univ., Bangkok, Thailand","2013 International Conference on Information Science and Applications (ICISA)","15 Aug 2013","2013","","","1","4","A GPU-style processor has large amount of processing power on a given die compared to a general purpose processor. However, a Graphic Processing Unit must be executed in lock-step where a group of cores execute the same instruction. This constraint puts a real limitation on programming of a GPU. This work proposed a design of a processor that unifies the execution of Graphic Processing Units and a general purpose processor. The discussion of programming model of vectorised instructions and the extension to allow multi-cores to run independently is presented. The proposed design required only 3% additional resource compared to the original design. This design is suitable for embedded applications.","2162-9048","978-1-4799-0604-8","10.1109/ICISA.2013.6579365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579365","","Graphics processing units;Multicore processing;Registers;Logic gates;Programming;Benchmark testing;Process control","embedded systems;graphics processing units;multiprocessing systems;parallel processing;resource allocation","GPU-style softcore;GPU-style processor;processing power;graphic processing unit;lock-step;general purpose processor;programming model;vectorised instructions;multicores;unified execution mode;embedded applications","","","","7","","15 Aug 2013","","","IEEE","IEEE Conferences"
"Parallel Computing Accelerated Image Inpainting using GPU CUDA, Theano, and Tensorflow","H. T. R. Adie; I. A. Pradana; Pranowo","Department of Informatic Engineering, Universitas Atma Jaya Yogyakarta, Yogyakarta, Indonesia; Department of Informatic Engineering, Universitas Atma Jaya Yogyakarta, Yogyakarta, Indonesia; Department of Informatic Engineering, Universitas Atma Jaya Yogyakarta, Yogyakarta, Indonesia","2018 10th International Conference on Information Technology and Electrical Engineering (ICITEE)","15 Nov 2018","2018","","","621","625","Image inpainting refers to image restoration process that reconstruct damaged image to obtain it lost information based on existing information. PDE-based approach is commonly used for image interpolation especially inpainting. Since PDE process express convolution and continuous change, the approach may take a lot of computational resources and will run slow on standard computer CPU. To overcome that, GPU parallel computing method for PDE-based image inpainting are proposed. These days, some handy platform or frameworks to utilize GPU are already exist like CUDA, Theano and Tensorflow. CUDA is well-known as parallel computing platform and programming model to work with programming language such as C/C++. In other hand Theano and Tensorflow is a bit different thing, both of them is a machine learning framework based on Python that also able to utilize GPU. Although Theano and Tensorflow are specialized for machine learning and deep learning, the system is general enough to applied for computational process like image inpainting. The results of this work show benchmark performance of PDE image inpainting running on CPU using C++, Theano, and Tensorflow and on GPU with CUDA, Theano, and Tensorflow. The benchmark shows that parallel computing accelerated PDE image inpainting can run faster on GPU either with CUDA, Theano, or Tensorflow compared to PDE image inpainting running on CPU.","","978-1-5386-4739-4","10.1109/ICITEED.2018.8534858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8534858","Image Inpainting;PDE;Parallel Computing;GPU;CUDA;Theano;Tensorflow","Graphics processing units;Parallel processing;Mathematical model;Handheld computers;Convolution;Heating systems;Acceleration","computer graphics;graphics processing units;image restoration;interpolation;learning (artificial intelligence);parallel architectures;partial differential equations;Python","parallel computing accelerated image inpainting;GPU CUDA;Theano;Tensorflow;image restoration process;image interpolation;GPU parallel computing method;PDE-based image inpainting;parallel computing platform;programming model;computer CPU;handy platform;machine learning framework;Python;deep learning;C++","","3","","13","","15 Nov 2018","","","IEEE","IEEE Conferences"
"Efficient data classification by GPU-accelerated linear mean squared slack minimization","G. A. Papakostas; K. I. Diamantaras","EMaTTech, Dept. Computer and Informatics Engineering, Kavala 65404, Greece; TEI of Thessaloniki, Dept. Information Technology, Sindos 57400, Greece","2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)","20 Nov 2014","2014","","","1","6","An efficient parallel implementation of the recently proposed Slackmin classification algorithm that minimizes the mean squared slack variables energy is proposed in this paper. The efficacy of the resulted scheme is demonstrated both in terms of accuracy and computation speed. The parallelization of the Slackmin algorithm is achieved in the framework of GPU programming. Based on this framework the “cuLSlackmin” algorithm for linear problems was implemented, by using the CUDA C/C++ programming model and proposed herein. The introduced parallel algorithm is making use of the advantages imposed by the GPU architecture and achieves high classification rates in a short computation time. A set of experiments with some UCI datasets have shown the high performance of the cuLSlackmin algorithm compared to the Slackmin, LIBSVM and GPULIBSVM algorithms. The high performance of cuLSlackmin algorithm makes it appropriate for big data classification problems.","2378-928X","978-1-4799-3694-6","10.1109/MLSP.2014.6958885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958885","big data classification;machine learning;GPU programming;CUDA;slack minimization","Graphics processing units;Classification algorithms;Big data;Accuracy;Training;Machine learning algorithms;Instruction sets","Big Data;C++ language;graphics processing units;parallel algorithms;parallel architectures;pattern classification","GPU-accelerated linear mean squared slack minimization;parallel implementation;Slackmin classification algorithm;mean squared slack variables energy minimization;computation speed;Slackmin algorithm parallelization;GPU programming;cuLSlackmin algorithm;linear problem;CUDA C/C++ programming model;parallel algorithm;GPU architecture;classification rate;computation time;UCI dataset;GPULIBSVM algorithm;big data classification problem","","1","","22","","20 Nov 2014","","","IEEE","IEEE Conferences"
"GGAKE: GPU Based Genome Assembly Using K-Mer Extension","A. Garg; A. Jain; K. Paul","Dept. of Comput. Sci. & Eng., IIT Delhi, New Delhi, India; Dept. of Comput. Sci. & Eng., IIT Delhi, New Delhi, India; Dept. of Comput. Sci. & Eng., IIT Delhi, New Delhi, India","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","12 Jun 2014","2013","","","1105","1112","The genome assembly problem involves constructing the complete genome sequence from the reads generated by the sequencers. The Next Generation Sequencing (NGS) platforms produce a large number of short reads at a very low cost. Many assemblers have been developed to work with NGS reads. The assembly process is computation intensive and also requires a large amount of memory to store the reads. Numerous efforts are being made in recent times to parallelize the assembly process in order to reduce computation time. In this paper we present the design and development of a GPU based genome assembler (GGAKE). Our assembler works using the concept of k-mer extension. Prefix and suffix k-mers are spotted out of every read. Suffix k-mers are matched with prefix k-mers and extensions for every read are noted. Contigs are generated by extending the reads. We have implemented GGAKE on Nvidia's GPU using the CUDA programming model and benchmarked it on five bacterial genomes. Our results prove that at high coverage GGAKE is capable of producing good quality assembly in a small amount of time.","","978-0-7695-5088-6","10.1109/HPCC.and.EUC.2013.156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832038","parallel processing;bioinformatics;GPU;genome assembly","Graphics processing units;DNA;Bioinformatics;Assembly;Genomics;Encoding;Sorting","biology computing;cellular biophysics;genomics;graphics processing units;microorganisms;parallel architectures;parallel programming","GGAKE;GPU based genome assembly;genome sequence;next generation sequencing platforms;NGS reads;assembly process parallelization;GPU based genome assembler;k-mer extension;Nvidia's GPU;CUDA programming model;bacterial genomes;contig generation","","3","","21","","12 Jun 2014","","","IEEE","IEEE Conferences"
"Shared Memory and GPU Parallelization of an Operational Atmospheric Transport and Dispersion Application","F. Yu; P. Strazdins; J. Henrichs; T. Pugh",The Australian National University; The Australian National University; Bureau of Meteorology; Bureau of Meteorology,"2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","29 Jul 2019","2019","","","729","738","The HYSPLIT air concentration model is an operational Lagrangian trajectory and dispersion model that calculates the concentration or the distribution of pollutants by releasing and tracking particles or puffs. It is widely used for tracking and forecasting the release of pollutants such as radioactive material, dust storms and volcanic ash. Due to the massively parallel nature of the particle tracking system, the HYSPLIT model shows potential to be accelerated by multiple CPUs and GPUs. However, porting such a legacy application to a GPU requires non-trivial work to isolate the parallel code regions and correctly adapt the code to a parallel programming model. This paper presents methods to port the current HYSPLIT code to the shared-memory OpenMP and the CUDA programming models. In the OpenMP approach, the original HYSPLIT concentration model is analyzed, profiled and refactored to remove barriers for parallelizing on multi-core CPUs. A linear speed-up with some serial overhead is achieved for the OpenMP version. In the CUDA approach, we utilize the computing power of NVIDIA GTX960 and Tesla P100 GPUs, providing 4-5× faster overall performance than the original. With the help of GPU coarse-grained parallelism, a maximum 12.9× speedup has been measured, compared to the original program running on a CPU. The disturbance of different parallelization approaches to the original code base shows that most of the highly difficult work is in the refactoring, and that CUDA requires extensive, but mostly relatively shallow, changes.","","978-1-7281-3510-6","10.1109/IPDPSW.2019.00121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778420","Lagrangian trajectory and dispersion model, HYSPLIT, GPU acceleration, CUDA Fortran, OpenMP, MPI, CUDA coarse grained parallelism, High performance computing","Atmospheric modeling;Graphics processing units;Computational modeling;Dispersion;Mathematical model;Meteorology;Parallel processing","computer graphic equipment;graphics processing units;message passing;parallel algorithms;parallel architectures;parallel processing;parallel programming;shared memory systems","shared memory;GPU parallelization;operational atmospheric transport;dispersion application;HYSPLIT air concentration model;dispersion model;tracking particles;puffs;radioactive material;dust storms;volcanic ash;massively parallel nature;particle tracking system;HYSPLIT model;multiple CPUs;legacy application;nontrivial work;parallel code regions;parallel programming model;current HYSPLIT code;shared-memory OpenMP;CUDA programming models;OpenMP approach;original HYSPLIT concentration model;OpenMP version;CUDA approach;GPU coarse-grained parallelism;original program;multicore CPU;parallelization approaches;Tesla P100 GPU;original code base;NVIDIA GTX960","","","","14","","29 Jul 2019","","","IEEE","IEEE Conferences"
"Architecting Waferscale Processors - A GPU Case Study","S. Pal; D. Petrisko; M. Tomei; P. Gupta; S. S. Iyer; R. Kumar","Dept. of Electr. & Comput. Eng., Univ. of California, Los Angeles, Los Angeles, CA, USA; Dept. of Electr. & Comput. Eng., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Electr. & Comput. Eng., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Electr. & Comput. Eng., Univ. of California, Los Angeles, Los Angeles, CA, USA; Dept. of Electr. & Comput. Eng., Univ. of California, Los Angeles, Los Angeles, CA, USA; Dept. of Electr. & Comput. Eng., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)","28 Mar 2019","2019","","","250","263","Increasing communication overheads are already threatening computer system scaling. One approach to dramatically reduce communication overheads is waferscale processing. However, waferscale processors [1], [2], [3] have been historically deemed impractical due to yield issues [1], [4] inherent to conventional integration technology. Emerging integration technologies such as Silicon-Interconnection Fabric (Si-IF) [5], [6], [7], where pre-manufactured dies are directly bonded on to a silicon wafer, may enable one to build a waferscale system without the corresponding yield issues. As such, waferscalar architectures need to be revisited. In this paper, we study if it is feasible and useful to build today's architectures at waferscale. Using a waferscale GPU as a case study, we show that while a 300 mm wafer can house about 100 GPU modules (GPM), only a much scaled down GPU architecture with about 40 GPMs can be built when physical concerns are considered. We also study the performance and energy implications of waferscale architectures. We show that waferscale GPUs can provide significant performance and energy efficiency advantages (up to 18.9x speedup and 143x EDP benefit compared against equivalent MCM-GPU based implementation on PCB) without any change in the programming model. We also develop thread scheduling and data placement policies for waferscale GPU architectures. Our policies outperform state-of-art scheduling and data placement policies by up to 2.88x (average 1.4x) and 1.62x (average 1.11x) for 24 GPM and 40 GPM cases respectively. Finally, we build the first Si-IF prototype with interconnected dies. We observe 100% of the inter-die interconnects to be successfully connected in our prototype. Coupled with the high yield reported previously for bonding of dies on Si-IF, this demonstrates the technological readiness for building a waferscale GPU architecture.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675211","Waferscale Processors;GPU;Silicon Interconnect Fabric","Graphics processing units;Silicon;Integrated circuit interconnections;Copper;Substrates;Computer architecture","cache storage;coprocessors;graphics processing units;integrated circuit interconnections;multiprocessing systems;parallel algorithms;parallel processing;silicon","GPU modules;GPM cases;silicon-interconnection fabric;Si-IF;waferscale processing;threatening computer system scaling;communication overheads;GPU case study;waferscale processors;state-of-art scheduling;waferscale GPU architecture;data placement policies;thread scheduling;equivalent MCM-GPU;energy efficiency advantages;waferscale architectures;waferscalar architectures;corresponding yield issues;waferscale system;silicon wafer;pre-manufactured dies;integration technologies;conventional integration technology;Si","","14","","82","","28 Mar 2019","","","IEEE","IEEE Conferences"
"BigKernel -- High Performance CPU-GPU Communication Pipelining for Big Data-Style Applications","R. Mokhtari; M. Stumm","Dept. of Electr. & Comput. Eng., Univ. of Toronto, Toronto, ON, Canada; Dept. of Electr. & Comput. Eng., Univ. of Toronto, Toronto, ON, Canada","2014 IEEE 28th International Parallel and Distributed Processing Symposium","14 Aug 2014","2014","","","819","828","GPUs offer an order of magnitude higher compute power and memory bandwidth than CPUs. GPUs therefore might appear to be well suited to accelerate computations that operate on voluminous data sets in independent ways, e.g., for transformations, filtering, aggregation, partitioning or other ""Big Data"" style processing. Yet experience indicates that it is difficult, and often error-prone, to write GPGPU programs which efficiently process data that does not fit in GPU memory, partly because of the intricacies of GPU hardware architecture and programming models, and partly because of the limited bandwidth available between GPUs and CPUs. In this paper, we propose Big Kernel, a scheme that provides pseudo-virtual memory to GPU applications and is implemented using a 4-stage pipeline with automated prefetching to (i) optimize CPU-GPU communication and (ii) optimize GPU memory accesses. Big Kernel simplifies the programming model by allowing programmers to write kernels using arbitrarily large data structures that can be partitioned into segments where each segment is operated on independently, these kernels are transformed into Big Kernel using straight-forward compiler transformations. Our evaluation on six data-intensive benchmarks shows that Big Kernel achieves an average speedup of 1.7 over state-of-the-art double-buffering techniques and an average speedup of 3.0 over corresponding multi-threaded CPU implementations.","1530-2075","978-1-4799-3800-1","10.1109/IPDPS.2014.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877313","GPU;CPU;communication;optimization;stream processing","Graphics processing units;Kernel;Prefetching;Arrays;Memory management;Pipelines","Big Data;data structures;graphics processing units;pipeline processing;program compilers;storage management","BigKernel scheme;high performance CPU-GPU communication pipelining;Big Data-style processing;memory bandwidth;magnitude higher compute power;voluminous data sets;GPGPU programs;GPU hardware architecture;GPU programming models;pseudovirtual memory;automated prefetching;GPU memory accesses;data structures;straight-forward compiler transformations;double-buffering techniques;multithreaded CPU","","11","2","20","","14 Aug 2014","","","IEEE","IEEE Conferences"
"Profiling Kernels Behavior to Improve CPU / GPU Interactions","R. Salgado","Pleiad Lab., Univ. of Chile, Santiago, Chile","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","17 Aug 2015","2015","2","","754","756","Most modern computer and mobile devices have a graphical processing unit (GPU) available for any general purpose computation. GPU supports a programming model that is radically different from traditional sequential programming. As such, programming GPU is known to be hard and error prone, despite the large number of available APIs and dedicated programming languages. In this paper we describe a profiling technique that reports on the interaction between a CPU and GPUs. The resulting execution profile may then reveal anomalies and suboptimal situations, in particular due to an improper memory configuration. Our profiler has been effective at identifying suboptimal memory allocation usage in one image processing application.","1558-1225","978-1-4799-1934-5","10.1109/ICSE.2015.239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203060","gpgpu;opencl;memory profiling;profiling","Graphics processing units;Kernel;Programming;Visualization;Central Processing Unit;Computers;Measurement","graphics processing units;storage management","kernel behavior profiling technique;CPU-GPU interactions;mobile devices;graphical processing unit;programming model;improper memory configuration;suboptimal memory allocation usage;image processing application","","","","9","","17 Aug 2015","","","IEEE","IEEE Conferences"
"Performance improvement of CUDA applications by reducing CPU-GPU data transfer overhead","N. V. Sunitha; K. Raju; N. N. Chiplunkar","Department of CSE, NMAMIT, Nitte; Department of CSE, NMAMIT, Nitte; Department of CSE, NMAMIT, Nitte","2017 International Conference on Inventive Communication and Computational Technologies (ICICCT)","17 Jul 2017","2017","","","211","215","In a CPU-GPU based heterogeneous computing system, the input data to be processed by the kernel resides in the host memory. The host and the device memory address spaces are different. Therefore, the device can not directly access the host memory. In CUDA programming model, the data is moved between the host memory and the device memory. This data transfer is a time consuming task. The communication overhead can be hidden by overlapping the data transfer and the kernel execution. CUDA streams provide a means for overlapping data transfer and the kernel execution. In this paper we explore the effects of overlapping data transfer and the kernel execution on overall execution time of some CUDA applications. The results show that the usage of the different levels of concurrency supported by the streams enhances the performance of the CUDA applications.","","978-1-5090-5297-4","10.1109/ICICCT.2017.7975190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975190","Heterogeneous system;CUDA;Kernel;Stream","Graphics processing units;Kernel;Data transfer;Concurrent computing;Engines;Performance evaluation;Central Processing Unit","concurrency (computers);electronic data interchange;graphics processing units;parallel architectures;parallel programming","performance improvement;CUDA applications;CPU-GPU data transfer overhead reduction;CPU-GPU based heterogeneous computing system;host memory;CUDA programming model;device memory;communication overhead;kernel execution;CUDA streams;overlapping data transfer;overall execution time;concurrency level","","4","","12","","17 Jul 2017","","","IEEE","IEEE Conferences"
"Selective GPU caches to eliminate CPU-GPU HW cache coherence","N. Agarwal; D. Nellans; E. Ebrahimi; T. F. Wenisch; J. Danskin; S. W. Keckler",University of Michigan; NVIDIA; NVIDIA; University of Michigan; NVIDIA; NVIDIA,"2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)","4 Apr 2016","2016","","","494","506","Cache coherence is ubiquitous in shared memory multiprocessors because it provides a simple, high performance memory abstraction to programmers. Recent work suggests extending hardware cache coherence between CPUs and GPUs to help support programming models with tightly coordinated sharing between CPU and GPU threads. However, implementing hardware cache coherence is particularly challenging in systems with discrete CPUs and GPUs that may not be produced by a single vendor. Instead, we propose, selective caching, wherein we disallow GPU caching of any memory that would require coherence updates to propagate between the CPU and GPU, thereby decoupling the GPU from vendor-specific CPU coherence protocols. We propose several architectural improvements to offset the performance penalty of selective caching: aggressive request coalescing, CPU-side coherent caching for GPU-uncacheable requests, and a CPU-GPU interconnect optimization to support variable-size transfers. Moreover, current GPU workloads access many read-only memory pages; we exploit this property to allow promiscuous GPU caching of these pages, relying on page-level protection, rather than hardware cache coherence, to ensure correctness. These optimizations bring a selective caching GPU implementation to within 93% of a hardware cache-coherent implementation without the need to integrate CPUs and GPUs under a single hardware coherence protocol.","2378-203X","978-1-4673-9211-2","10.1109/HPCA.2016.7446089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446089","","Graphics processing units;Coherence;Hardware;Memory management;Bandwidth;Protocols;System-on-chip","cache storage;graphics processing units;microprocessor chips;protocols;shared memory systems","HW cache coherence;hardware cache coherence;shared memory multiprocessor;programming model;aggressive request coalescing;CPU-side coherent caching;CPU-GPU interconnect optimization;hardware coherence protocol","","38","7","70","","4 Apr 2016","","","IEEE","IEEE Conferences"
"The Unicorn Runtime: Efficient Distributed Shared Memory Programming for Hybrid CPU-GPU Clusters","T. Beri; S. Bansal; S. Kumar","Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, India; Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, India; Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, India","IEEE Transactions on Parallel and Distributed Systems","12 Apr 2017","2017","28","5","1518","1534","Programming hybrid CPU-GPU clusters is hard. This paper addresses this difficulty and presents the design and runtime implementation of Unicorn-a parallel programming model for hybrid CPU-GPU clusters. In particular, this paper proves that efficient distributed shared memory style programing is possible and its simplicity can be retained across CPUs and GPUs in a cluster, minus the frustration of dealing with race conditions. Further, this can be done with a unified abstraction, avoiding much of the complication of dealing with hybrid architectures. This is achieved with the help of transactional semantics (on shared global address spaces), deferred bulk data synchronization, workload pipelining and various communication and computation scheduling optimizations. We describe the said abstraction, our computation and communication scheduling system and report its performance on a few benchmarks like Matrix Multiplication, LU Decomposition and 2D FFT. We find that parallelization of coarse-grained applications like matrix multiplication or 2D FFT using our system requires only about 30 lines of C code to set up the runtime. The rest of the application code is regular single CPU/GPU implementation. This indicates the ease of extending parallel code to a distributed environment. The execution is efficient as well. When multiplying two square matrices of size 65, 536 χ 65,536, Unicornachieves a peak performance of 7.88 TFlop/s when run over a cluster of 14 nodes with each node equipped with two Tesla M2070 GPUs and two 6-core Intel Xeon 2.67 GHz CPUs, connected over a 32Gbps Infiniband network. In this paper, we also demonstrate that the Unicorn programming model can be efficiently used to implement high level abstractions like MapReduce. We use such an extension to implement PageRank and report its performance. For a sample web of 500 million web pages, our implementation completes a page rank iteration in about 18 seconds (on average) on a 14-node cluster.","1558-2183","","10.1109/TPDS.2016.2616314","Ministry of Earth Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588161","Unicorn runtime;distributed system design;scheduling;load balancing;accelerators;bulk synchronous parallelism","Runtime;Programming;Optimization;Processor scheduling;Kernel;Synchronization;Graphics processing units","distributed shared memory systems;fast Fourier transforms;graphics processing units;information retrieval;Internet;mathematics computing;matrix multiplication;parallel programming;pipeline processing;processor scheduling;programming language semantics;transaction processing","Unicorn runtime;distributed shared memory programming;hybrid CPU-GPU clusters;parallel programming model;unified abstraction;transactional semantics;bulk data synchronization;workload pipelining;computation scheduling optimizations;communication scheduling optimizations;matrix multiplication;LU decomposition;2D FFT;coarse-grained applications;C code;application code;parallel code;Tesla M2070 GPU;6-core Intel Xeon CPU;Infiniband network;Unicorn programming model;PageRank implementation;page rank iteration;Web pages","","6","","40","IEEE","11 Oct 2016","","","IEEE","IEEE Journals"
"PACXX: Towards a Unified Programming Model for Programming Accelerators Using C++14","M. Haidl; S. Gorlatch","Univ. of Muenster, Muenster, Germany; Univ. of Muenster, Muenster, Germany","2014 LLVM Compiler Infrastructure in HPC","30 Mar 2015","2014","","","1","11","We present PACXX -- a unified programming model for programming many-core systems that comprise accelerators like Graphics Processing Units (GPUs). One of the main difficulties of the current GPU programming is that two distinct programming models are required: the host code for the CPU is written in C/C++ with the restricted, C-like API for memory management, while the device code for the GPU has to be written using a device-dependent, explicitly parallel programming model, e.g., OpenCL or CUDA. This leads to long, poorly structured and error-prone codes. In PACXX, both host and device programs are written in the same programming language -- the newest C++14 standard, with all modern features including type inference (auto), variadic templates, generic lambda expressions, as well as STL containers and algorithms. We implement PACXX by a custom compiler (based on the Clang front-end and LLVM IR) and a runtime system, that together perform major tasks of memory management and data synchronization automatically and transparently for the programmer. We evaluate our approach by comparing it to CUDA and OpenCL regarding program size and target performance.","","978-1-4799-7023-0","10.1109/LLVM-HPC.2014.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069296","","Kernel;Graphics processing units;Programming;Vectors;Synchronization;Runtime;Standards","application program interfaces;C++ language;graphics processing units;multiprocessing systems;parallel architectures;parallel programming;program compilers;reasoning about programs;storage management;type theory","PACXX;unified programming model;programming accelerator;many-core system;graphics processing units;GPU programming;C/C++;C-like API;memory management;device code;device-dependent programming model;parallel programming model;OpenCL;CUDA;error-prone code;host program;device program;programming language;C++14 standard;type inference;variadic template;generic lambda expression;STL container;custom compiler;Clang front-end;LLVM IR;runtime system;data synchronization","","19","","15","","30 Mar 2015","","","IEEE","IEEE Conferences"
"An Efficient Parallelization Strategy for Dynamic Programming on GPU","K. Berger; F. Galea","LIST Embedded Real Time Syst. Lab., CEA, Gif-sur-Yvette, France; LIST Embedded Real Time Syst. Lab., CEA, Gif-sur-Yvette, France","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum","31 Oct 2013","2013","","","1797","1806","Optimization methods generally do not fall into the most suitable algorithms for parallelization on a GPU. However, a relatively good efficiency still can be obtained if the method is properly adapted to the GPU programming model, which is the case for dynamic programming. In this article, we propose a parallelization strategy for thread grouping for dynamic programming in CUDA. We show that parametrizing the solver parallelism according to the hardware allows better performance. The strategy provides good acceleration compared to a standard GPU parallel strategy on a dynamic programming-based implementation of the knapsack problem. We show this strategy is helpful in the case of the multi-dimensional knapsack problem, where computing multi-dimensional indices is a costly operation.","","978-0-7695-4979-8","10.1109/IPDPSW.2013.208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651080","dynamic programming;parallelism;knapsack problem;GPU computing;combinatorial optimization;CUDA","Graphics processing units;Instruction sets;Vectors;Dynamic programming;Kernel;Parallel processing;Arrays","dynamic programming;graphics processing units;knapsack problems;parallel architectures","multidimensional indices;multidimensional knapsack problem;dynamic programming based implementation;standard GPU parallel strategy;hardware;solver parallelism;CUDA;GPU programming model;optimization methods;parallelization strategy","","9","","34","","31 Oct 2013","","","IEEE","IEEE Conferences"
"CUDA parallel programming model","M. Garland","NVIDIA Research, USA","2008 IEEE Hot Chips 20 Symposium (HCS)","4 Jul 2016","2008","","","1","29","Presents a collection of slides covering the following topics: parallel threads; parallel algorithms; heterogeneous systems; CPU; GPU; concurrent threads; shared memory model; vector addition kernel; block synchronization; thread block; per-block shared memory; parallel reduction; serial SAXPY routine; and parallel SAXPY routine.","","978-1-4673-8871-9","10.1109/HOTCHIPS.2008.7476519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476519","","Instruction sets;Graphics processing units;Tutorials;Parallel programming;Kernel;Parallel algorithms;Synchronization","concurrency control;graphics processing units;multi-threading;parallel algorithms;parallel architectures;shared memory systems","parallel threads;parallel algorithms;CUDA parallel programming model;heterogeneous systems;CPU;GPU;concurrent threads;shared memory model;vector addition kernel;block synchronization;thread block;per-block shared memory;parallel reduction;serial SAXPY routine;parallel SAXPY routine","","1","","","","4 Jul 2016","","","IEEE","IEEE Conferences"
"POSTER - collective dynamic parallelism for directive based GPU programming languages and compilers","G. Ozen; E. Ayguade; J. Labarta","Barcelona Supercomputing Center, Universitat Politecnica de Catalunya, Spain; Barcelona Supercomputing Center, Universitat Politecnica de Catalunya, Spain; Barcelona Supercomputing Center, Universitat Politecnica de Catalunya, Spain","2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)","1 Dec 2016","2016","","","423","424","Early programs for GPU (Graphics Processing Units) acceleration were based on a flat, bulk parallel programming model, in which programs had to perform a sequence of kernel launches from the host CPU. In the latest releases of these devices, dynamic (or nested) parallelism is supported, making possible to launch kernels from threads running on the device, without host intervention. Unfortunately, the overhead of launching kernels from the device is higher compared to launching from the host CPU, making the exploitation of dynamic parallelism unprofitable. This paper proposes and evaluates the basic idea behind a user-directed code transformation technique, named collective dynamic parallelism, that targets the effective exploitation of nested parallelism in modern GPUs. The technique dynamically packs dynamic parallelism kernel invocations and postpones their execution until a bunch of them are available. We show that for sparse matrix vector multiplication, CollectiveDP outperforms well optimized libraries, making GPU useful when matrices are highly irregular.","","978-1-4503-4121-9","10.1145/2967938.2974056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7756778","","Kernel;Parallel processing;Graphics processing units;Context;Sparse matrices;Parallel programming;Libraries","graphics processing units;matrix multiplication;microprocessor chips;parallel programming;program compilers;programming languages;vectors","programming languages;program compilers;graphics processing units;GPU;parallel programming model;host CPU;user-directed code transformation;collective dynamic parallelism;sparse matrix vector multiplication","","","","3","","1 Dec 2016","","","IEEE","IEEE Conferences"
"GPU accelerated three dimensional unstructured geometric multigrid solver","J. Sebastian; N. Sivadasan; R. Banerjee","Dept. of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India; Dept. of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India; Dept. of Mechanical Engineering, Indian Institute of Technology Hyderabad, India","2014 International Conference on High Performance Computing & Simulation (HPCS)","22 Sep 2014","2014","","","9","16","Graphics processor units (GPUs) have started becoming an integral part of high performance computing. We develop a GPU based 3D-unstructured geometric multigrid solver, which is extensively used in Computational Fluid Dynamics (CFD) applications. Parallelization for GPUs is not straightforward because of the irregularity of the mesh. Using combination of graph coloring and greedy maximal independent set computations, we obtain significant performance improvements in the multigrid solver and its parallelization. We use NVIDIAs CUDA programming model for the implementation. In our experiments, we solve heat conduction problems on unstructured 3D meshes. Different schemes for implementing the multigrid algorithm are evaluated. For a mesh of size 1.6 million, our multigrid GPU implementation gives 24 times speed up compared to multigrid serial implementation and 1630 times speed up compared to non-multigrid serial implementation.","","978-1-4799-5313-4","10.1109/HPCSim.2014.6903663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903663","GPU Computing;High Performance Computing;Computational Fluid Dynamics;Multigrid Flow Solver","Graphics processing units;Instruction sets;Image color analysis;Iterative methods;Smoothing methods;Computational fluid dynamics;Jacobian matrices","computational fluid dynamics;differential equations;graph colouring;graphics processing units;greedy algorithms;heat conduction;mechanical engineering computing;mesh generation;parallel architectures","GPU accelerated three dimensional unstructured geometric multigrid solver;graphics processor units;high performance computing;GPU based 3D-unstructured geometric multigrid solver;computational fluid dynamics;CFD;mesh irregularity;graph coloring;greedy maximal independent set computations;NVIDIA CUDA programming model;heat conduction problems;unstructured 3D mesh;multigrid serial implementation;nonmultigrid serial implementation","","3","","29","","22 Sep 2014","","","IEEE","IEEE Conferences"
"Accelerating of color moments and texture features extraction using GPU based parallel computing","H. Heidari; A. Chalechale; A. A. Mohammadabadi","Department of Computer Engineering Razi University Kermanshah, Iran; Department of Computer Engineering Razi University Kermanshah, Iran; Department of Computer Engineering Razi University Kermanshah, Iran","2013 8th Iranian Conference on Machine Vision and Image Processing (MVIP)","31 Mar 2014","2013","","","430","435","Image retrieval tools can assist people in making efficient use of digital image collections; also it has become imperative to find efficient methods for the retrieval of these images. Most image processing algorithms are inherently parallel, so multithreading processors are suitable in such applications. In very big image databases, image processing takes very long time for run on a single core processor because of single thread execution of algorithms. GPU is more common in most image processing applications due to multithread execution of algorithms, programmability and low cost. In this paper we implement color moments and texture based image retrieval (entropy, standard deviation and local range) in parallel using CUDA programming model to run on GPUs. These features are applied to search images from a database which are similar to a query image. We evaluated our retrieval system using recall, precision, and average precision measures. Experimental results showed that parallel implementation led to an average speed up of 144.67×over the serial implementation when running on a NVIDIA GPU GeForce GT610M. Also the average precision and the average recall of proposed method are 61.968% and 55% respectively.","2166-6784","978-1-4673-6184-2","10.1109/IranianMVIP.2013.6780024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6780024","color moment;CUDA;GPU;texture based image retrieval","Feature extraction;Graphics processing units;Image color analysis;Image retrieval;Shape;Instruction sets","feature extraction;graphics processing units;image colour analysis;image retrieval;image texture;multi-threading;parallel architectures","color moments;texture features extraction;GPU based parallel computing;graphics processing unit;image retrieval tools;digital image collections;image processing algorithms;multithreading processors;very big image databases;CUDA programming model;compute unified device architecture;recall measure;precision measure;average precision measure;NVIDIA GPU GeForce GT610M","","2","","23","","31 Mar 2014","","","IEEE","IEEE Conferences"
"Parallel Monte Carlo Integration Algorithm Based on GPU","H. Zong; R. Hua; J. Zhao; Z. Cao","Huaiyin Institute of Technology,Faculty of Computer and Software Engineering,Huaian,China; Huaiyin Institute of Technology,Faculty of Computer and Software Engineering,Huaian,China; Huaiyin Institute of Technology,Faculty of Computer and Software Engineering,Huaian,China; Nanjing University of Aeronautics and Astronautics,College of Computer Science and Technology,Nanjing,China","2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","18 Aug 2020","2019","","","790","794","Big data comes with the increasing maturity of computer network technology and its application popularity. How to process big data and mine its potential value is a hot issue today. Artificial intelligence is one of the main sources of algorithms for dealing with big data, while parallel computation is the only choice to improve computing performance. Monte Carlo method plays an important role in the application of artificial intelligence, such as computer game AlphaGo, and it requires a lot of random data to calculate, so we use it, in this paper, to explore the general method of parallel processing big data. According to Monte Carlo method, the calculation time of the algorithm will increase exponentially with the increase of the calculation accuracy. As the calculation accuracy will increase accordingly when the number of points increases. In order to improve the computing performance, a Monte Carlo parallel algorithm based on GPU-CUDA programming model is designed. The quality of the random number generator is the key factor affecting the accuracy of Monte Carlo calculation, so the higher quality Mersenne Twister random number algorithm is chosen. Taking π and e as examples, the experimental results show that the GPU-based Monte Carlo parallel algorithm is effective. This method improves both the calculation performance and the calculation accuracy.","","978-1-7281-2348-6","10.1109/ISKE47853.2019.9170329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9170329","GPU;Monte Carlo method;definite integral;parallel computing;random number","","graphics processing units;Monte Carlo methods;parallel algorithms;parallel architectures;parallel processing;random number generation","Monte Carlo calculation;higher quality Mersenne Twister random number algorithm;GPU-based Monte Carlo parallel algorithm;calculation performance;parallel Monte Carlo integration algorithm;increasing maturity;computer network technology;application popularity;artificial intelligence;parallel computation;computing performance;Monte Carlo method;computer game AlphaGo;random data;parallel processing big data;calculation time;points increases;GPU-CUDA programming model;random number generator","","","","7","","18 Aug 2020","","","IEEE","IEEE Conferences"
"Accelerating gene regulatory networks inference through GPU/CUDA programming","F. F. Borelli; R. Y. Camargo; D. C. Martins; B. Stransky; L. C. S. Rozante","Universidade Federal do ABC, Santo André-SP, Brazil; Universidade Federal do ABC, Santo André-SP, Brazil; Universidade Federal do ABC, Santo André-SP, Brazil; Universidade Federal do ABC, Santo André-SP, Brazil; Universidade Federal do ABC, Santo André-SP, Brazil","2012 IEEE 2nd International Conference on Computational Advances in Bio and medical Sciences (ICCABS)","12 Apr 2012","2012","","","1","6","Gene regulatory networks (GRN) inference is an important bioinformatics problem in which the gene interactions need to be deduced from gene expression data, such as microarray data. Feature selection methods can be applied to this problem. A feature selection technique is composed by two parts: a search algorithm and a criterion function. Among the search algorithms already proposed, there is the exhaustive search where the best feature subset is returned, although its computational complexity is unfeasible in almost all situations. The objective of this work is the development of a low cost parallel solution based on GPU architectures for exhaustive search with a viable cost-benefit. CUDA™ is a general purpose parallel architecture with a new parallel programming model allowing that the NVIDIA<sup>®</sup> GPUs solve complex problems in an efficient way. We developed a parallel algorithm for GRN inference based on the GPU/CUDA and encouraging speedups (60x) were achieved when assuming that each target gene has two predictors. The idea behind the proposed method can be applied considering three or more predictors for each target gene as well.","","978-1-4673-1321-6","10.1109/ICCABS.2012.6182628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182628","GRNs Inference;exhaustive search;mean conditional entropy;GPU Computing;CUDA","Graphics processing unit;Computer architecture;Entropy;Instruction sets;Random access memory;Prediction algorithms;Performance evaluation","bioinformatics;computational complexity;feature extraction;genetics;graphics processing units;inference mechanisms;parallel architectures;parallel programming","gene regulatory network inference;GPU-CUDA programming;bioinformatics;gene interactions;feature selection method;gene expression data;search algorithm;feature subset;computational complexity;GPU architectures;parallel architecture;parallel programming model;parallel algorithm;GRN inference;exhaustive search","","3","","17","","12 Apr 2012","","","IEEE","IEEE Conferences"
"Comparison of GPU and FPGA based hardware platforms for ultrasonic flaw detection using support vector machines","K. Virupakshappa; E. Oruklu; Y. Jiang; Y. Yuan","ECE Department, Illinois Institute of Technology, Chicago, IL, USA; ECE Department, Illinois Institute of Technology, Chicago, IL, USA; ECE Department, Illinois Institute of Technology, Chicago, IL, USA; ECE Department, Illinois Institute of Technology, Chicago, IL, USA","2017 IEEE International Ultrasonics Symposium (IUS)","2 Nov 2017","2017","","","1","1","Our earlier work on support vector machines (SVM) and ultrasonic flaw detection algorithms demonstrated i) highly accurate classifier performance and ii) the feasibility of the algorithm for real-time implementation on low-cost embedded systems with graphical processing units (GPU) and CUDA library (a parallel computing platform and programming model) support. This works extends the implementation to another programmable hardware platform, FPGA, and also evaluates the performance of a different numerical computation library, TensorFlow by Google.","1948-5727","978-1-5386-3383-0","10.1109/ULTSYM.2017.8091862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8091862","","Graphics processing units;Support vector machines;Field programmable gate arrays;Libraries;Classification algorithms;Hardware;Acoustics","embedded systems;field programmable gate arrays;flaw detection;graphics processing units;parallel architectures;pattern classification;support vector machines","parallel computing;embedded systems;programmable hardware platform;programming model;ultrasonic flaw detection algorithms;support vector machines;FPGA;GPU","","","","","","2 Nov 2017","","","IEEE","IEEE Conferences"
"GPU-based Real-time Decoding Technique for High-definition Videos","H. Deng; C. Deng; J. Li","Sch. of Comput. Sci. & Eng., South China Univ. of Technol., Guangzhou, China; Dept. of Comput. Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Software Eng., South China Univ. of Technol., Guangzhou, China","2012 Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","23 Aug 2012","2012","","","186","190","In this paper, we first discussed the video decoding standard and its architecture, and then analyzed the decoding complexity of each process. By using the benefit of the CUDA programming model, and taking advantages of GPU to optimize the decoding process of MC (motion compensation) and CSC(color space conversion) that are very time consuming, we proposed a MC accelerating method based on CUDA, and a CSC accelerating method based on CUDA and OpenGL shader. The experiments show that it is feasible to decode high definition video in real time using GPGPU-CUDA.","","978-1-4673-1741-2","10.1109/IIH-MSP.2012.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6274644","CUDA;real-time decoding;high-definition video;GPGPU;GLSL","Graphics processing unit;Decoding;Videos;Instruction sets;Kernel;Central Processing Unit;Real time systems","graphics processing units;image colour analysis;parallel architectures;video coding","GPU based real-time decoding technique;high definition videos;video decoding;CUDA programming model;motion compensation;MC;color space conversion;CSC;OpenGL shader","","1","","8","","23 Aug 2012","","","IEEE","IEEE Conferences"
"Coordinate strip-mining and kernel fusion to lower power consumption on GPU","G. Wang","National Laboratory for Parallel and Distributed Processing, School of Computer, National University of Defense Technology, Changsha, Hunan, China","2011 Design, Automation & Test in Europe","5 May 2011","2011","","","1","4","Although general purpose GPUs have relatively high computing capacity, they also introduce high power consumption compared with general purpose CPUs. Therefore low-power techniques targeted for GPUs will be one of the most hot topics in the future. On the other hand, in several application domains, users are unwilling to sacrifice performance to save power. In this paper, we propose an effective kernel fusion method to reduce the power consumption for GPUs without performance loss. Different from executing multiple kernels serially, the proposed method fuses several kernels into one larger kernel. Owing to the fact that most consecutive kernels in an application have data dependency and could not be fused directly, we split large kernel into multiple slices with strip-mining method, then fuse independent sliced kernels into one kernel. Based on the CUDA programming model, we propose three different kernel fusion implementations, with each one targeting for a special case. Based on the different strip-ming methods, we also propose two fusion mechanisms, which are called invariant-slice fusion and variant-slice fusion. The latter one could be better adapted to the requirements of the kernels to be fused. The experimental results validate that the proposed kernel fusion method could effectively reduce the power consumption for GPU.","1558-1101","978-3-9810801-8-6","10.1109/DATE.2011.5763317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763317","GPGPV;Kernel Fusion;Strip-mining;Power Efficiency","Kernel;Energy consumption;Graphics processing unit;Power demand;Instruction sets;Programming;Optimization","computer graphic equipment;coprocessors;general purpose computers;low-power electronics;parallel architectures;program slicing","coordinate strip-mining;kernel fusion;power consumption;general purpose GPU;low-power techniques;data dependency;CUDA programming model;invariant-slice fusion;graphics processing unit","","1","","10","","5 May 2011","","","IEEE","IEEE Conferences"
"A Case Study on the HACCmk Routine in SYCL on Integrated Graphics","Z. Jin; V. Morozov; H. Finkel","Argonne National Laboratory,Leadership Computing Facility,9700 S Cass Ave, Lemont,IL,60439; Argonne National Laboratory,Leadership Computing Facility,9700 S Cass Ave, Lemont,IL,60439; Argonne National Laboratory,Leadership Computing Facility,9700 S Cass Ave, Lemont,IL,60439","2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","28 Jul 2020","2020","","","368","374","As opposed to the Open Computing Language (OpenCL) programming model in which host and device codes are generally written in different languages, the SYCL programming model can combine host and device codes for an application in a type-safe way to improve development productivity. In this paper, we chose the HACCmk routine, a representative compute-bound kernel, as a case study on the performance of the SYCL programming model targeting a heterogeneous computing device. More specifically, we introduced the SYCL programming model, presented the OpenCL and SYCL implementations of the routine, and compared the performance of the two implementations using the offline and online compilation on Intel® IrisTM Pro integrated GPUs. We found that the overhead of online compilation may become significant compared to the execution time of a kernel. Compared to the performance of OpenCL implementations, the SYCL implementation can maintain the performance using the offline compilation. The number of execution units in a GPU are critical to improving the raw performance of a compute-bound kernel.","","978-1-7281-7445-7","10.1109/IPDPSW50202.2020.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150310","Programming model;Compilation modes;Compute-bound kernel;GPU","Kernel;Programming;Graphics processing units;Computational modeling;Performance evaluation;C++ languages","application program interfaces;graphics processing units;parallel processing;program compilers","SYCL programming model;online compilation;OpenCL implementation;SYCL implementation;HACCmk routine;Open Computing Language programming model;representative compute-bound kernel;heterogeneous computing device;Intel IrisTM Pro;GPU","","1","","26","","28 Jul 2020","","","IEEE","IEEE Conferences"
"Exploring the Suitability of Remote GPGPU Virtualization for the OpenACC Programming Model Using rCUDA","A. Castelló; A. J. Peña; R. Mayo; P. Balaji; E. S. Quintana-Ortí","Univ. Jaume I de Castello, Castello de la Plana, Spain; Argonne Nat. Lab., Argonne, IL, USA; Univ. Jaume I de Castello, Castello de la Plana, Spain; Argonne Nat. Lab., Argonne, IL, USA; Univ. Jaume I de Castello, Castello de la Plana, Spain","2015 IEEE International Conference on Cluster Computing","29 Oct 2015","2015","","","92","95","OpenACC is an application programming interface (API) that aims to unleash the power of heterogeneous systems composed of CPUs and accelerators such as graphic processing units (GPUs) or Intel Xeon Phi coprocessors. This directive-based programming model is intended to enable developers to accelerate their application's execution with much less effort. Coprocessors offer significant computing power but in many cases these devices remain largely underused because not all parts of applications match the accelerator architecture. Remote accelerator virtualization frameworks introduce a means to address this problem. In particular, the remote CUDA virtualization middleware rCUDA provides transparent remote access to any GPU installed in a cluster. Combining these two technologies, OpenACC and rCUDA, in a single scenario is naturally appealing. In this work we explore how the different OpenACC directives behave on top of a remote GPGPU virtualization technology in two different hardware configurations. Our experimental evaluation reveals favorable performance results when the two technologies are combined, showing low overhead and similar scaling factors when executing OpenACC-enabled directives.","2168-9253","978-1-4673-6598-7","10.1109/CLUSTER.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307570","GPUs;OpenACC;remote virtualization;rCUDA","Graphics processing units;Virtualization;Acceleration;Programming;Coprocessors;Kernel","graphics processing units;middleware;parallel architectures;virtualisation","GPGPU virtualization;graphics processing unit;OpenACC programming model;rCUDA;CUDA virtualization middleware;application programming interface;API;directive-based programming model;remote accelerator virtualization framework","","3","","12","","29 Oct 2015","","","IEEE","IEEE Conferences"
"A Performance Prediction Model for Memory-Intensive GPU Kernels","Z. Hu; G. Liu; Z. Hu","Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; NA","2014 IEEE Symposium on Computer Applications and Communications","2 Oct 2014","2014","","","14","18","Commodity graphic processing units (GPUs) have rapidly evolved to become high performance accelerators for data-parallel computing through a large array of processing cores and the CUDA programming model with a C-like interface. However, optimizing an application for maximum performance based on the GPU architecture is not a trivial task for the tremendous change from conventional multi-core to the many-core architectures. Besides, the GPU vendors do not disclose much detail about the characteristics of the GPU's architecture. To provide insights into the performance of memory-intensive kernels, we propose a pipelined global memory model to incorporate the most critical global memory performance related factor, uncoalesced memory access pattern, and provide a basis for predicting performance of memory-intensive kernels. As we will demonstrate, the pipeline throughput is dynamic and sensitive to the memory access patterns. We validated our model on the NVIDIA GPUs using CUDA (Compute Unified Device Architecture). The experiment results show that the pipeline captures performance factors related to global memory and is able to estimate the performance for memory-intensive GPU kernels via the proposed model.","","978-0-7695-5319-1","10.1109/SCAC.2014.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913158","GPU;CUDA;performance prediction;memory-intensive","Graphics processing units;Instruction sets;Kernel;Pipelines;Memory management;Random access memory;Throughput","DRAM chips;graphics processing units;parallel architectures;performance evaluation;pipeline processing","DRAM chips;memory-intensive GPU kernels;global memory;Compute Unified Device Architecture;CUDA;NVIDIA GPU;dynamic sensitive pipeline throughput;uncoalesced memory access pattern;critical global memory performance related factor;pipelined global memory model;memory-intensive kernel performance;GPU architecture;data-parallel computing;high-performance accelerators;graphic processing units;performance prediction model","","","","13","","2 Oct 2014","","","IEEE","IEEE Conferences"
"LoSCache: Leveraging Locality Similarity to Build Energy-Efficient GPU L2 Cache","J. Tan; K. Yan; S. L. Song; X. Fu","College of Computer Science and Technology, Jilin University, Changchun, China; College of Communication Engineering, Jilin University, Changchun, China; HPC Group, Pacific Northwest National Laboratory, Richland, USA; ECE Department, University of Houston, Houston, USA","2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)","16 May 2019","2019","","","1190","1195","This paper presents a novel energy-efficient cache design for massively parallel, throughput-oriented architectures like GPUs. Unlike L1 data cache on modern GPUs, L2 cache shared by all the streaming multiprocessors is not the primary performance bottleneck but it does consume a large amount of chip energy. We observe that L2 cache is significantly under-utilized by spending 95.6% of the time storing useless data. If such ""dead time"" on L2 is identified and reduced, L2's energy efficiency can be drastically improved. Fortunately, we discover that the SIMT programming model of GPUs provides a unique feature among threads: instruction-level data locality similarity, which can be used to accurately predict the data re-reference counts at L2 cache block level. We propose a simple design that leverages this Locality Similarity to build an energy-efficient GPU L2 Cache, named LoSCache. Specifically, LoSCache uses the data locality information from a small group of CTAs to dynamically predict the L2-level data re-reference counts of the remaining CTAs. After that, specific L2 cache lines can be powered off if they are predicted to be ""dead"" after certain accesses. Experimental results on a wide range of applications demonstrate that our proposed design can significantly reduce the L2 cache energy by an average of 64% with only 0.5% performance loss.","1558-1101","978-3-9819263-2-3","10.23919/DATE.2019.8714911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8714911","GPU;cache;energy-efficiency;locality similarity","Instruction sets;Graphics processing units;Kernel;Hardware;Programming;Data models;System-on-chip","cache storage;energy conservation;graphics processing units;multi-threading;parallel architectures;power aware computing","energy-efficient GPU L2 cache;L1 data cache;instruction-level data locality similarity;L2 cache block level;L2 cache energy;energy-efficient cache design;SIMT programming model;LoSCache;massively parallel architectures;single instruction multiple thread programming model","","","","20","","16 May 2019","","","IEEE","IEEE Conferences"
"dCUDA: Hardware Supported Overlap of Computation and Communication","T. Gysi; J. Bär; T. Hoefler",NA; NA; NA,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","16 Mar 2017","2016","","","609","620","Over the last decade, CUDA and the underlying GPU hardware architecture have continuously gained popularity in various high-performance computing application domains such as climate modeling, computational chemistry, or machine learning. Despite this popularity, we lack a single coherent programming model for GPU clusters. We therefore introduce the dCUDA programming model, which implements device-side remote memory access with target notification. To hide instruction pipeline latencies, CUDA programs over-decompose the problem and over-subscribe the device by running many more threads than there are hardware execution units. Whenever a thread stalls, the hardware scheduler immediately proceeds with the execution of another thread ready for execution. This latency hiding technique is key to make best use of the available hardware resources. With dCUDA, we apply latency hiding at cluster scale to automatically overlap computation and communication. Our benchmarks demonstrate perfect overlap for memory bandwidth-bound tasks and good overlap for compute-bound tasks.","2167-4337","978-1-4673-8815-3","10.1109/SC.2016.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877130","Distributed memory;gpu;latency hiding;programming model;remote memory access","Graphics processing units;Programming;Computational modeling;Hardware;Instruction sets;Kernel","multi-threading;parallel architectures;storage management","hardware supported overlap;computation;communication;GPU hardware architecture;high-performance computing;GPU clusters;dCUDA programming model;device-side remote memory access;instruction pipeline latencies;hardware scheduler;latency hiding;hardware resources;memory bandwidth-bound tasks;compute-bound tasks;threads","","13","1","31","","16 Mar 2017","","","IEEE","IEEE Conferences"
"GPU Computing","J. D. Owens; M. Houston; D. Luebke; S. Green; J. E. Stone; J. C. Phillips","Univ. of California, Davis; NA; NA; NA; NA; NA","Proceedings of the IEEE","15 Apr 2008","2008","96","5","879","899","The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.","1558-2256","","10.1109/JPROC.2008.917757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4490127","General-purpose computing on the graphics processing unit (GPGPU);GPU computing;parallel computing","Graphics;Central Processing Unit;Physics computing;Engines;Arithmetic;Bandwidth;Microprocessors;Hardware;Computational biophysics;Performance gain","computer graphic equipment;microcomputers;parallel programming","GPU computing;graphics processing unit;graphics engine;parallel programmable processor;peak arithmetic;memory bandwidth;general-purpose computing;microprocessor;high-performance computer system;programming model;game physics;computational biophysics;parallel computing","","1060","30","32","","15 Apr 2008","","","IEEE","IEEE Journals"
"Data Parallel Programming Model for Many-Core Architectures","Y. Zhang","North Carolina State Univ., Raleigh, NC, USA","2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum","1 Sep 2011","2011","","","2065","2068","Emerging accelerating architectures, such as GPUs, have proved successful in providing significant performance gains to various application domains. This is done by exploiting data parallelism in existing algorithms. However, programming in a data-parallel fashion imposes extra burdens to programmers, who are used to writing sequential programs. New programming models and frameworks are needed to reach a balance between programmability, portability and performance. We start from stream processing domain and propose GStream, a general-purpose, scalable data streaming framework on GPUs. The contributions of GStream are as follows: (1) We provide powerful, yet concise language abstractions suitable to describe conventional algorithms as streaming problems. (2) We project these abstractions onto GPUs to fully exploit their inherent massive data-parallelism. (3) We demonstrate the viability of streaming on accelerators. Experiments show that the proposed framework provides flexibility, programmability and performance gains for various benchmarks from a collection of domains, including but not limited to data streaming, data parallel problems, numerical codes and text search. This work lays a foundation to our future work to develop more general data parallel programming models for many-core architectures.","1530-2075","978-1-61284-425-1","10.1109/IPDPS.2011.378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009018","","Graphics processing unit;Parallel processing;Benchmark testing;Libraries;Kernel;Computer architecture","computer graphic equipment;multiprocessing systems;parallel programming","data parallel programming model;many-core architectures;GPU;GStream;general-purpose scalable data streaming framework;language abstractions;massive data-parallelism","","","","22","","1 Sep 2011","","","IEEE","IEEE Conferences"
"A Unified Programming Model for Heterogeneous Computing with CPU and Accelerator Technologies","Y. Xiong","Shanghai Institute of Technology,Computer Science Department,Shanghai,China,201418","2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","23 Jan 2020","2019","","","1","4","This paper provides a unified programming model for heterogeneous computing with CPU and accelerator (like GPU, FPGA, Google TPU, and more) technologies. This new programming model provides a clean interface to application developers and to some extent makes programming across CPUs and accelerators turn into usual programming tasks with common programming languages. Thus it can contribute to improve software productivity for computing across CPUs and accelerators. It can be achieved by extending file managements in common programming languages, such as C/C++, Fortran, Python, MPI, etc., to cover GPUs, FPGAs, and other accelerators (such as Google TPU, etc.) as I/O devices. The programming model can be applied to a number of different computing systems equipped with accelerators, ranging from embedded systems, to mobile computing systems, to HPC systems, and to cloud computing systems.","","978-1-7281-4852-6","10.1109/CISP-BMEI48845.2019.8965946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965946","","Graphics processing units;Programming;Field programmable gate arrays;Computer languages;Printers;Task analysis;Deep learning","cloud computing;embedded systems;message passing;mobile computing;parallel programming;programming languages","unified programming model;heterogeneous computing;Google TPU;CPUs;programming tasks;programming languages;mobile computing systems","","","","50","","23 Jan 2020","","","IEEE","IEEE Conferences"
"A graphical dataflow programming model for on-line signal processing on parallel architectures","Yongsen Jiang","Beihua University, Jilin, China","2010 Third International Symposium on Knowledge Acquisition and Modeling","29 Nov 2010","2010","","","107","110","Many real-world signal processing applications require an enormous amount of computational power. When these applications are deployed in on-line settings, many hurdles including stringent timing constraints must be overcome. Additionally, the number of channels feeding mathematical DSP routines is growing rapidly, easily reaching 1,000 to 100,000 channels. These applications have increasingly demanding performance requirements for generating control outputs which interact with real-world processes, where 1ms loop times are not uncommon. In this paper, we describe a graphical dataflow approach capable of yielding the necessary computational power and meeting aggressive timing constraints. We combine this methodology with strategies for targeting a combination of processors including CPUs, FPGAs, and GPUs deployed on standard PCs, workstations, and real-time systems. We demonstrate this approach through case studies on adaptive mirror control for an extremely large telescope and plasma measurement via soft X-ray tomography.","","978-1-4244-8007-4","10.1109/KAM.2010.5646310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646310","Software Engineering;Parallel Programming;Program Design;Graphical Dataflow","Real time systems;Mirrors;Communities","computer graphic equipment;computerised tomography;coprocessors;data flow computing;field programmable gate arrays;parallel architectures;signal processing;X-ray microscopy","graphical dataflow programming model;online signal processing;parallel architectures;stringent timing constraints;mathematical DSP routines;computational power;aggressive timing constraints;CPU;FPGA;GPU;adaptive mirror control;extremely large telescope;plasma measurement;soft X-ray tomography","","","","6","","29 Nov 2010","","","IEEE","IEEE Conferences"
"Study on Transient Temperature Field Parallel Computing in Cooling Control Based on a GPU Fourier Method","L. Wang; Y. -s. Zhang","State Key Lab. of Mater. Process. & Die & Mould Technol., Huazhong Univ. of Sci. & Technol. Wuhan, Wuhan, China; State Key Lab. of Mater. Process. & Die & Mould Technol., Huazhong Univ. of Sci. & Technol. Wuhan, Wuhan, China","2010 International Conference on Computational Intelligence and Software Engineering","30 Dec 2010","2010","","","1","4","With the evolution of graphics processing units (GPUs) in floating point operations and programmability, GPU has increasingly become powerful and cost-efficient computing architectures, its range of application has expanded tremendously, especially in the area of computational simulation. In this article, the Fourier method combined with GPU acceleration techniques is applied to simulate large-scale transient temperature field in cooling control. Although it is possible to perform temperature field simulation on a personal computer through Fourier method, when grids are huge, a tremendous CPU calculating time is required which is unacceptable. Thus GPU accelerating technique is used for the parallel processing of Fourier method and a significant speedup can be observed. Following the programming model of compute unified device architecture (CUDA), the iteration process of Fourier method is improved into several kernel functions by the single instruction multiple thread (SIMT) mode and multiple processors of the GPU execute these kernel functions. Numerical results with over 13 speedups demonstrate the efficiency of GPU computing technique of the Fourier method. The absolute error between GPU and CPU is less than 10-12 in double-precision.","","978-1-4244-5391-7","10.1109/CISE.2010.5676712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676712","","Graphics processing unit;Mathematical model;Instruction sets;Equations;Transient analysis;Parallel processing;Computational modeling","computer architecture;control engineering computing;cooling;coprocessors;heat systems","transient temperature field parallel computing;cooling control;GPU Fourier method;graphics processing units;floating point operations;programmability;cost-efficient computing architecture;computational simulation;GPU acceleration;large-scale transient temperature field;compute unified device architecture;single instruction multiple thread mode","","1","","6","","30 Dec 2010","","","IEEE","IEEE Conferences"
"Towards high performance and usability programming model for heterogeneous HPC platforms","Myungho Lee; Heeseung Jo; Dong Hoon Choi","Dept of Computer Science and Engineering, Myongji University, 38-2 San Namdong, Cheo-In Gu, Yong In, Kyung Ki Do, Korea 449-728; Department of Information Technology, Chonbuk National University, Jeonju, Jeonbuk, Korea 561-756; Korea Institute of Science and Technology Information (KISTI), 245 Dae Hak Ro, Yu Seong Gu, Daejeon, Korea 305-806","2012 8th International Conference on Computing Technology and Information Management (NCM and ICNIT)","16 Aug 2012","2012","1","","51","57","Latest High Performance Computing (HPC) platforms are built with heterogeneous chips such as multicore microprocessors and multicore GPUs (Graphic Processing units), thus they are commonly called as Heterogeneous High Performance Computing (HPC) platforms. Various programming models have been developed and proposed for heterogeneous platforms. However, their wide adoption in the user community is predicted to be limited, because of low performance, low usability due to revealing architectural details in the program which burdens the programmers, and most importantly the limited SIMD execution model which relies on the GPU for most of the computations in the program which can limit the performance. Thus a more general programming model beyond SIMD which is easy to use and leads to high performance needs to be developed. In this paper, we propose methods to achieve this goal by considering all types of parallelism according to Flynn's classification (SIMD, MIMD, MISD). Our proposed methods incorporate these parallelisms in the existing high usability programming models and can lead to significant performance improvements.","","978-89-88678-68-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6268469","Heterogenesou HP platformC;programming model;OpenMP;CUDA;OpenACC","Graphics processing unit;Programming;Computational modeling;Registers;Levee;Engines","graphics processing units;multiprocessing systems;parallel processing","heterogeneous HPC platforms;usability programming model;high performance computing;multicore microprocessors;multicore GPU;graphic processing units;SIMD execution model;Flynn classification","","1","","20","","16 Aug 2012","","","IEEE","IEEE Conferences"
"2D-FMFI SAR application on HPC architectures with OmpSs parallel programming model","F. Kraja; A. Bode; X. Martorell","Chair of Computer Architecture, Department of Informatics, Technische Universität München, Munich, Germany; Chair of Computer Architecture, Department of Informatics, Technische Universität München, Munich, Germany; Computer Architecture Department, Universitat Politecnica de Catalunya, Barcelona Supercomputing Center, Barcelona, Spain","2012 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)","16 Aug 2012","2012","","","115","121","Spacecraft on-board platforms will soon contain high performance off-the-shelf computing components, as the only way to deal with applications which require High Performance Computing (HPC) capabilities. Heterogeneous architectures, give the possibility to profit from specific features of such components, by combining them on the same computing platform. Such architectures integrate different multicore CPUs with many-core accelerators (GPGPUs) so that parts of the same application can efficiently execute on the CPU while other parts can profit more from the GPU features. The problem stands in the way how to program these heterogeneous architectures. In this paper we introduce the OmpSs Programming Model as a framework to program heterogeneous architectures. For benchmarking purposes, we use a Synthetic Aperture Radar (SAR) application, which we parallelized with OmpSs for Symmetric Multi-Processor (SMP) architectures and for hybrid SMP/GPU environments. We also compare the results obtained with OmpSs with the ones obtained with OpenMP in SMP environments and it turns out that OmpSs outperforms OpenMP. When applying OmpSs to the SAR code for hybrid architectures, OmpSs does not offer better performance than CUDA, but it offers support for heterogeneous architectures and it also increases programmer's productivity.","","978-1-4673-1916-4","10.1109/AHS.2012.6268638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6268638","OmpSs;OpenMP;CUDA;Space Applications;SAR;SMP;GPU","Graphics processing unit;Computer architecture;Synthetic aperture radar;Image reconstruction;Runtime;Interpolation;Programming","aerospace engineering;aerospace instrumentation;graphics processing units;multiprocessing systems;parallel programming;software packages;space vehicles;synthetic aperture radar","2D-FMFI SAR application;HPC architectures;OmpSs parallel programming model;spacecraft on-board platforms;high performance computing;HPC;heterogeneous architectures;multicore CPU;GPGPU;many-core accelerators;synthetic aperture radar;symmetric multiprocessor architectures;SMP/GPU environments;OpenMP","","","","18","","16 Aug 2012","","","IEEE","IEEE Conferences"
"Heterogeneous Habanero-C (H2C): A Portable Programming Model for Heterogeneous Processors","D. Majeti; V. Sarkar",NA; NA,"2015 IEEE International Parallel and Distributed Processing Symposium Workshop","1 Oct 2015","2015","","","708","717","Heterogeneous architectures with their diverse architectural features impose significant programmability challenges. Existing programming systems involve non-trivial learning and are not productive, not portable, and are challenging to tune for performance. In this paper, we introduce Heterogeneous Habanero-C (H2C), which is an implementation of the Habanero execution model for modern heterogeneous (CPU + GPU) architectures. The H2C language provides high-level constructs to specify the computation, communication, and synchronization in a given application. H2C also implements novel constructs for task partitioning and locality. The H2C (source-to-source) compiler and runtime framework efficiently map these high-level constructs onto the underlying heterogeneous platform, which can include multiple CPU cores and multiple GPU devices, possibly from different vendors. Experimental evaluations of four applications show significant improvements in productivity, portability, and performance.","","978-1-4673-7684-6","10.1109/IPDPSW.2015.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284378","heterogeneous architectures;programming model;automatic data distribution;OpenCL;event management","Runtime;Graphics processing units;Computer architecture;Optimization;Kernel;Programming","graphics processing units;multiprocessing systems;program compilers","portable programming model;heterogeneous processors;programming systems;Heterogeneous Habanero-C;Habanero execution model;modern heterogeneous architectures;compiler;runtime framework;multiple CPU cores;multiple GPU devices","","6","","20","","1 Oct 2015","","","IEEE","IEEE Conferences"
"High-speed parallel wavelet algorithm based on CUDA and its application in three-dimensional surface texture analysis","Wang Jianjun; Lu Wenlong; Liu Xiaojun; Jiang Xiangqian","The State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; The State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; The State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; The State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China","2011 International Conference on Electric Information and Control Engineering","27 May 2011","2011","","","2249","2252","A new efficient parallel wavelet algorithm was presented in order to speed up wavelet transform in three dimensional surface texture analysis. It is based NVIDIA's CUDA (Compute Unified Device Architecture), a new general purpose parallel programming model and instruction set architecture that leverage computational problems on GPU more efficient than CPU. Compared with CPU, GPU has evolved into a highly parallel, multithread, multicore processor with tremendous computational horsepower and very high memory bandwidth. GPU is well-suited to address data-parallel computation problems rather than flow controlled problems. Wavelet transform can use data-parallel programming model so data elements will be mapped to parallel processing threads to speed up the computations. CUDA wavelet decomposition and reconstruction algorithms were realized based on the analysis above. Experiments show that the parallelization of the fast wavelet decomposition transform for GPU speedup 34×-38× over CPU, reconstruction transform speedup 29x-33x over CPU.","","978-1-4244-8039-5","10.1109/ICEICE.2011.5778225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5778225","CUDA;GPU;Wavelet;surface texture","Graphics processing unit;Wavelet transforms;Convolution;Surface waves;Wavelet analysis;Instruction sets","computer graphic equipment;computer graphics;coprocessors;general purpose computers;instruction sets;multiprocessing systems;multi-threading;parallel algorithms;parallel architectures;wavelet transforms","high-speed parallel wavelet algorithm;CUDA;three-dimensional surface texture analysis;NVIDIA;compute unified device architecture;general purpose parallel programming model;instruction set architecture;GPU;multithread processor;parallel processor;multicore processor;computational horsepower;memory bandwidth;data-parallel computation problems;data-parallel programming model;reconstruction transform speedup","","2","","13","","27 May 2011","","","IEEE","IEEE Conferences"
"Gdarts: A GPU-Based Runtime System for Dataflow Task Programming on Dependency Applications","M. Li; Q. Jiang; H. Lin; H. An","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China","2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","26 Mar 2020","2019","","","547","552","Massively multithreaded GPUs achieve considerable performance improvements by thousands of processing cores running simultaneously for compute-intensive applications. However, present parallel programming model suffers significant performance degradation on dependency scenarios, where tasks are assigned into multiple thread blocks and the parallelism is limited by inter-block dependencies. Massively multithreaded GPUs achieve considerable performance improvements by thousands of processing cores running simultaneously for compute-intensive applications. However, present parallel programming model suffers significant performance degradation on dependency scenarios, where tasks are assigned into multiple thread blocks and the parallelism is limited by inter-block dependencies. This paper proposes Gdarts, a dataflow runtime system for GPU workloads with data dependencies. Employing asynchronous task programming, Gdarts builds Stream Multiprocessors (SM) as standalone workers towards fine-granular and independent kernels. Meanwhile, it designs a two-level task scheduler to adapt GPU's hierarchical memory environment. Such a scheme, leveraging the hybrid memory resources, fulfills requirements for workload balancing across SMs, as well as flexible priority scheduling. Based on the improved controllability and flexible support fosr kernel scheduling, Gdarts complements the existing GPU with insights of dataflow diagram by two heuristic schedule approaches, node-degree and data-driven priority.","","978-1-7281-4328-6","10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9047478","task based execution;dataflow runtime system;schedule;GPU programming","Task analysis;Graphics processing units;Runtime;Programming;Parallel processing;Schedules;Kernel","coprocessors;graphics processing units;multiprocessing systems;multi-threading;parallel programming;scheduling","Gdarts;GPU-based runtime system;dataflow task programming;GPUs;processing cores;compute-intensive applications;parallel programming model;parallelism;dataflow runtime system;data dependencies;asynchronous task programming;two-level task scheduler;kernel scheduling","","","","20","","26 Mar 2020","","","IEEE","IEEE Conferences"
"Divide and conquer skeleton on GPU","F. Baghayeri; H. Deldari; D. Bahrepour","Department of Computer Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran; Department of Computer Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran; Department of Computer Engineering, Mashhad Branch Islamic Azad University, Mashhad, Iran","2014 International Congress on Technology, Communication and Knowledge (ICTCK)","9 Feb 2015","2014","","","1","6","Parallelism is a suitable approach for speeding up the massive computations of applications, but parallel programming is difficult yet. Algorithmic skeleton is a parallel programming model that provides a high level of abstraction for programmers. This approach uses the pre-defined components to facilitate easier parallel programming. Divide and conquer (DC) is an appropriate parallel pattern for implementation as a skeleton. The solution of the original problem is obtained by dividing it into smaller sub-problems and solving them in parallel. Today, graphics processor unit (GPU) is an attractive computational processor for doing tasks in parallel, because it has a large number of process units. In this paper, divide and conquer skeleton on GPU has been proposed and named OC_GFV.DC_GPU is a divide and conquer skeleton that is implemented on GPU that using a consistent programming interface in C++ for easier parallel programming. Performance of this skeleton has been evaluated by mergesort and sobeledge detection. The results show that obtained speedup at this skeleton is more than 2 on GPU.","","978-1-4799-8021-5","10.1109/ICTCK.2014.7033531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033531","Algorithmic Skeleton;Divide and Conquer Skeleton;Graphics Processor Unit(GPU);Programming Interface in C++;Mergesort;Sobel Edge Detection","Skeleton;Graphics processing units;Parallel processing;Parallel programming;Libraries;Computational modeling","C++ language;divide and conquer methods;edge detection;graphics processing units;parallel programming","parallel programming model;parallel pattern;graphics processor unit;computational processor;divide and conquer skeleton;DC_GPU;consistent programming interface;C++;mergesort;sobeledge skeleton","","1","","12","","9 Feb 2015","","","IEEE","IEEE Conferences"
"Task Scheduling Greedy Heuristics for GPU Heterogeneous Cluster Involving the Weights of the Processor","K. Zhang; B. Wu","Sch. of Comput. Sci., Fudan Univ., Shanghai, China; Sch. of Comput. Sci., Fudan Univ., Shanghai, China","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum","31 Oct 2013","2013","","","1817","1827","Modern GPUs are gradually used by more and more cluster computing systems as the high performance computing units due to their outstanding computational power, whereas bringing system-level (among different nodes) architectural heterogeneity to cluster. In this paper, based on MPI and CUDA programming model, we aim to investigate task scheduling for GPU heterogeneous cluster by taking into account the system-level heterogeneous characteristics and also involving the weights of the processor (both CPUs and GPUs). At first, based on our GPU heterogeneous cluster, we classify executing tasks to six major classifications according to their parallelism degrees, input data sizes, and processing workloads. Then, aiming to realize the approximately optimal mapping between tasks and computing resources, a task scheduling strategy is presented. In this paper, we present the WSLSA greedy heuristic which can involve the weights of the processor. Besides, we also define two measurement factors for the task assignments. One is the maximum value of total workloads for all task assignments to consider the maximum workloads for the GPU heterogeneity cluster. The other is the distribution of task assignments which can determine the load balance of the task assignments for the GPU heterogeneity cluster. The other is the distribution of task assignments which can determine the load balance of the task assignments for the GPU heterogeneity cluster.","","978-0-7695-4979-8","10.1109/IPDPSW.2013.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651082","Task Scheduling;Greedy Heuristics;GPU Heterogeneous Cluster;WSLSA Greedy Heuristic","Graphics processing units;Kernel;Processor scheduling;Computer architecture;Scheduling;Clustering algorithms","application program interfaces;graphics processing units;greedy algorithms;parallel architectures;scheduling;task analysis;workstation clusters","task scheduling greedy heuristics;GPU heterogeneous cluster;cluster computing systems;high performance computing units;computational power;system-level architectural heterogeneity;MPI;CUDA programming model;system-level heterogeneous characteristics;executing tasks classify;parallelism degrees;data sizes;processing workloads;computing resources;WSLSA greedy heuristic;task assignments;load balance","","2","","27","","31 Oct 2013","","","IEEE","IEEE Conferences"
"Mini-Gunrock: A Lightweight Graph Analytics Framework on the GPU","Yangzihao Wang; S. Baxter; J. D. Owens",NA; NA; NA,"2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","24 Aug 2017","2017","","","616","626","Existing GPU graph analytics frameworks are typically built from specialized, bottom-up implementations of graph operators that are customized to graph computation. In this work we describe Mini-Gunrock, a lightweight graph analytics framework on the GPU. Unlike existing frameworks, Mini-Gunrock is built from graph operators implemented with generic transform-based data-parallel primitives. Using this method to bridge the gap between programmability and high performance for GPU graph analytics, we demonstrate operator performance on scale-free graphs with an average 1.5x speedup compared to Gunrock's corresponding operator performance. Mini-Gunrock's graph operators, optimizations, and applications code have 10x smaller code size and comparable overall performance vs. Gunrock.","","978-1-5386-3408-0","10.1109/IPDPSW.2017.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965101","GPU computing;Graph analytics;Programming model;Runtime system","Transforms;Graphics processing units;Kernel;Libraries;Programming;Optimization;Computational modeling","graph theory;graphics processing units;mathematics computing;optimisation;parallel processing","miniGunrock;lightweight graph analytics framework;GPU graph analytics frameworks;graph operators;graph computation;generic transform-based data-parallel primitives;scale-free graphs;optimizations","","","","11","","24 Aug 2017","","","IEEE","IEEE Conferences"
"Programming GPU Clusters with Shared Memory Abstraction in Software","K. I. Karantasis; E. D. Polychronopoulos","Dept. of Comput. Eng. & Inf., Univ. of Patras, Rio, Greece; Dept. of Comput. Eng. & Inf., Univ. of Patras, Rio, Greece","2011 19th International Euromicro Conference on Parallel, Distributed and Network-Based Processing","24 Mar 2011","2011","","","223","230","As many-core graphics processors gain an increasingly important position concerning the advancements on modern highly concurrent processors, we are experiencing the deployment of the first heterogeneous clusters that are based on GPUs. The attempts to match future expectations in computational power and energy saving with hybrid - GPU-based - clusters are expected to grow in the next years, and much of their success will depend on the provision of the appropriate programming tools. In the current paper we propose a programming model for GPU clusters that is based on shared memory abstraction. We give evidence for the applicability of the proposed model under two cases. In the first case we describe an implementation procedure that involves the utilization of Intel Cluster OpenMP, a cluster-enabled OpenMP implementation. Subsequently, we present an extended version of Pleiad, a cluster middleware which is based on the Java platform. The evaluation of these schemes under two characteristic computationally intensive applications on a 4-node multi-GPU cluster, reveals that such approaches can easily enhance existing GPU software development tools, such as CUDA, and they can lead to a significant acceleration of applications that can benefit from many-core GPU clusters.","2377-5750","978-1-4244-9682-2","10.1109/PDP.2011.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5739005","GPU Clusters;Software DSM;CUDA;OpenMP;Pleiad","Graphics processing unit;Programming;Middleware;Benchmark testing;Computational modeling;Kernel;Multicore processing","computer graphic equipment;coprocessors;Java;middleware;multiprocessing programs;power aware computing;shared memory systems;software tools","GPU cluster programming;shared memory abstraction;many-core graphics processors;energy saving;programming tools;Intel Cluster OpenMP;cluster middleware;Pleiad;Java platform;4-node multiGPU cluster;GPU software development tools","","8","","27","","24 Mar 2011","","","IEEE","IEEE Conferences"
"Matrix multiplication beyond auto-tuning: Rewrite-based GPU code generation","M. Steuwer; T. Remmelg; C. Dubach","University of Edinburgh, United Kingdom; University of Edinburgh, United Kingdom; University of Edinburgh, United Kingdom","2016 International Conference on Compliers, Architectures, and Sythesis of Embedded Systems (CASES)","17 Nov 2016","2016","","","1","10","Graphics Processing Units (GPUs) are used as general purpose parallel accelerators in a wide range of applications. They are found in most computing systems, and mobile devices are no exception. The recent availability of programming APIs such as OpenCL for mobile GPUs promises to open up new types of applications on these devices. However, producing high performance GPU code is extremely difficult. Subtle differences in device characteristics can lead to large performance variations when different optimizations are applied. As we will see, this is especially true for a mobile GPU such as the ARM Mali GPU which has a very different architecture than desktop-class GPUs. Code optimized and tuned for one type of GPUs is unlikely to achieve the performance potential on another type of GPUs. Auto-tuners have traditionally been an answer to this performance portability challenge. For instance, they have been successful on CPUs for matrix operations, which are used as building blocks in many high-performance applications. However, they are much harder to design for different classes of GPUs, given the wide variety of hardware characteristics. In this paper, we take a different perspective and show how performance portability for matrix multiplication is achieved using a compiler approach. This approach is based on a recently developed generic technique that combines a high-level programming model with a system of rewrite rules. Programs are automatically rewritten in successive steps, where optimizations decision are made.This approach is truly performance portable, resulting in high-performance code for very different types of architectures such as desktop and mobile GPUs. In particular, we achieve a speedup of 1.7x over a state-of-the-art auto-tuner on the ARM Mali GPU.","","978-1-4503-4482-1","10.1145/2968455.2968521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745278","","Graphics processing units;Optimization;Mobile communication;Hardware;Instruction sets;Performance evaluation;Registers","graphics processing units;mathematics computing;matrix multiplication;optimising compilers;rewriting systems;software portability","matrix multiplication;rewrite-based GPU code generation;graphics processing units;general purpose parallel accelerators;programming APIs;OpenCL;mobile GPUs;high performance GPU code;ARM Mali GPU;code optimization;auto-tuners;performance portability;compiler approach;high-level programming model;optimizations decision","","4","","25","","17 Nov 2016","","","IEEE","IEEE Conferences"
"GPU-Accelerated Computations for Supersonic Flow Modeling on Hybrid Grids","Z. Tian; J. Lai; F. Yang; H. Li","College of Aerospace Science and Engineering, National University of Defense Technology,Changsha,China; College of Aerospace Science and Engineering, National University of Defense Technology,Changsha,China; College of Aerospace Science and Engineering, National University of Defense Technology,Changsha,China; College of Aerospace Science and Engineering, National University of Defense Technology,Changsha,China","2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)","13 May 2021","2020","","","1391","1397","With its strong floating-point operation capability and high memory bandwidth in data parallelism, the graphics processing unit (GPU) has been widely used in general-purpose computing. GPU-based computations have been extensively applied in the field of computational fluid dynamics (CFD). This paper aims to design an extremely efficient double-precision GPU-accelerated parallel algorithm for supersonic flow computations on hybrid grids. Compute unified device architecture (CUDA) is used as a general-purpose parallel computing platform and programming model to perform parallel computing codes on GPUs. The cell-centered finite volume method based on unstructured grids is used in the spatial discretization of governing equations, whereas the three-stage explicit Runge-Kutta scheme with second-order accuracy is used for temporal discretization. The turbulence is solved by using the K-ω SST two-equation model. Three test cases are studied to validate the computational accuracy of the proposed algorithm. The numerical results agree well with the experiment data, thereby suggesting that the GPU-accelerated parallel algorithm has good accuracy.","","978-1-6654-2314-4","10.1109/ICMCCE51767.2020.00305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9421767","graphics processing unit;compute unified device architecture;supersonic flow;hybrid grids;parallel algorithm","Performance evaluation;Computational modeling;Computational fluid dynamics;Graphics processing units;Process control;Life estimation;Programming","computational fluid dynamics;finite volume methods;graphics processing units;grid computing;parallel algorithms;parallel architectures;Runge-Kutta methods;supersonic flow","GPU-accelerated computations;hybrid grids;floating-point operation capability;high memory bandwidth;data parallelism;graphics processing unit;GPU-based computations;computational fluid dynamics;double-precision GPU-accelerated parallel algorithm;supersonic flow computations;compute unified device architecture;programming model;parallel computing codes;cell-centered finite volume method;unstructured grids;three-stage explicit Runge-Kutta scheme;K-ω SST two-equation model;supersonic flow modeling;general-purpose parallel computing platform","","","","30","","13 May 2021","","","IEEE","IEEE Conferences"
"Multimobile Robot Cooperative Localization Using Ultrawideband Sensor and GPU Acceleration","J. Xin; G. Xie; B. Yan; M. Shan; P. Li; K. Gao","Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an 710048, China.; Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an 710048, China (e-mail: guoxie@xaut.edu.cn); Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an 710048, China.; Australian Centre for Field Robotics, The University of Sydney, Sydney, NSW 2006, Australia.; Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an 710048, China.; Shaanxi Key Laboratory of Complex System Control and Intelligent Information Processing, Xi'an University of Technology, Xi'an 710048, China.","IEEE Transactions on Automation Science and Engineering","","2021","PP","99","1","12","To tackle the poor localization accuracy of multimobile robots caused by non-line-of-sight (NLOS) errors in a complex indoor environment and to meet the real-time requirement, this article proposes a multimobile robot cooperative localization system using ultrawideband (UWB) sensor and GPU hardware acceleration. First, a UWB multinode ranging network is established to obtain the relative distance information between robots and anchors. Then, the line-of-sight (LOS) and NLOS errors in distance information are effectively mitigated by using the proposed UWB ranging error mitigation algorithm based on the Bayesian filter. A cooperative particle filter (PF) localization algorithm based on the Gibbs sampling is designed to estimate the position information of each robot at any time. Finally, in order to improve the real-time performance of the collaborative localization system, a parallel Gibbs collaborative localization algorithm that can be accelerated by GPU is proposed considering the characteristics of GPU hardware and CUDA programming model. The experimental results of three TurtleBot2 mobile robots in real scene show that the proposed multimobile robot cooperative localization system using UWB technology can estimate the position information of each robot robustly and accurately, and the localization accuracy is superior to that of the popular extended Kalman filter (EKF) and PF algorithms. It is shown through further evaluations that the proposed parallel algorithm achieves about 3.2 times acceleration effect in the scenarios of three mobile robots. The speed gain is found more significant with more robots, which substantially improves the real-time performance of the cooperative localization system. In the test with seven mobile robots, the speedup is as high as 11.9, that is, the execution time of the algorithm is only 8.39% of that of the original algorithm.","1558-3783","","10.1109/TASE.2021.3117949","National Natural Science Foundation of China(grant numbers:61873200,61873201,U20A20225,61833013); National Key Research and Development Program of China(grant numbers:2018YFB1201500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576116","Cooperative localization;GPU acceleration;multimobile robot systems;real scene;real time","Location awareness;Robots;Robot sensing systems;Distance measurement;Real-time systems;Graphics processing units;Wireless sensor networks","","","","","","","IEEE","15 Oct 2021","","","IEEE","IEEE Early Access Articles"
"Vispark: GPU-accelerated distributed visual computing using spark","W. Choi; W. Jeong","School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea; School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea","2015 IEEE 5th Symposium on Large Data Analysis and Visualization (LDAV)","7 Dec 2015","2015","","","125","126","With the growing need of big data processing in diverse application domains, MapReduce (e.g., Hadoop) becomes one of the standard computing paradigms for large-scale computing on a cluster system. Despite of its popularity, the current MapReduce framework suffers from inflexibility and inefficiency inherent from its programming model and system architecture. In order to address these problems, we propose Vispark, a novel extension of Spark for GPU-accelerated MapReduce processing on array-based scientific computing and image processing tasks. Vispark provides an easy-to-use, Python-like high-level language syntax and a novel data abstraction for MapReduce programming on a GPU cluster system. Vispark introduces a programming abstraction for accessing neighbor data in the mapper function, which greatly simplifies many image processing tasks using MapReduce by reducing memory footprints and bypassing the reduce stage. We demonstrate the performance of our prototype system on several visual computing tasks, such as image processing, and K-means clustering.","","978-1-4673-8517-6","10.1109/LDAV.2015.7348080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348080","","Graphics processing units;Sparks;Kernel;Programming;Visualization;Arrays;Image processing","Big Data;computer vision;graphics processing units;parallel programming;pattern clustering;software architecture;visual programming","Vispark;GPU-accelerated distributed visual computing;big data processing;Hadoop;large-scale computing;GPU cluster system;system architecture;Spark;GPU-accelerated MapReduce processing;array-based scientific computing;image processing tasks;Python-like high-level language syntax;data abstraction;MapReduce programming;programming abstraction;neighbor data access;memory footprints reduction;K-means clustering","","2","","3","","7 Dec 2015","","","IEEE","IEEE Conferences"
"A Data Communication Scheduler for Stream Programs on CPU-GPU Platform","T. Tang; X. Xu; Y. Lin","Nat. Lab. for Parallel & Distrib. Process., Nat. Univ. of Defense Technol., Changsha, China; Nat. Lab. for Parallel & Distrib. Process., Nat. Univ. of Defense Technol., Changsha, China; Nat. Lab. for Parallel & Distrib. Process., Nat. Univ. of Defense Technol., Changsha, China","2010 10th IEEE International Conference on Computer and Information Technology","16 Sep 2010","2010","","","139","146","In recent years, heterogeneous parallel system have become a focus research area in high performance computing field. Generally, in a heterogeneous parallel system, CPU provides the basic computing environment and special purpose accelerator (GPU in this paper) provides high computing performance. However, the overall performance of the system is prone to be limited by the data communication between the CPU and the GPU. Data communication is typically used to synchronize the array on the CPU and the stream (in AMD's terminology) on the GPU. In many cases, programmers just add data synchronization for each GPU invoking independently. It is easy to program in this manner but much redundant communication may be introduced, which will dramatically degrade the overall performance. To alleviate this problem, based on the stream programming model, we propose a heuristic data communication schedule approach in this paper. By analyzing the state transition of stream/array data pair, relaxing the synchronization strategy conditionally and considering optimization for branch and loop control structure, our approach can significantly reduce the redundant data communication in most cases.","","978-1-4244-7548-3","10.1109/CIT.2010.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5578445","GPU;data communication schedule;structural analysis","Graphics processing unit;Data communication;Arrays;Kernel;Streaming media;Schedules","computer graphic equipment;coprocessors;data communication;scheduling","data communication scheduler;stream programs;CPU-GPU platform;heterogeneous parallel system;high performance computing field;stream programming model;synchronization strategy;loop control structure","","","","18","","16 Sep 2010","","","IEEE","IEEE Conferences"
"Performance Analysis of Parallel Programming Paradigms on CPU-GPU Clusters","B. N. Chandrashekhar; H. A. Sanjay; T. Srinivas","Nitte meenakshi Institute of Technology,Department of Information science and Engineering,Benagluru,India,64; Nitte meenakshi Institute of Technology,Department of Information science and Engineering,Benagluru,India,64; Nitte meenakshi Institute of Technology,Department of Information science and Engineering,Benagluru,India,64","2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS)","12 Apr 2021","2021","","","646","651","CPU-GPU based cluster computing in today's modern world encompasses the domain of complex and high-intensity computation. To exploit the efficient resource utilization of a cluster, traditional programming paradigm is not sufficient. Therefore, in this article, the performance parallel programming paradigms like OpenMP on CPU cluster and CUDA on GPU cluster using BFS and DFS graph algorithms is analyzed. This article analyzes the time efficiency to traverse the graphs with the given number of nodes in two different processors. Here, CPU with OpenMP platform and GPU with CUDA platform support multi-thread processing to yield results for various nodes. From the experimental results, it is observed that parallelization with the OpenMP programming model using the graph algorithm does not boost the performance of the CPU processors, instead, it decreases the performance by adding overheads like idling time, inter-thread communication, and excess computation. On the other hand, the CUDA parallel programming paradigm on GPU yields better results. The implementation achieves a speed-up of 187 to 240 times over the CPU implementation. This comparative study assists the programmers provocatively and select the optimum choice among OpenMP and CUDA parallel programming paradigms.","","978-1-7281-9537-7","10.1109/ICAIS50930.2021.9395977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395977","Breadth First Search(BFS);Central Processing Unit(CPU);CUDA;Depth First Search(DFS);Graphics Processing Unit(GPU) OpenMP;Performance analysis","Parallel programming;Computational modeling;Graphics processing units;Clustering algorithms;Cluster computing;Performance analysis;Resource management","cluster computing;graph theory;graphics processing units;message passing;multi-threading;parallel architectures;performance evaluation","multithread processing;DFS graph algorithm;BFS graph algorithm;resource utilization;cluster computing;CUDA parallel programming paradigms;CPU processors;OpenMP programming model;parallelization;time efficiency;CPU-GPU clusters;performance analysis","","","","17","","12 Apr 2021","","","IEEE","IEEE Conferences"
"Towards a parallelization and performance optimization of Viola and Jones algorithm in heterogeneous CPU-GPU mobile system","A. Ghorbel; N. Ben Amor; M. Jallouli","Computer and Embedded Systems Laboratory, Ecole Nationale d'Ingénieurs de Sfax (ENIS), Tunisia; Computer and Embedded Systems Laboratory, Ecole Nationale d'Ingénieurs de Sfax (ENIS), Tunisia; Computer and Embedded Systems Laboratory, Ecole Nationale d'Ingénieurs de Sfax (ENIS), Tunisia","2015 15th International Conference on Intelligent Systems Design and Applications (ISDA)","13 Jun 2016","2015","","","528","532","Parallel computing on heterogeneous multiprocessor architecture is a new technique used to tackle the complexity of actual media applications. Such technique is used on an embedded architecture composed of 2ARMs coupled to a GPU. In this paper, an approach for real time Viola and Jones face detection algorithm using CPU-GPU based platform is presented. First, the application is implemented and parallelized on two identical ARM CortexA9 CPUs using tasks and data levels parallelism. This technique does not achieve the timing objectives. To ensure greater performance while reducing energy ratio, we extend our parallelization technique to support a GPU as an accelerator to perform non graphical tasks. OpenCL, the heterogeneous parallel programming model, is used to ensure communication between CPU and GPU.","2164-7151","978-1-4673-8709-5","10.1109/ISDA.2015.7489172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489172","Parallel computing;GPU;mobile device","Computer architecture;Neon;Graphics processing units;Acceleration;Hardware;Synchronization;Gray-scale","face recognition;graphics processing units;microprocessor chips;mobile computing;parallel programming","Viola algorithm performance optimization;Jones algorithm performance optimization;heterogeneous CPU-GPU mobile system;parallel computing;heterogeneous multiprocessor architecture;media applications;embedded architecture;face detection algorithm;identical ARM CortexA9 CPU;data level parallelism;task level parallelism;energy ratio reduction;parallelization technique;OpenCL;heterogeneous parallel programming model","","1","","14","","13 Jun 2016","","","IEEE","IEEE Conferences"
"A GPU implementation of tiled belief propagation on Markov Random Fields","H. Eslami; T. Kasampalis; M. Kotsifakou","Department of Computer Science University of Illinois at Urbana-Champaign Urbana, Illinois 61801; Department of Computer Science University of Illinois at Urbana-Champaign Urbana, Illinois 61801; Department of Computer Science University of Illinois at Urbana-Champaign Urbana, Illinois 61801","2013 Eleventh ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE 2013)","21 Nov 2013","2013","","","143","146","In the MEMOCODE Design Contest 2013, we are participating with a parallelized version of tiled belief propagation method for stereo matching. The proposed algorithm is implemented in CUDA programming model to leverage parallel processing capabilities of GPUs. In our solution, the original tiled belief propagation algorithm is combined with a number of novel optimizations specific to parallel programs in CUDA. For the given test inputs, the proposed solution runs in 7.96 milliseconds on Nvidia Tesla C2050, achieving acceptable accuracy with respect to the reference code. To the best of authors' knowledge, there is no prior work in optimizing a parallelized version of the tiled belief propagation algorithm.","","978-1-4799-0905-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6670953","","Message systems;Tiles;Belief propagation;Graphics processing units;Synchronization;Stereo vision;Parallel processing","belief maintenance;graphics processing units;image matching;Markov processes;optimisation;parallel architectures;stereo image processing","GPU implementation;tiled belief propagation;Markov random fields;MEMOCODE design contest 2013;stereo matching;CUDA programming model;parallel processing capabilities;optimizations;parallel programs;Nvidia Tesla C2050","","","","5","","21 Nov 2013","","","IEEE","IEEE Conferences"
"Exploiting GPUDirect RDMA in Designing High Performance OpenSHMEM for NVIDIA GPU Clusters","K. Hamidouche; A. Venkatesh; A. A. Awan; H. Subramoni; C. -H. Chu; D. K. Panda","Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","2015 IEEE International Conference on Cluster Computing","29 Oct 2015","2015","","","78","87","GPUDirect RDMA (GDR) brings the high-performance communication capabilities of RDMA networks like InfiniBand (IB) to GPUs (referred to as ""Device""). It enables IB network adapters to directly write/read data to/from GPU memory. Partitioned Global Address Space (PGAS) programming models, such as OpenSHMEM, provide an attractive approach for developing scientific applications with irregular communication characteristics by providing shared memory address space abstractions, along with one-sided communication semantics. However, current approaches and designs of OpenSHMEM on GPU clusters do not take advantage of the GDR features leading to inefficiencies and sub-optimal performance. In this paper, we analyze the performance of various OpenSHMEM operations with different inter-node and intra-node communication configurations (Host-to-Device, Device-to-Device, and Device-to-Host) on GPU based systems. We propose novel designs that ensure ""truly one-sided"" communication for the different inter-/intra-node configurations identified above while working around the hardware limitations. To the best of our knowledge, this is the first work that investigates GDR-aware designs for OpenSHMEM communication operations. Experimental evaluations indicate 2.5X and 7X improvement in point-point communication for intra-node and inter-node, respectively. The proposed framework achieves 2.2µs for an intra-node 8 byte put operation from Host-to-Device, and 3.13µs for an inter-node 8 byte put operation from GPU to remote GPU. With Stencil2D application kernel from SHOC benchmark suite, we observe a 19% reduction in execution time on 64 GPU nodes. Further, for GPULBM application, we are able to improve the performance of the evolution phase by 53% and 45% on 32 and 64 GPU nodes, respectively.","2168-9253","978-1-4673-6598-7","10.1109/CLUSTER.2015.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307568","PGAS;OpenSHMEM;GPU Direct RDMA;CUDA","Graphics processing units;Performance evaluation;Peer-to-peer computing;Runtime;Electronics packaging;Programming;Pipeline processing","application program interfaces;graphics processing units;parallel architectures;shared memory systems","GPUDirect RDMA;GDR;high performance OpenSHMEM;NVIDIA GPU cluster;InfiniBand;IB network;partitioned global address space;PGAS programming model;shared memory address space abstraction;CUDA API","","8","","32","","29 Oct 2015","","","IEEE","IEEE Conferences"
"Designing a unified programming model for heterogeneous machines","M. Garland; M. Kudlur; Y. Zheng","NA; NA; Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","25 Feb 2013","2012","","","1","11","While high-efficiency machines are increasingly embracing heterogeneous architectures and massive multithreading, contemporary mainstream programming languages reflect a mental model in which processing elements are homogeneous, concurrency is limited, and memory is a flat undifferentiated pool of storage. Moreover, the current state of the art in programming heterogeneous machines tends towards using separate programming models, such as OpenMP and CUDA, for different portions of the machine. Both of these factors make programming emerging heterogeneous machines unnecessarily difficult. We describe the design of the Phalanx programming model, which seeks to provide a unified programming model for heterogeneous machines. It provides constructs for bulk parallelism, synchronization, and data placement which operate across the entire machine. Our prototype implementation is able to launch and coordinate work on both CPU and GPU processors within a single node, and by leveraging the GASNet runtime, is able to run across all the nodes of a distributed-memory machine.","2167-4337","978-1-4673-0806-9","10.1109/SC.2012.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468503","","Graphics processing units;Programming;Instruction sets;Runtime;Message systems;Parallel processing","distributed memory systems;multi-threading;programming languages","distributed memory machine;GASNet runtime;GPU processors;CPU processors;data placement;Phalanx programming model;CUDA;OpenMP;undifferentiated pool;mental model;contemporary mainstream programming languages;massive multithreading;heterogeneous architectures;high efficiency machines;heterogeneous machines;unified programming model","","19","","25","","25 Feb 2013","","","IEEE","IEEE Conferences"
"A case study of OpenCL on an Android mobile GPU","J. A. Ross; D. A. Richie; S. J. Park; D. R. Shires; L. L. Pollock","Engility Corporation, Chantilly, VA, USA; Brown Deer Technology, Forest Hill, MD, USA; U.S. Army Research Laboratory, APG, MD, USA; U.S. Army Research Laboratory, APG, MD, USA; University of Delaware, Newark, USA","2014 IEEE High Performance Extreme Computing Conference (HPEC)","12 Feb 2015","2014","","","1","6","An observation in supercomputing in the past decade illustrates the transition of pervasive commodity products being integrated with the world's fastest system. Given today's exploding popularity of mobile devices, we investigate the possibilities for high performance mobile computing. Because parallel processing on mobile devices will be the key element in developing a mobile and computationally powerful system, this study was designed to assess the computational capability of a GPU on a low-power, ARM-based mobile device. The methodology for executing computationally intensive benchmarks on a handheld mobile GPU is presented, including the practical aspects of working with the existing Android-based software stack and leveraging the OpenCL-based parallel programming model. The empirical results provide the performance of an OpenCL N-body benchmark and an auto-tuning kernel parameterization strategy. The achieved computational performance of the low-power mobile Adreno GPU is compared with a quad-core ARM, an ×86 Intel processor, and a discrete AMD GPU.","","978-1-4799-6233-4","10.1109/HPEC.2014.7040987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040987","handheld GPU;OpenCL;Android;N-body","Graphics processing units;Mobile communication;Androids;Humanoid robots;Performance evaluation;Benchmark testing;Computer architecture","Android (operating system);graphics processing units;mobile computing;parallel programming","Android mobile GPU;supercomputing;pervasive commodity product;mobile computing;parallel processing;mobile device;ARM;handheld mobile GPU;Android-based software stack;parallel programming;OpenCL N-body benchmark;autotuning kernel parameterization","","7","","25","","12 Feb 2015","","","IEEE","IEEE Conferences"
"Pro++: A Profiling Framework for Primitive-Based GPU Programming","N. Bombieri; F. Busato; F. Fummi","Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy","IEEE Transactions on Emerging Topics in Computing","2 Sep 2018","2018","6","3","382","394","Parallelizing software applications through the use of existing optimized primitives is a common trend that mediates the complexity of manual parallelization and the use of less efficient directivebased programming models. Parallel primitive libraries allow software engineers to map any sequential code to a target many-core architecture by identifying the most computational intensive code sections and mapping them into one or more existing primitives. On the other hand, the spreading of such a primitivebased programming model and the different graphic processing unit (GPU) architectures has led to a large and increasing number of third-party libraries, which often provide different implementations of the same primitive, each one optimized for a specific architecture. From the developer point of view, this moves the actual problem of parallelizing the software application to selecting, among the several implementations, the most efficient primitives for the target platform. This paper presents Pro++, a profiling framework for GPU primitives that allows measuring the implementation quality of a given primitive by considering the target architecture characteristics. The framework collects the information provided by a standard GPU profiler and combines them into optimization criteria. The criteria evaluations are weighed to distinguish the impact of each optimization on the overall quality of the primitive implementation. This paper shows how the tuning of the different weights has been conducted through the analysis of five of the most widespread existing primitive libraries and how the framework has been eventually applied to improve the implementation performance of two standard and widespread primitives.","2168-6750","","10.1109/TETC.2016.2546554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447754","GPUs;performance model;parallel applications","Graphics processing units;Computational modeling;Instruction sets;Computer architecture;Optimization;Measurement;Libraries","graphics processing units;parallel programming;program diagnostics;software architecture;software libraries","third-party libraries;software application;profiling framework;GPU primitives;optimization criteria;primitive-based GPU programming;parallel primitive libraries;computational intensive code sections;parallelization;directive-based programming models;many-core architecture;Pro++;GPU profiler;primitive-based programming model;graphic processing unit architectures","","","","40","IEEE","5 Apr 2016","","","IEEE","IEEE Journals"
"Atmospheric Model Cluster Performance Evaluation on Hybrid MPI/OpenMP/Cuda Programming Model Platform","C. Osthoff; R. P. Souto; P. L. S. Dias; J. Panetta; P. Lopes","LNCC (Nat. Lab. for Sci. Comput.), Petropolis, Brazil; LNCC (Nat. Lab. for Sci. Comput.), Petropolis, Brazil; LNCC (Nat. Lab. for Sci. Comput.), Petropolis, Brazil; Nat. Lab. for Space Res., INPE, Sao Jose dos Campos, Brazil; Nat. Lab. for Space Res., INPE, Sao Jose dos Campos, Brazil","2012 31st International Conference of the Chilean Computer Science Society","2 Jan 2014","2012","","","216","222","This work discuss the parallel performance of a global numerical simulation model, Ocean-Land-Atmosphere Model (OLAM), on a hybrid multicore/GPU cluster environment, under the following programming models: 1) OLAM MPI implementation, on the multicore system, 2) OLAM hybrid MPI/OpenMP, which starts one MPI process on each node of the platform and one OpenMP thread on each core of the node, 3) OLAM hybrid MPI/OpenMP/Cuda implementation, which starts one MPI process on each node of the platform, one OpenMP threads on each core of the node and Cuda kernels on the GPUs. The results shows that the adopted programming model impacts significantly the performance of the application. We show that as we increase the number of cores, the OLAM MPI parallel implementation running one process on each cluster core executes faster than the other implementations.","1522-4902","978-1-4799-2938-2","10.1109/SCCC.2012.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694092","Cluster;Multicore;GPU;Atmospheric Numerical Simulation Model;High Performance Computing","Atmospheric modeling;Graphics processing units;Computational modeling;Numerical models;Multicore processing;Kernel;Performance evaluation","atmospheric techniques;geophysics computing;graphics processing units;message passing;multiprocessing systems;multi-threading;parallel architectures","atmospheric model cluster performance evaluation;hybrid MPI/OpenMP/Cuda programming model platform;parallel performance;global numerical simulation model;ocean-land-atmosphere model;hybrid multicore/GPU cluster environment;programming models;multicore system;MPI process;OpenMP thread;Cuda kernels;OLAM MPI parallel implementation;cluster core","","1","","14","","2 Jan 2014","","","IEEE","IEEE Conferences"
"Performance and Power Efficiency Analysis of the Symmetric Cryptograph on Two Stream Processor Architectures","G. Xu; H. An; G. Liu; P. Yao; M. Xu; W. Han; X. Li; X. Hao","Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China","2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","17 Nov 2009","2009","","","917","920","Multimedia and some scientific applications have achieved good performance on the stream processor architecture by employing the stream programming model. In order to find out the way to accelerate the symmetric cryptograph on stream processor, we implement and analyze cryptograph algorithms on different stream processors in this paper. Four cipher algorithms including RC5, AES, TWOFISH and 3DES in ECB model are implemented on three platforms, which are stream processor SPI Storm SP16-G160, NVIDIA GeForce 9800GTX, Intel Core2 dual-core processor E7300. The difference of architecture between two stream processors and the character of programming model are described. When we compare throughput rate of these applications, 9800GTX is shown with 4-30x performance improvement over E7300, SP16 achieves the highest power efficiency and obtains 15-20x increase over E7300 in Gops/Watt.","","978-1-4244-4717-6","10.1109/IIH-MSP.2009.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337159","cryptograph;stream processor;GPU;stream programming model;accelerate;power efficiency","Performance analysis;Cryptography;Streaming media;Computer architecture;Acceleration;Signal processing algorithms;Storms;Multimedia systems;High performance computing;Hardware","computer architecture;cryptography;microcomputers;parallel processing;performance evaluation","performance analysis;power efficiency analysis;symmetric cryptograph;stream processor architecture;multimedia application;scientific application;stream programming model;cryptograph algorithms;cipher algorithms;RC5;AES;TWOFISH;3DES;SPI Storm SP16-G160;NVIDIA GeForce 9800GTX;Intel Core2 dual-core processor E7300","","1","","7","","17 Nov 2009","","","IEEE","IEEE Conferences"
"Dense linear algebra solvers for multicore with GPU accelerators","S. Tomov; R. Nath; H. Ltaief; J. Dongarra","Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA","2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)","24 May 2010","2010","","","1","8","Solving dense linear systems of equations is a fundamental problem in scientific computing. Numerical simulations involving complex systems represented in terms of unknown variables and relations between them often lead to linear systems of equations that must be solved as fast as possible. We describe current efforts toward the development of these critical solvers in the area of dense linear algebra (DLA) for multicore with GPU accelerators. We describe how to code/develop solvers to effectively use the high computing power available in these new and emerging hybrid architectures. The approach taken is based on hybridization techniques in the context of Cholesky, LU, and QR factorizations. We use a high-level parallel programming model and leverage existing software infrastructure, e.g. optimized BLAS for CPU and GPU, and LAPACK for sequential CPU processing. Included also are architecture and algorithm-specific optimizations for standard solvers as well as mixed-precision iterative refinement solvers. The new algorithms, depending on the hardware configuration and routine parameters, can lead to orders of magnitude acceleration when compared to the same algorithms on standard multicore architectures that do not contain GPU accelerators. The newly developed DLA solvers are integrated and freely available through the MAGMA library.","","978-1-4244-6534-7","10.1109/IPDPSW.2010.5470941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470941","Dense Linear Algebra Solvers;GPU Accelerators;Multicore;MAGMA;Hybrid Algorithms","Linear algebra;Multicore processing;Acceleration;Iterative algorithms;Linear accelerators;Linear systems;Equations;Computer architecture;Scientific computing;Numerical simulation","coprocessors;linear algebra;mathematics computing;matrix decomposition;multiprocessing systems;optimisation;parallel programming","dense linear algebra solvers;multicore systems;GPU accelerators;graphics processing unit;hybridization techniques;Cholesky factorization;LU factorization;QR factorization;parallel programming model;optimized BLAS software;LAPACK software;architecture-specific optimization;algorithm-specific optimization;MAGMA library","","117","2","14","","24 May 2010","","","IEEE","IEEE Conferences"
"An Efficient GPU Implementation for Large Scale Individual-Based Simulation of Collective Behavior","U. Erra; B. Frola; V. Scarano; I. Couzin","Dipt. di Mat. e Inf., Univ. della Basilicata, Italy; Dipt. di Mat. e Applicazioni, Univ. di Salerno, Salerno, Italy; Dipt. di Mat. e Applicazioni, Univ. di Salerno, Salerno, Italy; Dept. of Ecology & Evolutionary Biol., Princeton Univ., Princeton, NJ, USA","2009 International Workshop on High Performance Computational Systems Biology","30 Oct 2009","2009","","","51","58","In this work we describe a GPU implementation for an individual-based model for fish schooling. In this model each fish aligns its position and orientation with an appropriate average of its neighbors' positions and orientations. This carries a very high computational cost in the so-called nearest neighbors search. By leveraging the GPU processing power and the new programming model called CUDA we implement an efficient framework which permits to simulate the collective motion of high-density individual groups. In particular we present as a case study a simulation of motion of millions of fishes. We describe our implementation and present extensive experiments which demonstrate the effectiveness of our GPU implementation.","","978-0-7695-3809-9","10.1109/HiBi.2009.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298705","gpu;individual-based model;large scale simulation","Large-scale systems;Biological system modeling;Computational modeling;Marine animals;Educational institutions;Computational systems biology;Birds;Organisms;Graphics;Sorting","artificial life;C language;computer graphics;digital simulation;search problems","large scale individual-based simulation;collective behavior;fish schooling;computational cost;nearest neighbor search;GPU processing power;CUDA programming model;graphics processing unit;collective motion;fish motion simulation","","15","","25","","30 Oct 2009","","","IEEE","IEEE Conferences"
