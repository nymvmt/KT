"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"MR-Graph: A Customizable GPU MapReduce","Z. Qiao; S. Liang; H. Jiang; S. Fu","Dept. of Comput. Sci. & Eng., Univ. of North Texas, Denton, TX, USA; Dept. of Comput. Sci. & Eng., Univ. of North Texas, Denton, TX, USA; Dept. of Comput. Sci., Arkansas State Univ., AR, USA; NA","2015 IEEE 2nd International Conference on Cyber Security and Cloud Computing","7 Jan 2016","2015","","","417","422","The MapReduce programming model has been widely used in Big Data and Cloud applications. Criticism on its inflexibility when being applied to complicated scientific applications recently emerges. Several techniques have been proposed to enhance its flexibility. However, some of them exert special requirements on applications, while others fail to support the increasingly popular coprocessors, such as Graphics Processing Unit (GPU). In this paper, we propose MR-Graph, a customizable and unified framework for GPU-based MapReduce, which aims to improve the flexibility, scalability and performance of MapReduce. MR-Graph addresses the limitations and restrictions of the traditional MapReduce execution paradigm. The three execution modes integrated in MR-Graph facilitates users to write their applications in a more flexible fashion by defining a Map and Reduce function call graph. MR-Graph efficiently explores the memory hierarchy in GPUs to reduce the data transfer overhead between execution stages and accommodate big data applications. We have implemented a prototype of MR-Graph and experimental results show the effectiveness of using MR-Graph for flexible and scalable GPU-based MapReduce computing.","","978-1-4673-9300-3","10.1109/CSCloud.2015.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371516","MapReduce;GPU;Configurable;Flexible;Iterative;Recursive","Graphics processing units;Programming;Parallel processing;Computational modeling;Big data;Cloud computing;Data models","graphics processing units;parallel programming","MR-Graph framework;GPU MapReduce;MapReduce programming model;graphics processing unit;MapReduce execution paradigm;memory hierarchy;Big Data applications","","","","15","","7 Jan 2016","","","IEEE","IEEE Conferences"
"Performance Optimization of Top-k Queries on GPU","T. Luo; G. -Z. Sun; G. Chen","Key Lab. on High Performance Comput., Anhui Province Univ. of Sci. & Technol. of China, Hefei, China; Key Lab. on High Performance Comput., Anhui Province Univ. of Sci. & Technol. of China, Hefei, China; Key Lab. on High Performance Comput., Anhui Province Univ. of Sci. & Technol. of China, Hefei, China","2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming","12 Jan 2012","2011","","","9","13","With the development of web search engines, the concern on real-time performance of Top-k queries has attracted more and more attention. The author studies implement of classic algorithm - No Random Access Algorithm in order to optimize performance of Top-k queries on GPU. We give a novel GPU algorithm by using the features of CUDA's programming model. Experiment results show that an implementation of the algorithm on one GPU runs more than 7000 times faster than a single core implementation on a latest CPU.","2168-3042","978-1-4577-1808-3","10.1109/PAAP.2011.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128467","GPU;performance optimization;top-k queries","Graphics processing unit;Instruction sets;Upper bound;Programming;Algorithm design and analysis;Optimization;Vectors","graphics processing units;optimisation;parallel architectures;parallel programming;performance evaluation;query processing","performance optimization;top-k queries;GPU;Web search engines;classic algorithm;no random access algorithm;CUDA programming model","","1","","15","","12 Jan 2012","","","IEEE","IEEE Conferences"
"Towards an Effective Unified Programming Model for Many-Cores","A. L. Varbanescu; P. Hijma; R. van Nieuwpoort; H. Bal","Comput. Syst. Group, Vrije Univ. Amsterdam, Amsterdam, Netherlands; Comput. Syst. Group, Vrije Univ. Amsterdam, Amsterdam, Netherlands; Comput. Syst. Group, Vrije Univ. Amsterdam, Amsterdam, Netherlands; Comput. Syst. Group, Vrije Univ. Amsterdam, Amsterdam, Netherlands","2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum","1 Sep 2011","2011","","","681","692","Building an effective programming model for many-core processors is challenging. On the one hand, the increasing variety of platforms and their specific programming models force users to take a hardware-centric approach not only for implementing parallel applications, but also for designing them. This approach diminishes portability and, eventually, limits performance. On the other hand, to effectively cope with the increased number of large-scale workloads that require parallelization, a portable, application-centric programming model is desirable. Such a model enables programmers to focus first on extracting and exploiting parallelism from their applications, as opposed to generating parallelism for specific hardware, and only second on platform-specific implementation and optimizations. In this paper, we first present a survey of programming models designed for programming three families of many-cores: general purpose many-cores (GPMCs), graphics processing units (GPUs), and the Cell/B.E.. We analyze the usability of these models, their ability to improve platform programmability, and the specific features that contribute to this improvement. Next, we also discuss two types of generic models: parallelism-centric and application-centric. We also analyze their features and impact on platform programmability. Based on this analysis, we recommend two application-centric models (OmpSs and OpenCL) as promising candidates for a unified programming model for many-cores and we discuss potential enhancements for them.","1530-2075","978-1-61284-425-1","10.1109/IPDPS.2011.210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008837","","Programming;Parallel processing;Computer architecture;Data models;Concurrent computing;Computational modeling;Productivity","application program interfaces;computer graphic equipment;coprocessors;multiprocessing systems","unified programming model;many-core processors;application-centric programming model;parallelism generation;general purpose many-cores;GPMC;graphics processing units;GPU;Cell/B.E;parallelism-centric model;OmpS;OpenCL;multicore processors;many-core accelerators","","7","","26","","1 Sep 2011","","","IEEE","IEEE Conferences"
"Lit: A high performance massive data computing framework based on CPU/GPU cluster","Y. Zhai; E. Mbarushimana; W. Li; J. Zhang; Y. Guo","Beijing Engineering Research Center of Massive Language Information Processing and Cloud Computing Application, School of Computer Science, Beijing Institute of Technology, China 100081; Beijing Engineering Research Center of Massive Language Information Processing and Cloud Computing Application, School of Computer Science, Beijing Institute of Technology, China 100081; Science and Technology on Complex Systems Simulation Laboratory, Beijing, China; Science and Technology on Complex Systems Simulation Laboratory, Beijing, China; Beijing Engineering Research Center of Massive Language Information Processing and Cloud Computing Application, School of Computer Science, Beijing Institute of Technology, China 100081","2013 IEEE International Conference on Cluster Computing (CLUSTER)","9 Jan 2014","2013","","","1","8","Big data processing is receiving significant amount of interest as an important technology to reveal the information behind the data, such as trends, characteristics, etc. MapReduce is considered as the most efficient distributed parallel data processing framework. However, some high-end applications, especially some scientific analyses have both data-intensive and computation-intensive features. Current big data processing techniques like Hadoop are not designed for computation-intensive applications, thus have insufficient computation power. In this paper, we presented Lit, a high performance massive data computing framework based on CPU/GPU cluster. Lit integrated GPU with Hadoop to improve the computational power of each node in the cluster. Since the architecture and programming model of GPU is different from CPU, Lit provided an annotation based approach to automatically generate CUDA codes from Hadoop codes. Lit hided the complexity of programming on CPU/GPU cluster by providing extended compiler and optimizer. To utilize the simplified programming, scalability and fault tolerance benefits of Hadoop and combine them with the high performance computation power of GPU, Lit extended the Hadoop by applying a GPUClassloader to detect the GPU, generate and compile CUDA codes, and invoke the shared library. Our experimental results show that Lit can achieve an average speedup of 1x to 3x on three typical applications over Hadoop.","2168-9253","978-1-4799-0898-1","10.1109/CLUSTER.2013.6702614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702614","","Graphics processing units;Computational modeling;Handheld computers;Data models;Load modeling;Semantics","Big Data;fault tolerant computing;graphics processing units;parallel architectures;program compilers","Lit;high performance massive data computing framework;CPU cluster;GPU cluster;Big Data processing;MapReduce;distributed parallel data processing framework;data-intensive feature;computation-intensive feature;programming model;annotation based approach;CUDA code generation;Hadoop codes;extended compiler;extended optimizer;fault tolerance;high performance computation power;GPUClassloader;GPU detection;CUDA code compiling","","6","1","19","","9 Jan 2014","","","IEEE","IEEE Conferences"
"LOOG: Improving GPU Efficiency With Light-Weight Out-Of-Order Execution","K. Iliakis; S. Xydis; D. Soudris","National Technical University of Athens, Zografou, Greece; National Technical University of Athens, Zografou, Greece; National Technical University of Athens, Zografou, Greece","IEEE Computer Architecture Letters","10 Jan 2020","2019","18","2","166","169","GPUs are one of the most prevalent platforms for accelerating general-purpose workloads due to their intuitive programming model, computing capacity, and cost-effectiveness. GPUs rely on massive multi-threading and fast context switching to overlap computations with memory operations. Among the diverse GPU workloads, there exists a class of kernels that fail to maintain a sufficient number of active warps to hide the latency of memory operations, and thus suffer from frequent stalling. We observe that these kernels will benefit from increased levels of Instruction-Level Parallelism and we propose a novel architecture with lightweight Out-Of-Order execution capability. To minimize hardware overheads, we carefully design our extension to highly re-use the existing micro-architectural structures. We show that the proposed architecture outperforms traditional platforms by 15 to 46 percent on average for low occupancy kernels, with an area overhead of 0.74 to 3.94 percent. Finally, we prove the potential of our proposal as a GPU u-arch alternative, by providing a 5 percent speedup over a wide collection of 63 general-purpose kernels with as little as 0.74 percent area overhead.","1556-6064","","10.1109/LCA.2019.2951161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8890662","GPGPU;Out-of-Order execution;micro-architecture","Graphics processing units;Kernel;Registers;Radio access technologies;Copper;Radio frequency;Out of order","graphics processing units;multi-threading;parallel architectures;storage management","light-weight out-of-order execution;out-of-order execution capability;GPU u-arch;general-purpose kernels;area overhead;low occupancy kernels;hardware overheads;instruction-level parallelism;stalling;active warps;GPU workloads;memory operations;massive multithreading;cost-effectiveness;intuitive programming model;general-purpose workloads;GPU efficiency;LOOG","","","","18","IEEE","4 Nov 2019","","","IEEE","IEEE Journals"
"Template matching of aerial images using GPU","N. Nazneen; M. Shafiq; A. Hameed","KICSIT, Rawalpindi, Pakistan; CESAT, Islamabad, Pakistan; CESAT, Islamabad, Pakistan","2016 13th International Bhurban Conference on Applied Sciences and Technology (IBCAST)","10 Mar 2016","2016","","","206","212","During the last decade, processor architectures have emerged with hundreds and thousands of high speed processing cores in a single chip. These cores can work in parallel to share a work load for faster execution. This paper presents performance evaluations on such multicore and many-core devices by mapping a computationally expensive correlation kernel of a template matching process using various programming models. The work builds a base performance case by a sequential mapping of the algorithm on an Intel processor. In the second step, the performance of the algorithm is enhanced by parallel mapping of the kernel on a shared memory multicore machine using OpenMP programming model. Finally, the Normalized Cross-Correlation (NCC) kernel is scaled to map on a many-core K20 GPU using CUDA programming model. In all steps, the correctness of the implementation of algorithm is taken care by comparing computed data with reference results from a high level implementation in MATLAB. The performance results are presented with various optimization techniques for MATLAB, Sequential, OpenMP and CUDA based implementations. The results show that GPU based implementation achieves 32x and 5x speed-ups respectively to the base case and multicore implementations respectively. Moreover, using inter-block sub-sampling on an 8-bit 4000×4000 reference gray-scale image achieves the execution time upto 2.8sec with an error growth less than 20% for the selected templates of size 96×96.","2151-1411","978-1-4673-9127-6","10.1109/IBCAST.2016.7429878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429878","","Graphics processing units;MATLAB;Kernel;Computational modeling;Correlation;Instruction sets;Multicore processing","graphics processing units;image matching;parallel programming;shared memory systems","template matching;aerial image matching;graphics processing unit;multicore devices;many-core devices;correlation kernel mapping;sequential mapping;parallel mapping;shared memory multicore machine;OpenMP programming model;normalized cross-correlation kernel;NCC kernel;many-core K20 GPU;CUDA programming model;Compute Unified Device Architecture;Matlab;inter-block sub-sampling","","","","13","","10 Mar 2016","","","IEEE","IEEE Conferences"
"Sparse Matrix Formats Evaluation and Optimization on a GPU","M. R. Hugues; S. G. Petiton","TOTAL Exploration & Production, Pau, France; Lab. d'Inf. Fondamentale de Lille, CNRS/LIFL, Lille, France","2010 IEEE 12th International Conference on High Performance Computing and Communications (HPCC)","27 Sep 2010","2010","","","122","129","The data parallel programming model comes back with massive multicore architectures. The GPU is one of these and offers important possibilities to accelerate linear algebra. However, the irregular structure of sparse matrix operations generates problems with this programming model to obtain efficient performance. This depends on the used format to store values and the matrix structure. The sparse matrix-vector product (SpMV) is one of the most used kernel in scientific computing and is the main performance source of iterative methods. We propose an evaluation and optimization of several sparse formats for the SpMV kernel which have succeeded at the time of data parallel computer. This study is realized by analyzing the performances following the distribution of the non zeros values in the matrix to determine the best and the worst reachable value. The results show that all sparse formats converge to the same efficiency and perform poorly with a strong distribution of elements.","","978-1-4244-8335-8","10.1109/HPCC.2010.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581446","Sparse Format;SpMV;GPU;Many-Core;Data Parallel Programming","Arrays;Sparse matrices;Graphics processing unit;Indexes;Instruction sets;Artificial neural networks;Finite element methods","computer graphic equipment;coprocessors;iterative methods;multiprocessing systems;parallel programming;sparse matrices","sparse matrix format evaluation;GPU;sparse matrix format optimization;data parallel programming model;massive multicore;linear algebra;sparse matrix vector product;iterative method;SpMV kernel","","10","","14","","27 Sep 2010","","","IEEE","IEEE Conferences"
"Beyond the Socket: NUMA-Aware GPUs","U. Milic; O. Villa; E. Bolotin; A. Arunkumar; E. Ebrahimi; A. Jaleel; A. Ramirez; D. Nellans","Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC); NVIDIA; NVIDIA; Arizona State University; NVIDIA; NVIDIA; Google; NVIDIA","2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)","11 Apr 2019","2017","","","123","135","GPUs achieve high throughput and power efficiency by employing many small single instruction multiple thread (SIMT) cores. To minimize scheduling logic and performance variance they utilize a uniform memory system and leverage strong data parallelism exposed via the programming model. With Moore’s law slowing, for GPUs to continue scaling performance (which largely depends on SIMT core count) they are likely to embrace multi-socket designs where transistors are more readily available. However when moving to such designs, maintaining the illusion of a uniform memory system is increasingly difficult. In this work we investigate multi-socket non-uniform memory access (NUMA) GPU designs and show that significant changes are needed to both the GPU interconnect and cache architectures to achieve performance scalability. We show that application phase effects can be exploited allowing GPU sockets to dynamically optimize their individual interconnect and cache policies, minimizing the impact of NUMA effects. Our NUMA-aware GPU outperforms a single GPU by $1.5 \times, 2.3 \times$, and $3.2 \times$ while achieving 89%, 84%, and 76% of theoretical application scalability in 2, 4, and 8 sockets designs respectively. Implementable today, NUMA-aware multi-socket GPUs may be a promising candidate for scaling GPU performance beyond a single socket.CCS CONCEPTS• Computing methodologies → Graphics processors; • Computer systems organization → Single instruction, multiple data;","2379-3155","978-1-4503-4952-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8686671","Graphics Processing Units;Multi-socket GPUs;NUMA Systems","Graphics processing units;Bandwidth;Programming;Runtime;Sockets;Transistors;Throughput","cache storage;graphics processing units;integrated circuit design;multiprocessing systems;multi-threading;power aware computing","multisocket nonuniform memory access GPU designs;cache architectures;performance scalability;application phase effects;GPU sockets;individual interconnect;cache policies;NUMA effects;single GPU;scaling GPU performance;single socket;high throughput;power efficiency;single instruction multiple thread cores;scheduling logic;performance variance;uniform memory system;programming model;Moore's law slowing;scaling performance;SIMT core count;multisocket designs;NUMA-aware multisocket GPU;strong data parallelism","","","","63","","11 Apr 2019","","","IEEE","IEEE Conferences"
"A Branch-and-Bound algorithm using multiple GPU-based LP solvers","X. Meyer; B. Chopard; P. Albuquerque","Dept. of Computer Science, University of Geneva, Switzerland; Dept. of Computer Science, University of Geneva, Switzerland; Institute for Informatics & Telecommunications, University of Applied Sciences of Western Switzerland, Geneva, Switzerland","20th Annual International Conference on High Performance Computing","17 Apr 2014","2013","","","129","138","The Branch-and-Bound (B&B) method is a well-known optimization algorithm for solving integer linear programming (ILP) models in the field of operations research. It is part of software often employed by businesses for finding solutions to problems such as airline scheduling problems. It operates according to a divide-and-conquer principle by building a tree-like structure with nodes that represent linear programming (LP) problems. A LP solver commonly used to process the nodes is the simplex method. Nowadays its sequential implementation can be found in almost all commercial ILP solvers. In this paper, we present a hybrid CPU-GPU implementation of the B&B algorithm. The B&B tree is managed by the CPU, while the revised simplex method is mainly a GPU implementation, relying on the CUDA technology of NVIDIA. The CPU manages concurrently multiple instances of the LP solver. The principal difference with a sequential implementation of the B&B algorithm pertains to the LP solver, provided that the B&B tree is managed with the same strategy. We thus compared our GPU-based implementation of the revised simplex to a well-known open-source sequential solver, named CLP, of the COIN-OR project. For given problem densities, we measured a size threshhold beyond which our GPU implementation outperformed its sequential counterpart.","1094-7256","978-1-4799-0730-4","10.1109/HiPC.2013.6799105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799105","","Graphics processing units;Linear programming;Indexes;Mathematical model;Standards;Central Processing Unit;Equations","graphics processing units;integer programming;linear programming;public domain software;scheduling;travel industry;tree searching","branch-and-bound algorithm;multiple GPU-based LP solver;B&B method;optimization algorithm;integer linear programming model;ILP model;operations research;airline scheduling problem;divide-and-conquer principle;tree-like structure;LP problem;hybrid CPU-GPU implementation;B&B algorithm;B&B tree;simplex method;CUDA technology;NVIDIA;sequential implementation;GPU-based implementation;open-source sequential solver;CLP;COIN-OR project","","4","","33","","17 Apr 2014","","","IEEE","IEEE Conferences"
"Early evaluation of directive-based GPU programming models for productive exascale computing","S. Lee; J. S. Vetter","Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Georgia Inst. of Technol., Atlanta, GA, USA","SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","25 Feb 2013","2012","","","1","11","Graphics Processing Unit (GPU)-based parallel computer architectures have shown increased popularity as a building block for high performance computing, and possibly for future Exascale computing. However, their programming complexity remains as a major hurdle for their widespread adoption. To provide better abstractions for programming GPU architectures, researchers and vendors have proposed several directive-based GPU programming models. These directive-based models provide different levels of abstraction, and required different levels of programming effort to port and optimize applications. Understanding these differences among these new models provides valuable insights on their applicability and performance potential. In this paper, we evaluate existing directive-based models by porting thirteen application kernels from various scientific domains to use CUDA GPUs, which, in turn, allows us to identify important issues in the functionality, scalability, tunability, and debuggability of the existing models. Our evaluation shows that directive-based models can achieve reasonable performance, compared to hand-written GPU codes.","2167-4337","978-1-4673-0806-9","10.1109/SC.2012.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468490","","Graphics processing units;Programming;Computational modeling;Optimization;Data models;Computer architecture;Kernel","graphics processing units;parallel architectures;parallel programming;software performance evaluation","directive-based GPU programming model early evaluation;productive exascale computing;graphics processing unit-based parallel computer architectures;high performance computing;exascale computing;programming complexity;GPU architecture programming;abstraction levels;performance potential;programming effort levels;CUDA GPUs;hand-written GPU codes;program scalability;program functionality;program tunability;program debuggability","","40","","25","","25 Feb 2013","","","IEEE","IEEE Conferences"
"GAGM: Genome assembly on GPU using mate pairs","A. Jain; A. Garg; K. Paul","Dept. of Computer Science and Engineering, IIT Delhi, New Delhi, India; Dept. of Computer Science and Engineering, IIT Delhi, New Delhi, India; Dept. of Computer Science and Engineering, IIT Delhi, New Delhi, India","20th Annual International Conference on High Performance Computing","17 Apr 2014","2013","","","176","185","Genome fragment assembly has long been a time and computation intensive problem in the field of bioinformatics. Many parallel assemblers have been proposed to accelerate the process but there hasn't been any effective approach proposed for GPUs. Also with the increasing power of GPUs, applications from various research fields are being parallelized to take advantage of the massive number of “cores” available in GPUs. In this paper we present the design and development of a GPU based assembler (GAGM) for sequence assembly using Nvidia's GPUs with the CUDA programming model. Our assembler utilizes the mate pair reads produced by the current NGS technologies to build paired de Bruijn graph. Every paired read is broken into paired k-mers and l-mers. Every paired k-mer represents a vertex and paired l-mers are mapped as edges. Contigs are formed by grouping the regions of graph which can be unambiguously connected. We present parallel algorithms for k - mer extraction, paired de Bruijn graph construction and grouping of edges. We have benchmarked GAGM on four bacterial genomes. Our results show that the design on GPU is effective in terms of time as well as the quality of assembly produced.","1094-7256","978-1-4799-0730-4","10.1109/HiPC.2013.6799107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799107","parallel processing;bioinformatics;GPU;genome assembly","Graphics processing units;Genomics;Bioinformatics;Benchmark testing;DNA;Encoding","biocomputing;graph theory;graphics processing units;parallel algorithms;parallel architectures;program assemblers","GAGM;mate pairs;genome fragment assembly;bioinformatics;parallel assemblers;GPU based assembler;sequence assembly;CUDA programming model;NGS technologies;paired read;paired k-mers;paired l-mers;vertex;parallel algorithms;k-mer extraction;paired de Bruijn graph construction;edge grouping;bacterial genomes","","6","","31","","17 Apr 2014","","","IEEE","IEEE Conferences"
"Task Scheduling for GPU Heterogeneous Cluster","K. Zhang; B. Wu","Sch. of Comput. Sci., Fudan Univ., Shanghai, China; Sch. of Comput. Sci., Fudan Univ., Shanghai, China","2012 IEEE International Conference on Cluster Computing Workshops","20 Nov 2012","2012","","","161","169","Modern GPUs are gradually used by more and more cluster computing systems as the high performance computing units due to their outstanding computational power, whereas bringing node-level architectural heterogeneity to cluster. In this paper, based on MPI and CUDA programming model, we aim to investigate task scheduling for GPU heterogeneous cluster by taking into account the node-level heterogeneous characteristics. At first, based on our GPU heterogeneous cluster, we classify executing tasks to six major classifications according to their parallelism degrees, input data sizes, and processing workloads. Then, aiming to realize optimal mapping between tasks and computing resources, a task scheduling strategy is presented. The strategy consists of two key algorithms. The first is packing task algorithm (PTA) used to pack multiple tasks into a single task, such packing provides us a way of task classification converting according to the characteristic of computing resources. The second is system-level scheduling algorithm(SLSA) used to distribute parallel and sequential tasks to corresponding nodes, to maintain the load balance.","","978-0-7695-4844-9","10.1109/ClusterW.2012.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6355860","Task Scheduling;GPU Heterogeneous Cluster;PTA Algorithm;SLSA Algorithm","Graphics processing units;Kernel;Processor scheduling;Scheduling;Clustering algorithms;Computer architecture","application program interfaces;graphics processing units;message passing;parallel architectures;pattern classification;pattern clustering;resource allocation;scheduling","task scheduling strategy;GPU heterogeneous cluster;cluster computing systems;high performance computing units;node-level architectural heterogeneity;MPI;CUDA programming model;node-level heterogeneous characteristics;parallelism degrees;input data sizes;processing workloads;packing task algorithm;PTA;task classification;system-level scheduling algorithm;SLSA;parallel tasks;sequential tasks;load balance","","4","2","25","","20 Nov 2012","","","IEEE","IEEE Conferences"
"Parallelized computation for Edge Histogram Descriptor using CUDA on the Graphics Processing Units (GPU)","A. A. Mohammadabadi; A. Chalechale; H. Heidari","Department of Computer Engineering, Razi University, Kermanshah, Iran; Department of Computer Engineering, Razi University, Kermanshah, Iran; Department of Computer Engineering, Razi University, Kermanshah, Iran","The 17th CSI International Symposium on Computer Architecture & Digital Systems (CADS 2013)","20 Jan 2014","2013","","","9","14","Most image processing algorithms are inherently parallel, so multithreading processors are suitable in such applications. In huge image databases, image processing takes very long time for run on a single core processor because of single thread execution of algorithms. GPU is more common in most image processing applications due to multithread execution of algorithms, programmability and low cost. In this paper we show how to implement the MPRG-7 Edge Histogram Descriptor in parallel using CUDA programming model on a GPU. The Edge Histogram Descriptor describes the distribution of various types of edges with a histogram that can be a tool for image matching. This feature is applied to search images from a database which are similar to a query image. We evaluated the retrieval of the proposed technique using recall, precision, and average precision measures. Experimental results showed that parallel implementation led to an average speed up of 14.74×over the serial implementation. The average precision and the average recall of presented method are 67.02% and 55.00% respectively.","2325-937X","978-1-4799-0565-2","10.1109/CADS.2013.6714231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714231","content based image retrieval;CUDA;edge histogram descriptor;GPU","Image edge detection;Graphics processing units;Instruction sets;Feature extraction;Histograms;Image retrieval;Kernel","edge detection;graphics processing units;image matching;image retrieval;multi-threading;parallel architectures;visual databases","parallelized computation;MPRG-7 edge histogram descriptor;CUDA programming model;graphics processing unit;GPU;image processing algorithm;multithreading processors;image database;image matching;image searching;query image;recall measures;average precision measures","","","","11","","20 Jan 2014","","","IEEE","IEEE Conferences"
"Implementation and Performance Analysis of a Parallel Oil Reservoir Simulator Tool Using a CG Method on a GPU-Based System","L. Ismail; J. Abou-Kassem; B. Qamar","Comput. & Software Eng., UAE Univ., Al-Ain, United Arab Emirates; Dept. of Chem. & Pet. Eng., UAE Univ., Al-Ain, United Arab Emirates; HPGCL Res. Lab., UAE Univ., Al-Ain, United Arab Emirates","2014 UKSim-AMSS 16th International Conference on Computer Modelling and Simulation","23 Feb 2015","2014","","","375","380","An oil reservoir simulator is a crucial tool used by petroleum engineering to analyze reservoir conditions. To increase its performance, we implement a parallel version of the tool on a Graphic Processing Unit (GPU), using Computer Unified Device Architecture (CUDA) programming model and the Single Instruction Multiple Threads (SIMT). This paper presents our parallel implementation and performance analysis for 1-D, 2-D, and 3-D oil-phase reservoirs. The implementation and the performance evaluation reveal the gains and the losses achieved by the parallelization of a reservoir simulator on a Graphics Processing Unite (GPU) system. The performance results show that despite the interdependency between the different computational parts of the Conjugate Gradient (CG) method used as a linear solver in the parallel reservoirs, a speedup of 26 can be easily obtained for an oil reservoir simulator using 15 streaming multiprocessors (SMs), compared to a sequential CPU execution. The parallel execution scales well with grid dimensionality.","","978-1-4799-4922-9","10.1109/UKSim.2014.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046095","Oil Reservoir Simulator; Conjugate Gradient (CG) Method; GPU; High Performance Computing","Reservoirs;Graphics processing units;Sparse matrices;Vectors;Instruction sets;Memory management;Clustering algorithms","conjugate gradient methods;graphics processing units;hydrocarbon reservoirs;multiprocessing systems;parallel programming","parallel oil reservoir simulator tool;CG method;conjugate gradient method;GPU based system;graphics processing unit;computer unified device architecture;CUDA programming model;single instruction multiple threads;SIMT;linear solver;streaming multiprocessors;sequential CPU execution;grid dimensionality","","1","","40","","23 Feb 2015","","","IEEE","IEEE Conferences"
"GPU accelerate parallel Odd-Even merge sort: An OpenCL method","K. Zhang; J. Li; G. Chen; B. Wu","School of Computer Science, Fudan University, Shanghai 201203, China; School of Computer Science, Fudan University, Shanghai 201203, China; School of Computer Science, Fudan University, Shanghai 201203, China; School of Computer Science, Fudan University, Shanghai 201203, China","Proceedings of the 2011 15th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","21 Jul 2011","2011","","","76","83","Odd-Even merge sort is a basic problem in computer supported cooperative work in design area. However, it is not effective because of the high complexity O(nlg<sup>2</sup>n) in CPU platform. In this paper, we present a novel implementation based on the OpenCL programming model on recent GPU (Graphic Processing Unit). Our implementation was based on Knuth's algorithm and do some change. Due to limitations of OpenCL, we utilize a flag variable to make it avoid the direct backward control flow. As results, our implementation achieves 18× speedups compared with the CPU C++ STL quick sort. And it gets almost linear speedup for next generations of GPU because of the complete parallelism in each iteration process. Meanwhile, our approach makes the odd-even merge sort effectively in practice because of the high performance. Furthermore, the approach used in this paper for cooperating thousands of processing units to parallel process can also be used in other cooperation areas.","","978-1-4577-0387-4","10.1109/CSCWD.2011.5960058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960058","GPGPU;GPU;Odd-Even Merge Sort;OpenCL","Graphics processing unit;Sorting;Arrays;Computational modeling;Complexity theory;Instruction sets","computer graphic equipment;coprocessors;iterative methods","GPU accelerate parallel odd even merge sort;computer supported cooperative work;OpenCL programming model;graphic processing unit;Knuth algorithm;direct backward control flow;CPU C++ STL quick sort;parallel process;processing units;iteration process","","5","2","35","","21 Jul 2011","","","IEEE","IEEE Conferences"
"GPU Acceleration of Clustered DPCM for Lossless Compression of Hyperspectral Images","J. Li; J. Wu; G. Jeon","School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China","IEEE Transactions on Industrial Informatics","14 Feb 2020","2020","16","5","2906","2916","With the development of remote sensing technology, spatial and spectral resolutions of hyperspectral images have become increasingly dense. In order to overcome difficulties in the storage, transmission, and manipulation of hyperspectral images, an effective compression algorithm is requisite. The clustered differential pulse code modulation (C-DPCM), which is a prediction-based hyperspectral image lossless compression algorithm, can achieve a relatively high compression ratio, but its efficiency still requires improvement. This paper presents a parallel implementation of the C-DPCM algorithm on graphics processing units (GPUs) with the compute unified device architecture, which is a parallel computing platform and programming model developed by NVIDIA. Three optimization strategies are utilized to implement the C-DPCM algorithm in parallel, including a version that uses shared memory and registers, a version that employs multistream, and a version that uses multi-GPU. In addition, we studied how to assign all classes to each GPU to minimize the processing time. Finally, we reduced the compression time from approximately half an hour to an hour to several seconds, with almost no loss in accuracy.","1941-0050","","10.1109/TII.2019.2893437","National Natural Science Foundation of China(grant numbers:61775175,61771378); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613877","Clustered differential pulse code modulation (C-DPCM);compute unified device architecture (CUDA);graphics processing unit (GPU);hyperspectral image lossless compression","Image coding;Hyperspectral imaging;Prediction algorithms;Graphics processing units;Correlation;Bit rate","data compression;differential pulse code modulation;geophysical image processing;graphics processing units;image coding;parallel architectures;remote sensing","hyperspectral images;remote sensing technology;spatial resolutions;spectral resolutions;effective compression algorithm;clustered differential pulse code modulation;prediction-based hyperspectral image lossless compression;compression ratio;parallel implementation;C-DPCM algorithm;compute unified device architecture;parallel computing platform;programming model;compression time;GPU acceleration;clustered DPCM","","4","","30","IEEE","16 Jan 2019","","","IEEE","IEEE Journals"
"A parallel design of computer Go engine on CUDA-enabled GPU","Q. Zhang; Z. Liu","School of Software Beijing University of Posts and Telecommunications, Beijing, China; School of Software Beijing University of Posts and Telecommunications, Beijing, China","2011 IEEE International Conference on Cloud Computing and Intelligence Systems","13 Oct 2011","2011","","","85","88","With the rapid growth of Graphics Processing Unit (GPU) processing capability, using GPU as a coprocessor to assist the CPU in parallel computing has become indispensable. CUDA (Compute Unified Device Architecture) programming model also gives C/C++ language support which makes programming easily. This paper details how to design an engine of computer Go with Monte-Carlo algorithm which is based on GPU with Fermi architecture. We analyze the characteristics of Monte-Carlo algorithm, combined with the CUDA architecture features, divide the algorithm into various sub-modules for GPU computing fast and easily.","2376-595X","978-1-61284-204-2","10.1109/CCIS.2011.6045037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045037","Parallelization;CUDA;Monte-Carlo","Graphics processing unit;Games;Monte Carlo methods;Computers;Instruction sets;Engines;Computational modeling","C++ language;computer graphic equipment;coprocessors;Monte Carlo methods;parallel processing","parallel design;computer Go engine;CUDA enabled GPU;graphics processing unit;parallel computing;compute unified device architecture;C++ language support;C language support;Monte-Carlo algorithm;Fermi architecture","","","","7","","13 Oct 2011","","","IEEE","IEEE Conferences"
"Advanced genetic algorithm to solve MINLP problems over GPU","A. Munawar; M. Wahib; M. Munetomo; K. Akama","Graduate School of Information, Science and Technology, Hokkaido University, Sapporo, Japan; Graduate School of Information, Science and Technology, Hokkaido University, Sapporo, Japan; Information Systems Design Laboratory, Information Initiative Center, Hokkaido University, Sapporo, Japan; Information Systems Design Laboratory, Information Initiative Center, Hokkaido University, Sapporo, Japan","2011 IEEE Congress of Evolutionary Computation (CEC)","14 Jul 2011","2011","","","318","325","In this paper we propose a many-core implementation of evolutionary computation for GPGPU (General-Purpose Graphic Processing Unit) to solve non-convex Mixed Integer Non-Linear Programming (MINLP) and non-convex Non Linear Programming (NLP) problems using a stochastic algorithm. Stochastic algorithms being random in their behavior are difficult to implement over GPU like architectures. In this paper we not only succeed in implementation of a stochastic algorithm over GPU but show considerable speedups over CPU implementations. The stochastic algorithm considered for this paper is an adaptive resolution approach to genetic algorithm (arGA), developed by the authors of this paper. The technique uses the entropy measure of each variable to adjust the intensity of the genetic search around promising individuals. Performance is further improved by hybridization with adaptive resolution local search (arLS) operator. In this paper, we describe the challenges and design choices involved in parallelization of this algorithm to solve complex MINLPs over a commodity GPU using Compute Unified Device Architecture (CUDA) programming model. Results section shows several numerical tests and performance measurements obtained by running the algorithm over an nVidia Fermi GPU. We show that for difficult problems we can obtain a speedup of up to 20x with double precision and up to 42x with single precision.","1941-0026","978-1-4244-7835-4","10.1109/CEC.2011.5949635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949635","Adaptive Resolution Genetic Algorithm;Parallel Genetic Algorithms;General-Purpose computation on Graphics Processing Units (GPGPU);Compute Unified Device Architecture (CUDA)","Graphics processing unit;Genetic algorithms;Kernel;Stochastic processes;Entropy;Algorithm design and analysis;Genetics","computer graphic equipment;coprocessors;genetic algorithms;integer programming;nonlinear programming;parallel algorithms;parallel architectures;stochastic processes","advanced genetic algorithm;MINLP problems;evolutionary computation;GPGPU;general-purpose graphic processing unit;many-core implementation;nonconvex mixed integer nonlinear programming;stochastic algorithm;GPU like architectures;CPU implementations;adaptive resolution local search operator;parallelization;compute unified device architecture programming model;nVidia Fermi GPU","","10","","28","","14 Jul 2011","","","IEEE","IEEE Conferences"
"Hybrid CPU and GPU Computation to Detect Lung Nodule in Computed Tomography Images","I. W. B. Sentana; N. Jawas; A. E. Wardani","Department of Information, Systems Bali State Polytechnic, J1. Raya Bukit Jimbaran, Badung, Bali, Indonesia; Department of Computer, System STMIK STIKOM, Bali Jalan Raya Puputan No. 86, Renon, Denpasar, Indonesia; Radiology Unit, Airlangga university, Hospital Jalan Mayjen Dr. Moestopo No. 6–8, Surabaya, Indonesia","2018 Third International Conference on Informatics and Computing (ICIC)","1 Aug 2019","2018","","","1","6","Lung Nodule is a white patch on the thorax medical image, usually used as an early marker of lung cancer. Although there were some research deals with lung nodule detection, but none of those researches tailoring Graphical Processing Unit (GPU) to assist the computing process. This research aims to produce algorithms that can detect lung nodules automatically in CT images, by utilizing a combination of hybrid computing between Central Processing Unit (CPU) and Graphical Processing Unit. The framework used is Compute Unified Device Architecture, which consists of platform and programming model. The algorithm consists of several steps: read dicom and data normalization, lung segmentation, candidate nodule extraction, and classification. Normalization is required to facilitate calculation by changing the data type ui16 to ui8. Furthermore, segmentation is used to separate the lung parts with other organs, where at this stage the Otsu Algorithm and Moore Neighborhood Tracing (MNT) are used. The next step is Lung Nodule Extraction, which aims to find the nodule candidate. The last step is a classification that utilizes the Support Vector Machine (SVM) to distinguish which one is nodule or not. The algorithm successfully detects near round nodules that are free-standing or not attached to other parts of organs. After undergoing ground truth tests, it was found that under some conditions, the algorithm has not been able to distinguish nodules and other strokes that resemble nodules. While in terms of computing speed is found a very surprising result because overall single CPU computing provides better results compared to hybrid CPU and GPU computing.","","978-1-5386-6921-1","10.1109/IAC.2018.8780573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8780573","Lung Nodule;Hybrid Computing;GPU and CPU;CT images","Lung;Graphics processing units;Support vector machines;Computed tomography;Central Processing Unit;Kernel;Classification algorithms","cancer;computerised tomography;image segmentation;lung;medical image processing;support vector machines","compute unified device architecture;Otsu algorithm;graphical processing unit;central processing unit;hybrid CPU computing;lung nodule extraction;hybrid computing;CT images;lung nodules;computing process;lung nodule detection;lung cancer;thorax medical image;computed tomography;GPU computation;GPU computing;round nodules;lung segmentation;data normalization","","1","","18","","1 Aug 2019","","","IEEE","IEEE Conferences"
"GPU-Accelerated High-Throughput Online Stream Data Processing","Z. Chen; J. Xu; J. Tang; K. A. Kwiat; C. A. Kamhoua; C. Wang","Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; US Air Force Research Lab (AFRL), Rome, NY; US Air Force Research Lab (AFRL), Rome, NY; InterDigital, Inc, King of Prussia, PA","IEEE Transactions on Big Data","1 Jun 2018","2018","4","2","191","202","The Single Instruction Multiple Data (SIMD) architecture of Graphic Processing Units (GPUs) makes them perfect for parallel processing of big data. In this paper, we present the design, implementation and evaluation of G-Storm, a GPU-enabled parallel system based on Storm, which harnesses the massively parallel computing power of GPUs for high-throughput online stream data processing. G-Storm has the following desirable features: 1) G-Storm is designed to be a general data processing platform as Storm, which can handle various applications and data types. 2) G-Storm exposes GPUs to Storm applications while preserving its easy-to-use programming model. 3) G-Storm achieves high-throughput and low-overhead data processing with GPUs. 4) G-Storm accelerates data processing further by enabling Direct Data Transfer (DDT), between two executors that process data at a common GPU. We implemented G-Storm based on Storm 0.9.2 and tested it using three different applications, including continuous query, matrix multiplication and image resizing. Extensive experimental results show that 1) Compared to Storm, G-Storm achieves over 7χ improvement on throughput for continuous query, while maintaining reasonable average tuple processing time. It also leads to 2.3χ and 1.3χ throughput improvements on the other two applications, respectively. 2) DDT significantly reduces data processing time.","2332-7790","","10.1109/TBDATA.2016.2616116","Air Force Office of Scientific Research(grant numbers:FA9550-16-1-0077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7587437","Stream data processing;GPU;parallel computing;big data infrastructure","Graphics processing units;Storms;Fasteners;Programming;Computer architecture;Big data","Big Data;graphics processing units;matrix multiplication;parallel architectures;query processing","low-overhead data processing;parallel processing;big data;image resizing;matrix multiplication;continuous query;easy-to-use programming;GPU-enabled parallel system;G-Storm;storm applications;direct data transfer;tuple processing time;online stream data processing;GPU-accelerated high-throughput;parallel computing;graphic processing units;single instruction multiple data architecture","","7","","37","IEEE","10 Oct 2016","","","IEEE","IEEE Journals"
"Poster: GPU Accelerated Ultrasonic Tomography Using Propagation and Backpropagation Method","P. D. Bello; Y. Jin; E. Lu",NA; NA; NA,"2012 SC Companion: High Performance Computing, Networking Storage and Analysis","11 Apr 2013","2012","","","1447","1447","This paper develops implementation strategy and method to accelerate the propagation and backpropagation (PBP) tomographic imaging algorithm using Graphic Processing Units (GPUs). The Compute Unified Device Architecture (CUDA) programming model is used to develop our parallelized algorithm since the CUDA model allows the user to interact with the GPU resources more efficiently than traditional shader methods. The results show an improvement of more than 80x when compared to the C/C++ version of the algorithm, and 515x when compared to the MATLAB version while achieving high quality imaging for both cases. We test different CUDA kernel configurations in order to measure changes in the processing-time of our algorithm. By examining the acceleration rate and the image quality, we develop an optimal kernel configuration that maximizes the throughput of CUDA implementation for the PBP method.","","978-0-7695-4956-9","10.1109/SC.Companion.2012.249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496032","Medical Imaging;Ultrasonic Tomography;GPU;CUDA;Parallel Computing","","","","","","","","","11 Apr 2013","","","IEEE","IEEE Conferences"
"A Compute Unified System Architecture for Graphics Clusters Incorporating Data Locality","C. Muller; S. Frey; M. Strengert; C. Dachsbacher; T. Ertl",Visualisierungsinstitut der Universität Stuttgart; Visualisierungsinstitut der Universität Stuttgart; Visualisierungsinstitut der Universität Stuttgart; Visualisierungsinstitut der Universität Stuttgart; Visualisierungsinstitut der Universität Stuttgart,"IEEE Transactions on Visualization and Computer Graphics","12 May 2009","2009","15","4","605","617","We present a development environment for distributed GPU computing targeted for multi-GPU systems, as well as graphics clusters. Our system is based on CUDA and logically extends its parallel programming model for graphics processors to higher levels of parallelism, namely, the PCI bus and network interconnects. While the extended API mimics the full function set of current graphics hardware-including the concept of global memory-on all distribution layers, the underlying communication mechanisms are handled transparently for the application developer. To allow for high scalability, in particular for network-interconnected environments, we introduce an automatic GPU-accelerated scheduling mechanism that is aware of data locality. This way, the overall amount of transmitted data can be heavily reduced, which leads to better GPU utilization and faster execution. We evaluate the performance and scalability of our system for bus and especially network-level parallelism on typical multi-GPU systems and graphics clusters.","1941-0506","","10.1109/TVCG.2008.188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653488","GPU computing;graphics clusters;parallel programming.;Distributed/network graphics;Graphics Systems;Computer Graphics;Computing Methodologies;Concurrent Programming;Programming Techniques;Software/Software Engineering;Concurrent;distributed;and parallel languages;Language Classifications;Programming Languages Software;Graphics processors;Hardware Architecture;Computing Methodologies","Computer architecture;Graphics;Parallel processing;Concurrent computing;Distributed computing;Hardware;Parallel programming;Scalability;Power generation;Rendering (computer graphics)","computer graphics;coprocessors;parallel programming","compute unified system architecture;graphics clusters;data locality;distributed GPU computing;multi-GPU systems;parallel programming model;graphics processors;PCI bus;network interconnects;graphics hardware;communication mechanism;network-interconnected environment;automatic GPU-accelerated scheduling;GPU utilization;network-level parallelism","","17","","23","","17 Oct 2008","","","IEEE","IEEE Journals"
"Exploring Compiler Optimization Opportunities for the OpenMP 4.× Accelerator Model on a POWER8+GPU Platform","A. Hayashi; J. Shirako; E. Tiotto; R. Ho; V. Sarkar",NA; NA; NA; NA; NA,"2016 Third Workshop on Accelerator Programming Using Directives (WACCPD)","2 Feb 2017","2016","","","68","78","While GPUs are increasingly popular for high-performance computing, optimizing the performance of GPU programs is a time-consuming and non-trivial process in general. This complexity stems from the low abstraction level of standard GPU programming models such as CUDA and OpenCL: programmers are required to orchestrate low-level operations in order to exploit the full capability of GPUs. In terms of software productivity and portability, a more attractive approach would be to facilitate GPU programming by providing high-level abstractions for expressing parallel algorithms.OpenMP is a directive-based shared memory parallel programming model and has been widely used for many years. From OpenMP 4.0 onwards, GPU platforms are supported by extending OpenMP's high-level parallel abstractions with accelerator programming. This extension allows programmers to write GPU programs in standard C/C++ or Fortran languages, without exposing too many details of GPU architectures.However, such high-level parallel programming strategies generally impose additional program optimizations on compilers, which could result in lower performance than fully hand-tuned code with low-level programming models. To study potential performance improvements by compiling and optimizing high-level GPU programs, in this paper, we 1) evaluate a set of OpenMP 4.× benchmarks on an IBM POWER8 and NVIDIA Tesla GPU platform and 2) conduct a comparable performance analysis among hand-written CUDA and automatically-generated GPU programs by the IBM XL and clang/LLVM compilers.","","978-1-5090-6152-5","10.1109/WACCPD.2016.011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836582","Parallel programming","Graphics processing units;Optimization;Kernel;Instruction sets;Programming;Performance evaluation","C++ language;FORTRAN;graphics processing units;optimisation;parallel algorithms;parallel architectures;parallel programming;program compilers;shared memory systems;software portability","clang/LLVM compilers;IBM XL compilers;NVIDIA Tesla GPU platform;IBM POWER8 platform;FORTRAN languages;C/C++ languages;accelerator programming;directive-based shared memory parallel programming;parallel algorithms;high-level abstractions;software portability;software productivity;OpenCL;CUDA;GPU programming models;high-performance computing;POWER8+GPU platform;OpenMP 4.x accelerator model;compiler optimization","","4","1","26","","2 Feb 2017","","","IEEE","IEEE Conferences"
"GPu-based framework for interactive visualization of SAR data","M. Lambers; A. Kolb; H. Nies; M. Kalkuhl","Institute for Vision and Graphics, University of Siegen, Germany; Institute for Vision and Graphics, University of Siegen, Germany; Center for Sensorsystems (ZESS), University of Siegen, Germany; Department of Simulation, University of Siegen, Germany","2007 IEEE International Geoscience and Remote Sensing Symposium","7 Jan 2008","2007","","","4076","4079","Synthetic aperture radar data presents specific problems for interactive visualization. The high amount of multiplicative speckle noise has to be reduced. The high dynamic range of the amplitude data must be mapped to the lower dynamic range of display devices in a way that makes image features appropriately visible. In addition to interactive navigation in the data, it is desirable to allow interactive selection of despeckling and dynamic range reduction methods and adjustment of their parameters. Graphics processing units (GPUs) can be seen as ubiquitous parallel coprocessors with extreme computational power. In this paper, we propose a GPU-based framework for interactive visualization of SAR data. Data management techniques are used to make full use of the GPU. We reworked well-known despeckling and dynamic range reduction techniques for the GPU programming model and implemented them in our framework. Both navigation in large data sets and adjustment of processing parameters are fully interactive.","2153-7003","978-1-4244-1211-2","10.1109/IGARSS.2007.4423745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4423745","","Data visualization;Dynamic range;Navigation;Speckle;Noise reduction;Displays;Graphics;Coprocessors;Concurrent computing;Pervasive computing","coprocessors;data visualisation;geophysical signal processing;image denoising;interactive systems;radar signal processing;speckle;synthetic aperture radar","GPU based framework;interactive SAR data visualization;synthetic aperture radar;multiplicative speckle noise reduction;interactive despeckling selection;dynamic range reduction;graphics processing unit;parallel coprocessors;data management techniques;GPU programming model;large dataset navigation;processing parameter adjustment","","4","1","8","","7 Jan 2008","","","IEEE","IEEE Conferences"
"A Dynamically Balanced OpenMP-CUDA Implementation of PDE-Based Contrast Source Inversion for Microwave Imaging","N. Geddert; I. Jeffrey","Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Canada; Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, Canada","2018 18th International Symposium on Antenna Technology and Applied Electromagnetics (ANTEM)","13 Dec 2018","2018","","","1","2","An implementation of a PDE-based Contrast Source Inversion (CSI) algorithm using a hybrid Open MP-CUDA parallel programming model is presented for the acceleration of microwave imaging. The CSI algorithm uses a time-harmonic discontinuous Galerkin method forward solver. The programming model ensures high computational throughput by dynamically balancing the amount of work performed on the GPU and CPU. The resulting implementation is capable of substantial speed-up, bringing the computational performance to near real-time. Implementation and optimization details are discussed; results show GPU acceleration alone gives a speedup of at least 5 times.","2473-3555","978-1-5386-1338-2","10.1109/ANTEM.2018.8572994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8572994","Graphics processing unit (GPU);Open MP;hybrid parallel programming;contrast source inversion;microwave imaging","Graphics processing units;Acceleration;Programming;Computational modeling;Magnetic resonance imaging;Magnetic domains","Galerkin method;graphics processing units;Maxwell equations;microwave imaging;parallel architectures;parallel programming","microwave imaging;CSI algorithm;time-harmonic discontinuous Galerkin method;high computational throughput;computational performance;GPU acceleration;dynamically balanced OpenMP-CUDA implementation;PDE-based Contrast Source Inversion algorithm;hybrid Open MP-CUDA parallel programming model","","1","","9","","13 Dec 2018","","","IEEE","IEEE Conferences"
"Using graphics devices in reverse: GPU-based Image Processing and Computer Vision","J. Fung; S. Mann","NVIDIA Corporation, 2701 San Tomas Expressway, Santa Clara, California, USA; University of Toronto, Dept. of Electrical and Computer Engineering, 10 King's College Road, Mailstop. B540, Ontario, Canada","2008 IEEE International Conference on Multimedia and Expo","26 Aug 2008","2008","","","9","12","Graphics and vision are approximate inverses of each other: ordinarily graphics processing units (GPUs) are used to convert ldquonumbers into picturesrdquo (i.e. computer graphics). In this paper, we discuss the use of GPUs in approximately the reverse way: to assist in ldquoconverting pictures into numbersrdquo (i.e. computer vision). For graphical operations, GPUs currently provide many hundreds of gigaflops of processing power. This paper discusses how this processing power is being harnessed for image processing and computer vision, thereby providing dramatic speedups on commodity, readily available graphics hardware. A brief review of algorithms mapped to the GPU by using the graphics API for vision is presented. The NVIDIA CUDA programming model is then introduced as a way of expressing program parallelism without the need for graphics expertise.","1945-788X","978-1-4244-2570-9","10.1109/ICME.2008.4607358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4607358","GPU;Graphics Processing Unit;Computer Vision;Image Processing","Graphics;Hardware;Computer architecture;Computer vision;Programming;Acceleration;Pattern recognition","application program interfaces;computer graphics;computer vision;coprocessors;image processing equipment","computer vision;graphics processing units;computer graphics;graphics API;NVIDIA CUDA programming model;program parallelism;GPU-based image processing","","49","","32","","26 Aug 2008","","","IEEE","IEEE Conferences"
"Parallel AMG solver for three dimensional unstructured grids using GPU","K. R. Tej; N. Sivadasan; V. Sharma; R. Banerjee","Dept. of Computer Science and Engineering, Indian Institute of Technology Hyderabad, Hyderabad, India; Dept. of Computer Science and Engineering, Indian Institute of Technology Hyderabad, Hyderabad, India; Dept. of Mechanical Engineering, Indian Institute of Technology Hyderabad, Hyderabad, India; Dept. of Mechanical Engineering, Indian Institute of Technology Hyderabad, Hyderabad, India","2014 21st International Conference on High Performance Computing (HiPC)","4 Jun 2015","2014","","","1","10","Graphics Processing Units (GPUs) have evolved over the years from being graphics accelerator to scalable coprocessor. We implement an algebraic multigrid solver for three dimensional unstructured grids using GPU. Such a solver has extensive applications in Computational Fluid Dynamics (CFD). Using a combination of vertex coloring, optimized memory representations, multi-grid and improved coarsening techniques, we obtain considerable speedup in our parallel implementation. Our solver provides significant acceleration for solving pressure Poisson equations, which is the most time consuming part while solving Navier-Stokes equations. In our experimental study, we solve pressure Poisson equations for flow over lid driven cavity and for laminar flow past square cylinder. Our implementation achieves 915 times speed up for the lid driven cavity problem on a grid of size 2.6 million and a speed up of 1020 times for the laminar flow past square cylinder problem on a grid of size 1.7 million, compared to serial non-multigrid implementations. For our implementation, we used NVIDIA's CUDA programming model.","1094-7256","978-1-4799-5976-1","10.1109/HiPC.2014.7116899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116899","GPU Computing;Computational Fluid Dynamics;Multigrid Flow Solver;Gauss-Seidel;Navier-Stokes","Graphics processing units;Image color analysis;Instruction sets;Smoothing methods;Kernel;Mathematical model;Computational fluid dynamics","computational fluid dynamics;external flows;graphics processing units;laminar flow;Navier-Stokes equations;parallel architectures;parallel programming;Poisson equation","parallel AMG solver;three-dimensional unstructured grids;GPU;graphics processing units;graphics accelerator;scalable co-processor;algebraic multigrid solver;computational fluid dynamics;CFD;vertex coloring;optimized memory representations;improved coarsening techniques;parallel implementation;pressure Poisson equations;Navier-Stokes equations;laminar flow;lid driven cavity problem;square cylinder problem;NVIDIA CUDA programming model","","1","","37","","4 Jun 2015","","","IEEE","IEEE Conferences"
"Evaluation of GPU Architectures Using Spiking Neural Networks","V. K. Pallipuram; M. A. Bhuiyan; M. C. Smith","Dept. of Electr. & Comput. Eng., Clemson Univ., Clemson, SC, USA; Dept. of Electr. & Comput. Eng., Clemson Univ., Clemson, SC, USA; Dept. of Electr. & Comput. Eng., Clemson Univ., Clemson, SC, USA","2011 Symposium on Application Accelerators in High-Performance Computing","29 Sep 2011","2011","","","93","102","During recent years General-Purpose Graphical Processing Units (GP-GPUs) have entered the field of High-Performance Computing (HPC) as one of the primary architectural focuses for many research groups working with complex scientific applications. Nvidia's Tesla C2050, codenamed Fermi, and AMD's Radeon 5870 are two devices positioned to meet the computationally demanding needs of supercomputing research groups across the globe. Though Nvidia GPUs powered by CUDA have been the frequent choices of the performance centric research groups, the introduction and growth of OpenCL has promoted AMD GP-GPUs as potential accelerator candidates that can challenge Nvidia's stronghold. These architectures not only offer a plethora of features for application developers to explore, but their radically different architectures calls for a detailed study that weighs their merits and evaluates their potential to accelerate complex scientific applications. In this paper, we present our performance analysis research comparing Nvidia's Fermi and AMD's Radeon 5870 using OpenCL as the common programming model. We have chosen four different neuron models for Spiking Neural Networks (SNNs), each with different communication and computation requirements, namely the Izhikevich, Wilson, Morris Lecar (ML), and the Hodgkin Huxley (HH) models. We compare the runtime performance of the Fermi and Radeon GPUs with an implementation that exhausts all optimization techniques available with OpenCL. Several equivalent architectural parameters of the two GPUs are studied and correlated with the application performance. In addition to the comparative study effort, our implementations were able to achieve a speed-up of 857.3x and 658.51x on the Fermi and Radeon architectures respectively for the most compute intensive HH model with a dense network containing 9.72 million neurons. The final outcome of this research is a detailed architectural comparison of the two GPU architectures with a common programming platform.","2166-515X","978-1-4577-0635-6","10.1109/SAAHPC.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031572","Fermi;AMD;OpenCL;GPU Architecture Comparison;Profiler Counters;SNNs;speed-up","Graphics processing unit;Computer architecture;Neurons;Computational modeling;Mathematical model;Optimization;Firing","computer graphic equipment;coprocessors;neural nets","GPU architectures;spiking neural networks;general-purpose graphical processing units;high-performance computing;Nvidia Tesla C2050;Fermi;AMD Radeon 5870;supercomputing research groups;CUDA;performance centric research groups;OpenCL;application developers;programming model;Morris Lecar models;Hodgkin Huxley models;optimization techniques","","5","","27","","29 Sep 2011","","","IEEE","IEEE Conferences"
"CUKNN: A parallel implementation of K-nearest neighbor on CUDA-enabled GPU","Shenshen Liang; Cheng Wang; Ying Liu; Liheng Jian","Graduate University of Chinese Academy of Sciences, Beijing, China 100190; Agilent Technologies Co. Ltd., Beijing, China 100102; Graduate University of Chinese Academy of Sciences, Beijing, China 100190; Graduate University of Chinese Academy of Sciences, Beijing, China 100190","2009 IEEE Youth Conference on Information, Computing and Telecommunication","15 Jan 2010","2009","","","415","418","Recent development in Graphics Processing Units (GPUs) has enabled inexpensive high performance computing for general-purpose applications. Due to GPU's tremendous computing capability, it has emerged as the co-processor of the CPU to achieve a high overall throughput. CUDA programming model provides the programmers adequate C language like APIs to better exploit the parallel power of the GPU. K-nearest neighbor is a widely used classification technique and has significant applications in various domains. The computational-intensive nature of KNN requires a high performance implementation. In this paper, we present a CUDA-based parallel implementation of KNN, CUKNN, using CUDA multi-thread model. Various CUDA optimization techniques are applied to maximize the utilization of the GPU. CUKNN outperforms significantly and achieve up to 15.2X speedup. It also shows good scalability when varying the dimension of the training dataset and the number of records in training dataset.","","978-1-4244-5074-9","10.1109/YCICT.2009.5382329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5382329","KNN;classification;CUDA;parallel computing","High performance computing;Graphics;Coprocessors;Central Processing Unit;Throughput;Parallel programming;Programming profession;Scalability","C language;computer graphics;coprocessors;multi-threading;parallel architectures","CUDA-enabled GPU;graphics processing units;C language;CUDA multi-thread model;k-nearest neighbor parallel implementation;coprocessor;compute unified device architecture;high performance computing","","8","","6","","15 Jan 2010","","","IEEE","IEEE Conferences"
"GPU Accelerated Krylov Subspace Methods for Computational Electromagnetics","S. Velamparambil; S. MacKinnon-Cormier; J. Perry; R. Lemos; M. Okoniewski; J. Leon","Acceleware Corporation, 1600 37th St. SW, Calgary, Alberta-T3E 3P1, Canada. sanjay.velamparambil@acceleware.com; Acceleware Corporation, 1600 37th St. SW, Calgary, Alberta-T3E 3P1, Canada; Acceleware Corporation, 1600 37th St. SW, Calgary, Alberta-T3E 3P1, Canada; Acceleware Corporation, 1600 37th St. SW, Calgary, Alberta-T3E 3P1, Canada; Dept. Elect. & Comp. Engg., University of Calgary, Calgary, Canada. michal.okoniewski@acceleware.com; Faculty of Engineering, Dalhousie University, Halifax, Canada. Joshua.Leon@dal.ca","2008 38th European Microwave Conference","19 Jan 2009","2008","","","1312","1314","Programmable graphics processor units (GPU), such as the NVIDIA<sup>R</sup> Geforce 8800 series, offer a raw computing power that is often an order of magnitude larger than even the most modern multicore CPUs, making them a relatively inexpensive platform for high performance computing. In this paper, we report the development of two Krylov subspace solvers, the generalized minimal residual (GMRES) and the quasi-minimal residual (QMR) algorithms, on the GPU using the NVIDIA CUDA<sup>R</sup> programming model. The algorithms have been implemented as a stand-alone library. We report a speed-up of up to 13 times, on a single GPU, in our preliminary experiments with the classic problem of computing the capacitance of conductors using an integral equation method.","","978-2-87487-006-4","10.1109/EUMC.2008.4751704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4751704","","Acceleration;Computational electromagnetics;Integral equations;Computer interfaces;Graphics;High performance computing;Multicore processing;Libraries;Concurrent computing;Kernel","capacitance;computational electromagnetics;computer graphics;integral equations","programmable graphics processor units;Krylov subspace methods;computational electromagnetics;NVIDIA Geforce 8800 series;generalized minimal residual algorithm;quasiminimal residual algorithm;stand-alone library;single GPU;integral equation;capacitance;conductors;high performance computing;multicore CPUs","","6","","7","","19 Jan 2009","","","IEEE","IEEE Conferences"
"DDGSim: GPU based simulator for large multicore with bufferless NoC","N. Kumar; A. Sahu","Deptt. of Comp. Sc. and Engg., Indian Institute of Technology Guwahati, Assam, India, 781039; Deptt. of Comp. Sc. and Engg., Indian Institute of Technology Guwahati, Assam, India, 781039","2014 Annual IEEE India Conference (INDICON)","5 Feb 2015","2014","","","1","6","In large scale chip multicore, last level cache management and core interconnection network play important roles in performance and power consumption. And in large scale chip multicore, mesh interconnect is used widely due to scalability and simplicity of design. As interconnection network occupied significant area and consumes significant percent of system power, bufferless network is an appealing alternative design to reduce power consumption and hardware cost. We have designed and implemented a simulator for simulation of distributed cache management of large chip multicore where cores are connected using bufferless interconnection network. Also, we have redesigned and implemented the DDGSim, which is a GPU compatible parallel version of the same simulator using CUDA programming model. We have simulated target large chip multicore with up to 43,000 cores and achieved up to 25 times speedup on NVIDIA GeForce GTX 690 GPU over serial simulation.","2325-9418","978-1-4799-5364-6","10.1109/INDICON.2014.7030460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030460","","Multicore processing;Graphics processing units;Ports (Computers);Instruction sets;Kernel;Routing;Radiation detectors","cache storage;digital simulation;multiprocessing systems;network-on-chip;parallel architectures","DDGSim;GPU based simulator;bufferless NoC;distributed cache management;bufferless interconnection network;CUDA programming model;NVIDIA GeForce GTX 690 GPU;large scale chip multicore;LCMP","","","","35","","5 Feb 2015","","","IEEE","IEEE Conferences"
"Analyzing CUDA workloads using a detailed GPU simulator","A. Bakhoda; G. L. Yuan; W. W. L. Fung; H. Wong; T. M. Aamodt","University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada","2009 IEEE International Symposium on Performance Analysis of Systems and Software","12 May 2009","2009","","","163","174","Modern graphic processing units (GPUs) provide sufficiently flexible programming models that understanding their performance can provide insight in designing tomorrow's manycore processors, whether those are GPUs or otherwise. The combination of multiple, multithreaded, SIMD cores makes studying these GPUs useful in understanding tradeoffs among memory, data, and thread level parallelism. While modern GPUs offer orders of magnitude more raw computing power than contemporary CPUs, many important applications, even those with abundant data level parallelism, do not achieve peak performance. This paper characterizes several non-graphics applications written in NVIDIA's CUDA programming model by running them on a novel detailed microarchitecture performance simulator that runs NVIDIA's parallel thread execution (PTX) virtual instruction set. For this study, we selected twelve non-trivial CUDA applications demonstrating varying levels of performance improvement on GPU hardware (versus a CPU-only sequential version of the application). We study the performance of these applications on our GPU performance simulator with configurations comparable to contemporary high-end graphics cards. We characterize the performance impact of several microarchitecture design choices including choice of interconnect topology, use of caches, design of memory controller, parallel workload distribution mechanisms, and memory request coalescing hardware. Two observations we make are (1) that for the applications we study, performance is more sensitive to interconnect bisection bandwidth rather than latency, and (2) that, for some applications, running fewer threads concurrently than on-chip resources might otherwise allow can improve performance by reducing contention in the memory system.","","978-1-4244-4184-6","10.1109/ISPASS.2009.4919648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4919648","","Analytical models;Yarn;Graphics;Parallel processing;Microarchitecture;Hardware;Process design;Concurrent computing;Parallel programming;Computational modeling","cache storage;computer graphic equipment;instruction sets;multiprocessing systems;multi-threading;parallel architectures","CUDA workload;GPU simulator;graphic processing unit;flexible programming model;CUDA programming;microarchitecture performance simulator;parallel thread execution;virtual instruction set;GPU hardware;high-end graphics card;microarchitecture design;interconnect topology;caches;memory controller;parallel workload distribution;memory request coalescing hardware","","890","4","46","","12 May 2009","","","IEEE","IEEE Conferences"
"SMGuard: A Flexible and Fine-Grained Resource Management Framework for GPUs","C. Yu; Y. Bai; H. Yang; K. Cheng; Y. Gu; Z. Luan; D. Qian","Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","11 Nov 2018","2018","29","12","2849","2862","GPUs have been becoming an indispensable computing platform in data centers, and co-locating multiple applications on the same GPU is widely used to improve resource utilization. However, performance interference due to uncontrolled resource contention severely degrades the performance of co-locating applications and fails to deliver satisfactory user experience. In this paper, we present SMGuard, a software approach to flexibly manage the GPU resource usage of multiple applications under co-location. We also propose a capacity based GPU resource model CapSM, which provisions the GPU resources in a fine-grained granularity among co-locating applications. When co-locating latency-sensitive applications with batch applications, SMGuard can prevent batch applications from occupying resources without constraint using quota based mechanism, and guarantee the resource usage of latency-sensitive applications with reservation based mechanism. In addition, SMGuard supports dynamic resource adjustment through evicting the running thread blocks of batch applications to release the occupied resources and remapping the uncompleted thread blocks to the remaining resources, which avoids the relaunch of the preempted kernel. The SMGuard is a pure software solution that does not rely on special GPU hardware or programming model, which is easy to adopt on commodity GPUs in data centers. Our evaluation shows that SMGuard improves the average performance of latency-sensitive applications by 9.8× when co-located with batch applications. In the meanwhile, the GPU utilization can be improved by 35 percent on average.","1558-2183","","10.1109/TPDS.2018.2848621","National Key Research and Development Program of China(grant numbers:2016YFB1000503); National Science Foundation of China(grant numbers:61572062,61502019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388218","GPU;parallel computing;resource management;application co-location","Graphics processing units;Task analysis;Instruction sets;Resource management;Quality of service;Data centers","computer centres;graphics processing units;resource allocation","SMGuard;data centers;co-locating multiple applications;resource utilization;uncontrolled resource contention;GPU resource usage;GPU resource model CapSM;fine-grained granularity;latency-sensitive applications;batch applications;quota based mechanism;reservation based mechanism;dynamic resource adjustment;programming model;GPU utilization;flexible resource management framework;fine-grained resource management framework;performance interference","","4","","42","IEEE","19 Jun 2018","","","IEEE","IEEE Journals"
"LU, QR, and Cholesky factorizations: Programming model, performance analysis and optimization techniques for the Intel Knights Landing Xeon Phi","A. Haidar; S. Tomov; K. Arturov; M. Guney; S. Story; J. Dongarra","University of Tennessee, Knoxville, 37916, USA; University of Tennessee, Knoxville, 37916, USA; Intel Corporation, Novosibirsk, Russia; Intel Corporation, Hillsboro, OR 97124, USA; Intel Corporation, Hillsboro, OR 97124, USA; University of Tennessee, Knoxville, Oak Ridge National Laboratory, USA","2016 IEEE High Performance Extreme Computing Conference (HPEC)","1 Dec 2016","2016","","","1","7","A wide variety of heterogeneous compute resources, ranging from multicore CPUs to GPUs and coprocessors, are available to modern computers, making it challenging to design unified numerical libraries that efficiently and productively use all these varied resources. For example, in order to efficiently use Intel's Knights Landing (KNL) processor, the next-generation of Xeon Phi architectures, one must design and schedule an application in multiple degrees of parallelism and task grain sizes in order to obtain efficient performance. We propose a productive and portable programming model that allows us to write a serial-looking code, which, however, achieves parallelism and scalability by using a lightweight runtime environment to manage the resource-specific workload, and to control the dataflow and the parallel execution. This is done through multiple techniques ranging from multi-level data partitioning to adaptive task grain sizes, and dynamic task scheduling. In addition, our task abstractions enable unified algorithmic development across all the heterogeneous resources. Finally, we outline the strengths and the effectiveness of this approach - especially in regards to hardware trends and ease of programming high-performance numerical software that current applications need - in order to motivate current work and future directions for the next generation of parallel programming models for high-performance linear algebra libraries on heterogeneous systems.","","978-1-5090-3525-0","10.1109/HPEC.2016.7761591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761591","","Hardware;Programming;Libraries;Multicore processing;Parallel processing;Software","data flow computing;data handling;graphics processing units;linear algebra;mathematics computing;multiprocessing systems;parallel programming;processor scheduling;software libraries","LU factorization;QR factorization;Cholesky factorization;performance analysis;optimization technique;Intel Knights Landing Xeon Phi;heterogeneous compute resources;multicore CPU;GPU;coprocessors;unified numerical libraries;Intel KNL processor;Xeon Phi architecture;productive portable programming model;serial-looking code;scalability;lightweight runtime environment;resource-specific workload management;dataflow control;parallel execution;multilevel data partitioning;adaptive task grain size;dynamic task scheduling;task abstraction;unified algorithmic development;high-performance numerical software programming;parallel programming model;high-performance linear algebra libraries;heterogeneous systems","","7","","24","","1 Dec 2016","","","IEEE","IEEE Conferences"
"Designing a Profiling and Visualization Tool for Scalable and In-depth Analysis of High-Performance GPU Clusters","P. Kousha; B. Ramesh; K. Kandadi Suresh; C. Chu; A. Jain; N. Sarkauskas; H. Subramoni; D. K. Panda",The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University,"2019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC)","13 Feb 2020","2019","","","93","102","The recent advent of advanced fabrics like NVIDIA NVLink is enabling the deployment of dense Graphics Processing Unit (GPU) systems, e.g., DGX-2 and Summit. The Message Passing Interface (MPI) has been the dominant programming model to design distributed applications on such clusters. The MPI Tools Interface (MPI_T) provides an opportunity for performance tools and external software to introspect and understand MPI runtime behavior at a deeper level to detect performance and scalability issues. However, the lack of low-overhead and scalable monitoring tools have thus far prevented a comprehensive study of efficiency and utilization of high-performance interconnects such as NVLinks on high-performance GPU-enabled clusters. In this paper, we address this deficiency by proposing and designing an in-depth, real-time analysis, profiling, and visualization tool for high-performance GPU-enabled clusters with NVLinks. The proposed tool builds on the top of the OSU InfiniBand Network Analysis and Monitoring Tool (INAM). It provides insights into the efficiency of different communication patterns by examining the utilization of underlying GPU interconnects. The contributions of the proposed tool are two-fold: 1) domain scientists and system administrators can understand how applications and runtime libraries interact with underlying high-performance interconnects, and 2)Proposed tool enables designers of high-performance communication libraries to gain low-level knowledge to optimize existing designs and develop new algorithms to optimally utilize cutting-edge interconnects on GPU clusters. To the best of our knowledge, this is the first such tool which is capable of presenting a unified and holistic view of MPI-level and fabric level information for emerging NVLink-enabled high-performance GPU clusters.","2640-0316","978-1-7281-4535-8","10.1109/HiPC.2019.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8990500","MPI;MPI_T;NVLINK;GPU;Profiling","Graphics processing units;Tools;Libraries;Measurement;Fabrics;Topology;Bandwidth","application program interfaces;data visualisation;graphics processing units;message passing;parallel processing;software libraries","visualization tool;high-performance interconnects;high-performance communication libraries;MPI-level;NVLink-enabled high-performance GPU clusters;in-depth analysis;message passing interface;performance tools;MPI runtime behavior;scalable monitoring tools;NVIDIA NVLink;graphics processing unit;MPI tools interface;low-overhead tools;infiniband network analysis and monitoring tool;cutting-edge interconnects;fabric level information;runtime libraries","","6","","30","","13 Feb 2020","","","IEEE","IEEE Conferences"
"Code Complexity versus Performance for GPU-accelerated Scientific Applications","A. W. U. Munipala; S. V. Moore","University of Texas at El Paso, TX, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA","2016 Fourth International Workshop on Software Engineering for High Performance Computing in Computational Science and Engineering (SE-HPCCSE)","2 Feb 2017","2016","","","50","50","Summary form only given. Graphics Processing Units (GPUs) are becoming widely used as parallel accelerators in high-performance computing. GPU programming until recently, has been done by using low-level programming models such as CUDA and OpenCL. The directive-based OpenACC programming model has been growing in popularity due to its higher level of abstraction. This technique, which uses “directive” or “pragma” statements to annotate source code written in traditional high-level languages such as Fortran, C, and C++, is intended to allow a single code base to work across multiple computational platforms. We attempt to compare code complexity and performance of CUDA, OpenCL, and OpenACC implementations for three benchmark codes - the Game of Life (GOL) example code, the LULESH hydrodynamics proxy application, and the CloverLeaf mini-app from the Mantevo suite For the GOL C, CUDA C, and OpenCL codes and the LULESH C++, CUDA, and OpenCL codes, we measured source lines of code (SLOC) and cyclomatic complexity using the Oxbow toolkit static analysis tools. We ran the commercial McCabe IQ tool on the CloverLeaf Fortran90, Fortran90 + OpenACC, and Fortran portion of CloverLeaf_CUDA to measure cyclomatic complexity, design complexity, and essential complexity. We found that the CUDA and OpenCL implementations have significantly more lines of code than the corresponding OpenACC implementations but that the measured cyclomatic complexity is not always higher. The CUDA and OpenCL implementations generally have better performance, but there is not a drastic difference if the OpenACC code is optimized. We conclude that the available metrics and tools for measuring complexity of GPU programs are inadequate, since they do not quantify the portability and maintainability of the codes, and that specialization and extensions are needed.","","978-1-5090-5224-0","10.1109/SE-HPCCSE.2016.012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839472","","Complexity theory;Graphics processing units;Programming;Measurement;Software engineering;Computational modeling;C++ languages","C++ language;graphics processing units;parallel architectures;software metrics;software performance evaluation;source code (software)","code complexity;GPU-accelerated scientific applications;graphics processing units;game of life;GOL;LULESH hydrodynamics proxy application;CloverLeaf mini-app;Mantevo suite;GOL C;CUDA C;OpenCL codes;LULESH C++;source lines of code;SLOC;cyclomatic complexity;Oxbow toolkit static analysis tools;McCabe IQ tool;CloverLeaf Fortran90;Fortran90 + OpenACC;design complexity;essential complexity","","2","","","","2 Feb 2017","","","IEEE","IEEE Conferences"
"GPU acceleration of the iterative physical optics (IPO) method","Kan Xu; Z. W. Liu; D. Z. Ding; R. S. Chen","Department of Electronic Engineering, Nanjing University of Science and Technology, China; Department of Electronic Engineering, Nanjing University of Science and Technology, China; Department of Electronic Engineering, Nanjing University of Science and Technology, China; Department of Electronic Engineering, Nanjing University of Science and Technology, China","2008 8th International Symposium on Antennas, Propagation and EM Theory","16 Mar 2009","2008","","","733","735","In this paper, we employ the Programmable Graphics Processing Unit (GPU) to accelerate the IPO computation for analyzing the scattering of open cavities. Since the iterative strategy accounts for multiple reflections on the inner wall, the IPO method provides a more accurate solution than the other high frequency asymptotic methods. However, it suffers from a large requirement of simulation time. To date, the GPU featuring inherent parallelism and powerful floating-point capability has become an attractive alternative to the central processing unit (CPU) for some of computation tasks. Therefore, we map the time-consuming parts of IPO into the graphics hardware following the stream programming model, and the CPU carries out the computation of the far field scattering. In addition, the numerical results demonstrate the accuracy and effectiveness of our proposed method.","","978-1-4244-2192-3","10.1109/ISAPE.2008.4735319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4735319","","Acceleration;Iterative methods;Physical optics;Central Processing Unit;Graphics;Optical scattering;Optical reflection;Frequency;Computational modeling;Parallel processing","computational electromagnetics;electromagnetic wave scattering;iterative methods;parallel programming;physical optics","GPU acceleration;programmable graphics processing unit;multiple reflections;central processing unit;inherent parallelism;floating-point capability;time-consuming parts;iterative physical optics method","","","","8","","16 Mar 2009","","","IEEE","IEEE Conferences"
"GraphReduce: Large-Scale Graph Analytics on Accelerator-Based HPC Systems","D. Sengupta; K. Agarwal; S. L. Song; K. Schwan","Georgia Inst. of Technol., Atlanta, GA, USA; Georgia Inst. of Technol., Atlanta, GA, USA; Pacific Northwest Nat. Lab., Richland, WA, USA; Georgia Inst. of Technol., Atlanta, GA, USA","2015 IEEE International Parallel and Distributed Processing Symposium Workshop","1 Oct 2015","2015","","","604","609","Recent work on graph analytics has sought to leverage the high performance offered by GPU devices, but challenges remain due to the inherent irregularity of graph algorithm and limitations in GPU-resident memory for storing large graphs. The Graph Reduce methods presented in this paper permit a GPU-based accelerator to operate on graphs that exceed its internal memory capacity. Graph Reduce operates with a combination of both edge- and vertex-centric implementations of the Gather-Apply-Scatter programming model, to achieve high degrees of parallelism supported by methods that partition graphs across GPU and host memories and efficiently move graph data between both. Graph Reduce-based programming is performed via device functions that include gather map, gather reduce, apply, and scatter, implemented by programmers for the graph algorithms they wish to realize. Experimental evaluations for a wide variety of graph inputs, algorithms, and system configuration demonstrate that Graph Reduce outperforms other competing approaches.","","978-1-4673-7684-6","10.1109/IPDPSW.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284365","graph processing;big data;GPU;CUDA","Graphics processing units;Computational modeling;Programming;Parallel processing;Algorithm design and analysis;Hardware;Data structures","graph theory;graphics processing units;storage management","GraphReduce;large-scale graph analytics;accelerator-based HPC systems;GPU devices;graph algorithm irregularity;GPU-resident memory;GPU-based accelerator;internal memory capacity;edge-centric implementations;vertex-centric implementations;gather-apply-scatter programming model;partition graphs;gather_map;gather_reduce","","3","","22","","1 Oct 2015","","","IEEE","IEEE Conferences"
"Automatic Code Tuning for Improving GPU Resource Utilization","R. Takeshima; T. Tsumura","Nagoya Inst. of Technol., Nagoya, Japan; Nagoya Inst. of Technol., Nagoya, Japan","2014 Second International Symposium on Computing and Networking","2 Mar 2015","2014","","","419","425","Utilizing a GPU to perform general purpose computation is called GPGPU. The high theoretical performance of GPU draws attention to GPGPU. CUDA supplies a platform for the developers of GPU applications. In CUDA programming model, massive threads are allocated to GPU's calculation units. Besides, CUDA has various kinds of memories on GPU. These memories have different features of access latency, capacity, and so on. Therefore, to produce high-performance GPU programs, developers should consider how to allocate the massive threads to cores and which memory should be used for storing data. Hence, developers should have deep understanding of the GPU architecture and CUDA APIs. To address this problem, we propose an auto tuning framework for GPU programs, and explain an implementation of a preprocessor for the framework, in this paper.","2379-1896","978-1-4799-4152-0","10.1109/CANDAR.2014.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052220","","Graphics processing units;Kernel;Message systems;Instruction sets;Registers;Tuning","application program interfaces;graphics processing units;parallel architectures;resource allocation","code tuning;GPU resource utilization;general purpose graphics processing unit;GPGPU;Compute Unified Device Architecture;high-performance GPU program;CUDA API;application program interface","","1","","11","","2 Mar 2015","","","IEEE","IEEE Conferences"
"Multi-Level Graph Layout on the GPU","Y. Frishman; A. Tal","Technion, Israel Institute of Technology; Technion, Israel Institute of Technology","IEEE Transactions on Visualization and Computer Graphics","5 Nov 2007","2007","13","6","1310","1319","This paper presents a new algorithm for force directed graph layout on the GPU. The algorithm, whose goal is to compute layouts accurately and quickly, has two contributions. The first contribution is proposing a general multi-level scheme, which is based on spectral partitioning. The second contribution is computing the layout on the GPU. Since the GPU requires a data parallel programming model, the challenge is devising a mapping of a naturally unstructured graph into a well-partitioned structured one. This is done by computing a balanced partitioning of a general graph. This algorithm provides a general multi-level scheme, which has the potential to be used not only for computation on the GPU, but also on emerging multi-core architectures. The algorithm manages to compute high quality layouts of large graphs in a fraction of the time required by existing algorithms of similar quality. An application for visualization of the topologies of ISP (Internet service provider) networks is presented.","1941-0506","","10.1109/TVCG.2007.70580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376155","Graph layout;GPU;graph partitioning.","Partitioning algorithms;Acceleration;Application software;Visualization;High performance computing;Parallel programming;Computer architecture;Quality management;Network topology;Web and internet services","computer graphics;graph theory;parallel programming","multi-level graph layout;directed graph layout;general multi-level scheme;spectral partitioning;data parallel programming model;naturally unstructured graph;multi-core architectures;Internet service provider;graph partitioning","","70","1","41","","5 Nov 2007","","","IEEE","IEEE Journals"
"On the Portability of the OpenCL Dwarfs on Fixed and Reconfigurable Parallel Platforms","K. Krommydas; M. Owaida; C. D. Antonopoulos; N. Bellas; W. -C. Feng",NA; NA; NA; NA; NA,"2013 International Conference on Parallel and Distributed Systems","1 May 2014","2013","","","432","433","The proliferation of heterogeneous computing systems presents the parallel computing community with the challenge of porting legacy and emerging applications to multiple processors with diverse programming abstractions. OpenCL is a vendor-agnostic and industry-supported programming model that offers code portability on heterogeneous platforms, allowing applications to be developed once and deployed ""anywhere."" In this paper, we use the OpenCL implementation of the Open Dwarfs, a benchmark suite that captures patterns of computation and communication common to classes of important applications, as delineated by Berkeley's Dwarfs. We evaluate portability across multicore CPU, GPU, APU (CPUs+GPUs on a die), the Intel Xeon Phi co-processor, and the FPGA. To realize FPGA portability, we exploit SOpenCL (Silicon OpenCL), a CAD tool that automatically converts OpenCL kernels to customizable hardware accelerators. We show that a single, unmodified OpenCL code base, i.e., Open Dwarfs, can be effectively used to target multiple, architecturally diverse platforms.","1521-9097","978-1-4799-2081-5","10.1109/ICPADS.2013.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808208","OpenCL;CPU;GPU;APU;Xeon Phi;FPGA;dwarfs;portability","Field programmable gate arrays;Graphics processing units;Hardware;Hardware design languages;Kernel;Programming;Parallel processing","parallel processing","OpenCL Dwarfs;reconfigurable parallel platforms;heterogeneous computing systems;parallel computing community;porting legacy;diverse programming abstractions;industry-supported programming model;code portability;heterogeneous platforms;multicore CPU;GPU;APU;Intel Xeon Phi coprocessor;FPGA portability;SOpenCL;Silicon OpenCL;CAD tool;OpenCL kernels;OpenCL code base","","4","","5","","1 May 2014","","","IEEE","IEEE Conferences"
"Using CUDA GPU to Accelerate the Ant Colony Optimization Algorithm","K. Wei; C. Wu; C. Wu","Comput. Sci. & Inf. Eng., Nat. Changhua Univ. of Educ., Changhua, Taiwan; Comput. Sci. & Inf. Eng., Nat. Changhua Univ. of Educ., Changhua, Taiwan; Comput. Sci. & Inf. Eng., Nat. Changhua Univ. of Educ., Changhua, Taiwan","2013 International Conference on Parallel and Distributed Computing, Applications and Technologies","22 Sep 2014","2013","","","90","95","Graph Processing Units (GPUs) have recently evolved into a super multi-core and a fully programmable architecture. In the CUDA programming model, the programmers can simply implement parallelism ideas of a task on GPUs. The purpose of this paper is to accelerate Ant Colony Optimization (ACO) for Traveling Salesman Problems (TSP) with GPUs. In this paper, we propose a new parallel method, which is called the Transition Condition Method. Experimental results are extensively compared and evaluated on the performance side and the solution quality side. The TSP problems are used as a standard benchmark for our experiments. In terms of experimental results, our new parallel method achieves the maximal speed-up factor of 4.74 than the previous parallel method. On the other hand, the quality of solutions is similar to the original sequential ACO algorithm. It proves that the quality of solutions does not be sacrificed in the cause of speed-up.","2379-5352","978-1-4799-2419-6","10.1109/PDCAT.2013.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6904238","GPU;CUDA;Ant Colony Optimization;ACO;TSP","Cities and towns;Graphics processing units;Instruction sets;Arrays;Wheels;Memory management","ant colony optimisation;graphics processing units;parallel architectures;parallel programming;travelling salesman problems","CUDA GPU;ant colony optimization algorithm;graph processing units;programmable architecture;ant colony optimization;traveling salesman problems;TSP;Transition Condition Method;parallel method;super multi-core;CUDA programming model","","2","","14","","22 Sep 2014","","","IEEE","IEEE Conferences"
"Comparison of Xeon Phi and Kepler GPU Performance for Finite Element Numerical Integration","K. Banas; F. Kruzel","Dept. of Appl. Comput. Sci. & Modelling, AGH Univ. of Sci. & Technol., Krakόw, Poland; Inst. of Comput. Sci., Cracow Univ. of Technol., Krakόw, Poland","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","12 Mar 2015","2014","","","145","148","We consider two recently introduced massively multi-core architectures designed for high performance computing, the Xeon Phi coprocessor and Kepler graphics processor. We discuss the OpenCL programming model, as one that allows to look at the platforms in a unified way and to construct efficient algorithms for both of them. As an example application we investigate a typical algorithm employed in finite element codes for numerical integration. We create kernels implementing the algorithm for the two considered platforms and compare the performance obtained.","","978-1-4799-6123-8","10.1109/HPCC.2014.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056730","OpenCL;performance analysis;Xeon Phi coprocessor;GPU;Kepler architecture;finite elements;numerical integration","Finite element analysis;Registers;Instruction sets;Computer architecture;Kernel;Graphics processing units;Numerical models","finite element analysis;graphics processing units;integration;multiprocessing systems;parallel processing","Xeon Phi coprocessor;Kepler GPU graphics processor;finite element numerical integration;massively multicore architectures;high performance computing;OpenCL programming model","","2","","8","","12 Mar 2015","","","IEEE","IEEE Conferences"
"Evaluating Optimization Strategies for HMMer Acceleration on GPU","S. Ferraz; N. Moreano","Sch. of Comput., Fed. Univ. of Mato Grosso do Sul, Campo Grande, Brazil; Sch. of Comput., Fed. Univ. of Mato Grosso do Sul, Campo Grande, Brazil","2013 International Conference on Parallel and Distributed Systems","1 May 2014","2013","","","59","68","Comparing a biological sequence to a family of sequences is an important task in Bioinformatics, commonly performed using tools such as HMMer. The Viterbi algorithm is applied as HMMer main step to compute the similarity between the sequence and the family. Due to the exponential growth of biological sequence databases, implementations of the Viterbi algorithm on several high performance platforms have been proposed. Nevertheless, few implementations of the Viterbi algorithm use GPUs as main platform. In this paper, we present the development and optimization of an accelerator for the Viterbi algorithm applied to biological sequence analysis on GPUs. Some of the optimizations analyzed are applied to the sequence comparison problem for the first time in the literature and others are evaluated in more depth than in related works. Our main contributions are: (a) an accelerator that achieves speedups up to 102.90 and 60.46, with respect to HMMer2 and HMMer3 execution on a general purpose computer, respectively, (b) the use of the multi-platform OpenCL programming model for the accelerator, (c) a detailed evaluation of several optimizations such as memory, control flow, execution space, instruction scheduling, and loop optimizations, and (d) a methodology of optimizations and evaluation that can also be applied to other sequence comparison algorithms, such as the HMMer3 MSV.","1521-9097","978-1-4799-2081-5","10.1109/ICPADS.2013.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808158","Sequence-profile alignment;Viterbi algorithm;GPU;Accelerator;Optimization;OpenCL","Hidden Markov models;Graphics processing units;Viterbi algorithm;Memory management;Optimization;Databases;Parallel processing","bioinformatics;graphics processing units;hidden Markov models;optimisation;scheduling","optimization strategies;HMMer acceleration;GPU;bioinformatics;Viterbi algorithm;biological sequence databases;HMMer2;general purpose computer;multiplatform OpenCL programming model;accelerator;control flow;execution space;instruction scheduling;loop optimizations;HMMer3 MSV;sequence comparison algorithms","","1","15","26","","1 May 2014","","","IEEE","IEEE Conferences"
"The Heterogeneous System Architecture: It's beyond the GPU","P. Blinzer","Advanced Micro Devices, Inc., United States","2014 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV)","8 Sep 2014","2014","","","iii","iii","Summary form only given. The use of GPUs in computation intensive tasks has an ever increasing impact across all platforms - including embedded - sometimes even used to create new forms of currency (Bitcoin, Litecoin, ...). And the exponential improvements in Performance per Watt gains are still ongoing unabated. At the same time, due to their “design heritage” as primarily 3D accelerators, GPUs have several properties that make it a SW challenge to unlock their full benefit in many real-world application scenarios, be it due to limiting API's (proprietary or limited functionality) or properties that require an advanced understanding of the platform architecture and managing the memory and other system resources, beyond the reach of the “average programmer”. The Heterogeneous System Architecture is established by the HSA Foundation to address many of the current shortcomings at a system architecture and programming model level while providing a great foundation for already established SW models, and in addition to the GPU allow extending the architecture to other specialty processors like DSPs, FPGAs and others to interoperate within the SW framework, a main task for the next level of work in the HSA Foundation. The HSA Foundation, a not-for-profit consortium of SOC and SOC IP vendors, OEMs, academia, OSVs and ISVs defining a consistent heterogeneous platform architecture to make it dramatically easier to program heterogeneous parallel devices like GPUs and other accelerators. The presentation gives the audience a high-level understanding of the goals of HSA, the HSA system architecture properties and its use models by system software, tools and applications.","","978-1-4799-3770-7","10.1109/SAMOS.2014.6893187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893187","","Computer architecture;Computational modeling;Graphics processing units;System-on-chip;Graphics;Computers","application program interfaces;computer architecture;graphics processing units","heterogeneous system architecture;GPU;computation intensive tasks;performance per watt gains;design heritage;3D accelerators;SW challenge;API;HSA foundation;DSP;FPGA;SOC IP vendors;OEM;OSV;ISV","","","","","","8 Sep 2014","","","IEEE","IEEE Conferences"
"Detailed Performance Analysis of Distributed Tensorflow on a GPU Cluster using Deep Learning Algorithms","A. Malik; M. Lu; N. Wang; Y. Lin; S. Yoo","Computer Science Initiative, Brookhaven National Laboratory; Computer Science Initiative, Brookhaven National Laboratory; Computer Science Initiative, Brookhaven National Laboratory; Computer Science Initiative, Brookhaven National Laboratory; Computer Science Initiative, Brookhaven National Laboratory","2018 New York Scientific Data Summit (NYSDS)","18 Nov 2018","2018","","","1","8","Long training times for building a high accuracy deep neural networks (DNNs) is impeding research for new DNN architectures. For example, time for training GoogleNet with the ImageNet dataset on a single Nvidia K20 GPU almost takes 25 days. Therefore, there is a great need in the AI community to speed up the training phase, especially when using a large dataset. For this, we need Distributed Deep Neural Networks (DDNNs) that can scale well with more computation resources. However, this involves two challenges.First, the deep learning framework or training library must support inter-node communication. Second, the user must modify the code to take advantage of the inter-node communication. The changes to the code can be minimal to significant depending upon the user expertize in the distributed systems. Current DNN frameworks support distributed learning using MPI. However, these frameworks come with poorly understood overheads associated with communication and data management. Tensorflow provides APIs for distributed learning using MPI programming model and gRPC. These APIs are not easy to use for a domain expert for designing an efficient distributed learning model. Recently, Uber Inc. provides the Horovod Framework which gives a fast and easy way to support distributed learning using Tensorflow, Pytorach, and Keras. In this paper we provide a detailed performance analysis of distributed Tensorflow using Horovod. We implemented distributed learning for AlexNet, GoogleNet, and ResNet50 using Horovod. We used Nvidia K 40,K80, and P100 GPUs for our experimentation. We used synthetic image data with different runtime variables (batch size and number of GPUs). Our results shows that the Horovod framework gives almost linear throughput (images/sec) scalability up to 256 GPUs.","","978-1-5386-7933-3","10.1109/NYSDS.2018.8538946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538946","High Performance Computing;Tensorflow;Deep Learning;Distributed Learning;Performance Analysis","Training;Neural networks;Parallel processing;Computer architecture;Graphics processing units;Computational modeling","application program interfaces;graphics processing units;learning (artificial intelligence);message passing;neural nets;parallel architectures","computation resources;deep learning framework;inter-node communication;distributed systems;current DNN frameworks;data management;MPI programming model;domain expert;distributed learning model;Horovod Framework;distributed tensorflow;Horovod framework;GPU cluster;deep learning algorithms;high accuracy deep neural networks;DNN architectures;training GoogleNet;ImageNet dataset;single Nvidia K20 GPU;AI community;API","","","","28","","18 Nov 2018","","","IEEE","IEEE Conferences"
"An improved vision-based wastewater velocity measurement system using discontinuity-preserving smoothing and GPU acceleration","C. Cao Pham; T. Tuong Nguyen; J. Jae Wook","School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Korea; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Korea; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, Korea","2011 11th International Conference on Control, Automation and Systems","19 Dec 2011","2011","","","1303","1308","Automatic long-term measuring wastewater velocity is an important and challenging task in hydraulic systems. This paper proposed a vision-based wastewater velocity measurement method using Bilateral filter that is a discontinuity-preserving smoothing as a prior-processing step. Experimental results showed that using Bilateral filter can improve estimation accuracy over existing methods. An effective background creation algorithm and simple floating waste tracking algorithm based on binary blob properties are also discussed in this paper. Furthermore, by implementing the proposed method on massively parallel GPU (graphics processing units) using the CUDA (compute unified device architecture) programming model, we can achieve a satisfactory acceleration to apply in real-time applications. Memory usage optimization methods are discussed and analyzed for effective implementation in graphics hardware.","2093-7121","978-89-93215-03-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106126","Water flow measurement;image processing;graphics processing units","Graphics processing unit;Particle measurements;Atmospheric measurements;Velocity measurement;Accuracy;Smoothing methods;Noise","computer vision;filtering theory;flow measurement;graphics processing units;object tracking;parallel architectures;storage management;velocity measurement;wastewater","vision-based wastewater velocity measurement system;discontinuity-preserving smoothing;GPU acceleration;hydraulic system;bilateral filter;background creation algorithm;floating waste tracking algorithm;binary blob properties;massively parallel GPU;graphics processing units;CUDA programming model;compute unified device architecture;real-time application;memory usage optimization;graphics hardware","","","","16","","19 Dec 2011","","","IEEE","IEEE Conferences"
"Porting LASG/ IAP Climate System Ocean Model to Gpus Using OpenAcc","J. Jiang; P. Lin; J. Wang; H. Liu; X. Chi; H. Hao; Y. Wang; W. Wang; L. Zhang","Computer Network Information Center, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; LASG, Institute of Atmospheric Physics, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; NVIDIA, Beijing, China; LASG, Institute of Atmospheric Physics, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; School of Information Engineering, China University of Geosciences (Beijing), Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China","IEEE Access","30 Oct 2019","2019","7","","154490","154501","GPUs have become important solutions for accelerating scientific applications. Most of the existing work on climate models now use code rewritten using CUDA to achieve a limited speedup. This restriction also greatly limits followup development and applications. In this paper, we designed and implemented a GPU-based acceleration of the LASG/IAP climate system ocean model (LICOM) version 2, called LICOM2-GPU. Considering the extremely large codebase of the model and the occasional need to modify the code, we implemented the model completely in OpenACC. Several accelerated methods, including OpenACC data locality optimization, loop optimization, and interprocess communication optimization are presented. Developing for GPUs using OpenACC is substantially simpler than using the CUDA port. Thus, the OpenACC is a suitable GPU programming model for complex systems, such as the earth system model and its components. Our experimental results using 4 NVIDIA K80 cards achieved up to a 6.6$ \times $ speedup compared with 4 Intel(R) Xeon(R) CPU E5-2690 v2 GPUs.","2169-3536","","10.1109/ACCESS.2019.2932443","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0200800); National Natural Science Foundation of China(grant numbers:61602477,61432018); Strategic Priority Research Programme(grant numbers:XDC01040000); Chinese Academy of Sciences(grant numbers:XXH13506-402,XXH13506-302); Open Research Project of the Key Laboratory of Geological Information Technology of Ministry of Natural Resources; Open Research Project of the Hubei Key Laboratory of Intelligent Geo-Information Processing(grant numbers:KLIGIP-2017A04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784044","High performance computing;parallel algorithm;GPU;LICOM;parallel acceleration","Graphics processing units;Acceleration;Computational modeling;Meteorology;Atmospheric modeling;Optimization;Oceans","geophysics computing;graphics processing units;parallel architectures","accelerated methods;OpenACC data locality optimization;interprocess communication optimization;CUDA port;suitable GPU programming model;complex systems;earth system model;scientific applications;climate models;code rewritten;GPU-based acceleration;LICOM2-GPU","","2","","25","CCBY","1 Aug 2019","","","IEEE","IEEE Journals"
"Hybrid Map Task Scheduling for GPU-Based Heterogeneous Clusters","K. Shirahata; H. Sato; S. Matsuoka","Tokyo Inst. of Technol., Tokyo, Japan; Tokyo Inst. of Technol., Tokyo, Japan; Tokyo Inst. of Technol., Tokyo, Japan","2010 IEEE Second International Conference on Cloud Computing Technology and Science","4 Feb 2011","2010","","","733","740","MapReduce is a programming model that enables efficient massive data processing in large-scale computing environments such as supercomputers and clouds. Such large-scale computers employ GPUs to enjoy its good peak performance and high memory bandwidth. Since the performance of each job is depending on running application characteristics and underlying computing environments, scheduling MapReduce tasks onto CPU cores and GPU devices for efficient execution is difficult. To address this problem, we have proposed a hybrid scheduling technique for GPU-based computer clusters, which minimizes the execution time of a submitted job using dynamic profiles of Map tasks running on CPU cores and GPU devices. We have implemented a prototype of our proposed scheduling technique by extending MapReduce framework, Hadoop. We have conducted some experiments for this prototype by using a K-means application as a benchmark on a supercomputer. The results show that the proposed technique achieves 1.93 times faster than the Hadoop original scheduling algorithm at 64 nodes (1024 CPU cores and 128 GPU devices). The results also indicate that the performance of map tasks, including both CPU and GPU tasks, is significantly affected by the overhead of map task invocation in the Hadoop framework.","","978-1-4244-9405-7","10.1109/CloudCom.2010.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708524","Large-scale data processing;MapReduce;GPGPU;Job Scheduling","Graphics processing unit;Java;Performance evaluation;Processor scheduling;Prototypes;Central Processing Unit;Computers","computer graphic equipment;coprocessors;microcomputers;parallel machines;pattern clustering","hybrid map task scheduling;MapReduce model;programming model;data processing;memory bandwidth;hybrid scheduling technique;GPU-based computer clusters;K-means application;supercomputer;Hadoop original scheduling algorithm","","50","1","15","","4 Feb 2011","","","IEEE","IEEE Conferences"
"Efficient Fork-Join on GPUs Through Warp Specialization","A. C. Jacob; A. E. Eichenberger; H. Sung; S. F. Antao; G. -T. Bercea; C. Bertolli; A. Bataev; T. Jin; T. Chen; Z. Sura; G. Rokos; K. O'Brien","IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA","2017 IEEE 24th International Conference on High Performance Computing (HiPC)","8 Feb 2018","2017","","","358","367","Graphics Processing Units (GPUs) are increasingly used to accelerate portions of general-purpose applications. Higher level language extensions have been proposed to help non-experts bridge the gap between a host and the GPU's threading model. Recent updates to the OpenMP standard allow a user to parallelize code on a GPU using the well known fork-join programming model for CPUs. Mapping this model to the architecturally visible threading model of typical GPUs has been challenging. In this work we propose a novel approach using the technique of Warp Specialization. We show how to specialize one warp (a unit of 32 GPU threads) to handle sequential code on a GPU. When this master warp reaches a user-specified parallel region, it awakens unused GPU warps to collectively execute the parallel code. Based on this method, we have implemented a Clang-based, OpenMP 4.5 compliant, open source compiler for GPUs. Our work achieves a 3.6x (and up to 32x) performance improvement over a baseline that does not exploit fork-join parallelism on an NVIDIA k40m GPU across a set of 25 kernels. Compared to state-of-the-art compilers (Clang-ykt, GCC-OpenMP, GCC-OpenACC) our work is 2.1 - 7.6x faster. Our proposed technique is simpler to implement, robust, and performant.","","978-1-5386-2293-3","10.1109/HiPC.2017.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8287767","OpenMP;Fork-Join;GPU;Warp Specialization","Graphics processing units;Parallel processing;Kernel;Programming;Benchmark testing;Hardware;Data models","graphics processing units;multiprocessing systems;parallel programming;sequential codes","parallelism;GCC-OpenMP;Warp Specialization;Graphics Processing Units;higher level language extensions;GPU's threading model;OpenMP standard;programming model;architecturally visible threading model;sequential code;unused GPU warps;parallel code;Fork-join","","9","","29","","8 Feb 2018","","","IEEE","IEEE Conferences"
"How Well do CPU, GPU and Hybrid Graph Processing Frameworks Perform?","T. K. Aasawat; T. Reza; M. Ripeanu","Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","6 Aug 2018","2018","","","458","466","The importance of high-performance graph processing to solve big data problems targeting high-impact applications is greater than ever before. Recent graph processing frameworks target different hardware platforms (e.g., shared memory systems, accelerators such as GPUs, and distributed systems) and differ with respect to the programming model they adopt (e.g., based on linear algebra formulations of graph algorithms or enabling direct access to the graph structure). To better understand the impact of these choices, this paper, presents a comparative study of five state-of-the-art graph processing frameworks: two CPU-only frameworks - GraphMat and Galois, two GPU-based frameworks - Nvgraph and Gunrock; and Totem, a hybrid (CPU+GPU) framework. We use three popular graph algorithms (PageRank, Single Source Shortest Path, and Breadth-First Search), and massive scale graphs with up to billions of edges. Our evaluation focuses on three performance metrics: (i) execution time, (ii) scalability and (iii) energy consumption.","","978-1-5386-5555-9","10.1109/IPDPSW.2018.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8425449","Graph Processing;CPU;GPU;Hybrid Systems;Performance Evaluation;PageRank;SSSP;BFS","Computational modeling;Graphics processing units;Programming;Linear algebra;Measurement;Sparse matrices;Instruction sets","Big Data;graph theory;graphics processing units;mathematics computing;parallel processing","graph processing frameworks;Nvgraph;GraphMat;Galois;Gunrock;Totem;hybrid graph;massive scale graphs;graph structure;linear algebra formulations;programming model;high-impact applications;big data problems;high-performance graph","","2","","35","","6 Aug 2018","","","IEEE","IEEE Conferences"
"Overlapping Data Transfers with Computation on GPU with Tiles","B. Bastem; D. Unat; W. Zhang; A. Almgren; J. Shalf","Koc Univ., Istanbul, Turkey; Koc Univ., Istanbul, Turkey; Lawrence Berkeley Nat. Lab., Berkeley, CA, USA; Lawrence Berkeley Nat. Lab., Berkeley, CA, USA; Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","2017 46th International Conference on Parallel Processing (ICPP)","7 Sep 2017","2017","","","171","180","GPUs are employed to accelerate scientific applications however they require much more programming effort from the programmers particularly because of the disjoint address spaces between the host and the device. OpenACC and OpenMP 4.0 provide directive based programming solutions to alleviate the programming burden however synchronous data movement can create a performance bottleneck in fully taking advantage of GPUs. We propose a tiling based programming model and its library that simplifies the development of GPU programs and overlaps the data movement with computation. The programming model decomposes the data and computation into tiles and treats them as the main data transfer and execution units, which enables pipelining the transfers to hide the transfer latency. Moreover, partitioning application data into tiles allows the programmer to still take advantage of GPU even though application data cannot fit into the device memory. The library leverages C++ lambda functions, OpenACC directives, CUDA streams and tiling API from TiDA to support both productivity and performance. We show the performance of the library on a data transfer-intensive and a compute-intensive kernels and compare its speedup against OpenACC and CUDA. The results indicate that the library can hide the transfer latency, handle the cases where there is no sufficient device memory, and achieves reasonable performance.","2332-5690","978-1-5386-1042-8","10.1109/ICPP.2017.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8025291","GPUs;Tiles;Programming Models;Overlapping communication with computation;Library;OpenACC;CUDA","Graphics processing units;Kernel;Programming;Data transfer;Performance evaluation;Libraries;Memory management","data handling;graphics processing units;parallel architectures","compute-intensive kernels;data transfer-intensive kernels;TiDA;API;CUDA streams;OpenACC directives;C++ lambda functions;device memory;application data;transfer latency;GPU programs;tiling based programming model;synchronous data movement;directive based programming solutions;OpenMP 4.0;programmers;programming effort;scientific applications;overlapping data transfers","","4","","34","","7 Sep 2017","","","IEEE","IEEE Conferences"
"AES finalists implementation for GPU and multi-core CPU based on OpenCL","Xingliang Wang; Xiaochao Li; Mei Zou; Jun Zhou","Department of Electronic Engineering, Xiamen University, China, 361005; Department of Electronic Engineering, Xiamen University, China, 361005; Department of Electronic Engineering, Xiamen University, China, 361005; Department of Electronic Engineering, Xiamen University, China, 361005","2011 IEEE International Conference on Anti-Counterfeiting, Security and Identification","28 Jul 2011","2011","","","38","42","Benefit from the OpenCL (Open Computing Language), applications can be easily transplanted among different GPUs, multi-core CPUs, and other processors. In this paper, we present implementation of AES finalists (Rijndael, Serpent, Twofish) in XTS mode, based on OpenCL. Benchmark testing is performed on 4 mainstream GPUs and multi-core CPUs. The results are also compared with implementations based on traditional serial programming model and CUDA. The resulting data shows that throughputs based on OpenCL are higher than serial programming model, while a little lower than CUDA. Which demonstrates that OpenCL promises a portable language for GPU programming, while entail a performance penalty.","2163-5056","978-1-61284-632-3","10.1109/ASID.2011.5967411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967411","OpenCL;GPU;Rijndael;Serpent;Twofish;XTS","Graphics processing unit;Programming;Encryption;Throughput;Kernel;Performance evaluation","computer graphic equipment;coprocessors;cryptography;multiprocessing systems","AES finalists implementation;multicore CPU;OpenCL;open computing language;XTS mode;GPU programming;advanced encryption standard","","5","","16","","28 Jul 2011","","","IEEE","IEEE Conferences"
"Computing 2D Constrained Delaunay Triangulation Using the GPU","M. Qi; T. Cao; T. Tan","National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore","IEEE Transactions on Visualization and Computer Graphics","19 Mar 2013","2013","19","5","736","748","We propose the first graphics processing unit (GPU) solution to compute the 2D constrained Delaunay triangulation (CDT) of a planar straight line graph (PSLG) consisting of points and edges. There are many existing CPU algorithms to solve the CDT problem in computational geometry, yet there has been no prior approach to solve this problem efficiently using the parallel computing power of the GPU. For the special case of the CDT problem where the PSLG consists of just points, which is simply the normal Delaunay triangulation (DT) problem, a hybrid approach using the GPU together with the CPU to partially speed up the computation has already been presented in the literature. Our work, on the other hand, accelerates the entire computation on the GPU. Our implementation using the CUDA programming model on NVIDIA GPUs is numerically robust, and runs up to an order of magnitude faster than the best sequential implementations on the CPU. This result is reflected in our experiment with both randomly generated PSLGs and real-world GIS data having millions of points and edges.","1941-0506","","10.1109/TVCG.2012.307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6361389","GPGPU;parallel computation;computational geometry;Voronoi diagram;image vectorization","Graphics processing units;Instruction sets;Arrays;Strips;Standards;Color","graphics processing units;mesh generation;parallel architectures","computing 2D constrained Delaunay triangulation;graphics processing unit;CDT;planar straight line graph;PSLG;computational geometry;parallel computing power;CUDA programming model;NVIDIA GPU","Algorithms;Computer Graphics;Image Enhancement;Image Enhancement;Image Interpretation, Computer-Assisted;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Imaging, Three-Dimensional;Numerical Analysis, Computer-Assisted;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted","17","","30","","26 Nov 2012","","","IEEE","IEEE Journals"
"Kernel Fusion: An Effective Method for Better Power Efficiency on Multithreaded GPU","G. Wang; Y. Lin; W. Yi","Nat. Lab. for Parallel & Distrib. Process. Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Nat. Lab. for Parallel & Distrib. Process. Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Nat. Lab. for Parallel & Distrib. Process. Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China","2010 IEEE/ACM Int'l Conference on Green Computing and Communications & Int'l Conference on Cyber, Physical and Social Computing","7 Mar 2011","2010","","","344","350","As one of the most popular accelerators, Graphics Processing Unit (GPU) has demonstrated high computing power in several application fields. On the other hand, GPU also produces high power consumption and has been one of the most largest power consumers in desktop and supercomputer systems. However, software power optimization method targeted for GPU has not been well studied. In this work, we propose kernel fusion method to reduce energy consumption and improve power efficiency on GPU architecture. Through fusing two or more independent kernels, kernel fusion method achieves higher utilization and much more balanced demand for hardware resources, which provides much more potential for power optimization, such as dynamic voltage and frequency scaling (DVFS). Basing on the CUDA programming model, this paper also gives several different fusion methods targeted for different situations. In order to make judicious fusion strategy, we deduce the process of fusing multiple independent kernels as a dynamic programming problem, which could be well solved with many existing tools and be simply embedded into compiler or runtime system. To reduce the overhead introduced by kernel fusion, we also propose effective method to reduce the usage of shared memory and coordinate the thread space of the kernels to be fused. Detailed experimental evaluation validates that the proposed kernel fusion method could reduce energy consumption without performance loss for several typical kernels.","","978-1-4244-9779-9","10.1109/GreenCom-CPSCom.2010.102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724850","Kernel Fusion;Power Efficiency;Power Optimization;GPGPU","Kernel;Instruction sets;Graphics processing unit;Energy consumption;Hardware;Mathematical model;Dynamic programming","computer graphic equipment;coprocessors;energy conservation;power aware computing","kernel fusion;multithreaded GPU;power efficiency;graphics processing unit;desktop systems;supercomputer systems;dynamic voltage and frequency scaling;CUDA programming model","","59","1","21","","7 Mar 2011","","","IEEE","IEEE Conferences"
"HLanc: Heterogeneous Parallel Implementation of the Implicitly Restarted Lanczos Method","S. Zhang; T. Li; X. Jiao; Y. Wang; Y. Yang","Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China; Coll. of Comput. & Control Eng., Nankai Univ., Tianjin, China","2014 43rd International Conference on Parallel Processing Workshops","11 May 2015","2014","","","403","410","Graphics Processing Unit (GPU) has been used as a ubiquitous accelerator for general purpose computing, such as linear algebra routines and numerical methods. The implicitly restarted Lanczos method (IRLM) is well suited for solving the partial eigenvalue problem for large symmetric sparse matrices, which is important in many real world applications. In this paper, we present the HLanc library, a parallel implementation of IRLM on the heterogeneous CPU-GPU architecture employing the CUDA programming model. The HLanc library is designed with separated heterogeneous parallel IRLM solvers and sparse matrix-vector multiplication (SPMV) operators. The SPMV operators hide the details about the storage of sparse matrices from the IRLM solvers, so the solvers can work with any spare matrix formats. Especially the SPMV operators and IRLM solvers can be combined arbitrarily for achieving the best performance of CPU-GPU heterogeneous system. The HLanc is evaluated using eight sparse matrices with the NVIDIA GTX 480 and GTX TITAN Black GPUs. The results show that HLanc achieves 15 times speedup than the ARPACK library and scales well across different GPU generations.","2332-5690","978-1-4799-5615-9","10.1109/ICPPW.2014.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103478","GPU;CUDA;symmetric sparse matrix;IRLM;SPMV;HLanc","Sparse matrices;Graphics processing units;Eigenvalues and eigenfunctions;Libraries;Computer architecture;Symmetric matrices;Hardware","eigenvalues and eigenfunctions;graphics processing units;mathematics computing;matrix multiplication;parallel architectures;parallel programming;software libraries;sparse matrices;vectors","heterogeneous parallel implementation;implicitly restarted Lanczos method;graphics processing unit;ubiquitous accelerator;general purpose computing;linear algebra routines;numerical methods;partial eigenvalue problem;symmetric sparse matrices;HLanc library;heterogeneous CPU-GPU architecture;CUDA programming model;heterogeneous parallel IRLM solvers;sparse matrix-vector multiplication operators;SPMV operators;CPU-GPU heterogeneous system;NVIDIA GTX 480;GTX TITAN Black GPUs","","2","","26","","11 May 2015","","","IEEE","IEEE Conferences"
"Stargazer: Automated regression-based GPU design space exploration","W. Jia; K. A. Shaw; M. Martonosi","Princeton University, USA; University of Richmond, USA; Princeton University, USA","2012 IEEE International Symposium on Performance Analysis of Systems & Software","26 Apr 2012","2012","","","2","13","Graphics processing units (GPUs) are of increasing interest because they offer massive parallelism for high-throughput computing. While GPUs promise high peak performance, their challenge is a less-familiar programming model with more complex and irregular performance trade-offs than traditional CPUs or CMPs. In particular, modest changes in software or hardware characteristics can lead to large or unpredictable changes in performance. In response to these challenges, our work proposes, evaluates, and offers usage examples of Stargazer<sup>1</sup>, an automated GPU performance exploration framework based on stepwise regression modeling. Stargazer sparsely and randomly samples parameter values from a full GPU design space and simulates these designs. Then, our automated stepwise algorithm uses these sampled simulations to build a performance estimator that identifies the most significant architectural parameters and their interactions. The result is an application-specific performance model which can accurately predict program runtime for any point in the design space. Because very few initial performance samples are required relative to the extremely large design space, our method can drastically reduce simulation time in GPU studies. For example, we used Stargazer to explore a design space of nearly 1 million possibilities by sampling only 300 designs. For 11 GPU applications, we were able to estimate their runtime with less than 1.1% average error. In addition, we demonstrate several usage scenarios of Stargazer.","","978-1-4673-1146-5","10.1109/ISPASS.2012.6189201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189201","","Graphics processing unit;Instruction sets;Mathematical model;Hardware;Space exploration;Runtime;Concurrent computing","computer architecture;graphics processing units;regression analysis","Stargazer;automated regression;GPU design space exploration;graphics processing unit;automated GPU performance exploration framework;stepwise regression modeling","","44","","28","","26 Apr 2012","","","IEEE","IEEE Conferences"
"DART-CUDA: A PGAS Runtime System for Multi-GPU Systems","L. Zhou; K. Fuerlinger","Dept. of Comput. Sci., Ludwig-Maximilians-Univ. (LMU) Munchen, Munich, Germany; Dept. of Comput. Sci., Ludwig-Maximilians-Univ. (LMU) Munchen, Munich, Germany","2015 14th International Symposium on Parallel and Distributed Computing","23 Jul 2015","2015","","","110","119","The Partitioned Global Address Space (PGAS) approach is a promising programming model in high performance parallel computing that combines the advantages of distributed memory systems and shared memory systems. The PGAS model has been used on a variety of hardware platforms in the form of PGAS programming languages like Unified Parallel C (UPC), Chapel and Fortress. However, in spite of the increasing adoption in distributed and shared memory systems, the extension of the PGAS model to accelerator platforms is still not well supported. To exploit the immense computational power of multi-GPU systems, this work is concerned with the design and implementation of a Partitioned Global Address Space model for multi-GPU systems. Several issues related to the combination of logically separate GPU memories on multiple graphic cards are addressed. Furthermore, the execution model of modern GPU architectures is studied and a task creation mechanism with load balancing is proposed. Our work is implemented in the context of the DASH project, a C++ template library that realizes PGAS semantics through operator overloading. Experimental results suggest promising performance of the design and its implementation.","2379-5352","978-1-4673-7148-3","10.1109/ISPDC.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165137","PGAS;Partitioned Global Address Space;MultiGPU systems;CUDA;Heterogeneous computing","Graphics processing units;Resource management;Electronics packaging;Runtime;Programming;Computational modeling;Kernel","C++ language;distributed memory systems;graphics processing units;parallel processing;resource allocation;software libraries","C++ template library;DASH project;load balancing;task creation mechanism;GPU architecture;multiple graphic card;GPU memories;PGAS programming language;Chapel;UPC;unified parallel C;shared memory system;distributed memory system;high performance parallel computing;PGAS approach;partitioned global address space approach;multiGPU system;PGAS runtime system;DART-CUDA","","2","","11","","23 Jul 2015","","","IEEE","IEEE Conferences"
"GPU acceleration of the dynamics routine in the HIRLAM weather forecast model","V. T. Vu; G. Cats; L. Wolters","Leiden Institute of Advanced Computer Science, Leiden University, 2333 CA, The Netherlands; Royal Netherlands, Meteorological Institute, 3730 AE De Bilt, The Netherlands; Leiden Institute of Advanced Computer Science, Leiden University, 2333 CA, The Netherlands","2010 International Conference on High Performance Computing & Simulation","12 Aug 2010","2010","","","31","38","Programmable graphics processing units (GPUs) nowadays offer very high performance computing power at relatively low hardware cost and power consumption. In this paper, we present the implementation of the dynamics routine of the HIRLAM weather forecast model on the NVIDIA GeForce 9800 GX2 GPU card using the Compute Unified Device Architecture (CUDA) as parallel programming model. We converted the original Fortran to C and CUDA by hand, straightforwardly, without much concern about optimization. On a single GPU, we observe speed-ups by an order of magnitude over our hosting CPU (Intel quad core, 1998 MHz). This includes the relatively very costly copying of data between GPU and CPU memories. Calculation times proper decreased by a factor of 2000. A single GPU, however, has not enough memory for practical use. Therefore, we investigated a parallel implementation on 4 GPUs. We found a parallel speed-up of 3.6, which is not very promising if memory limitations force the use of many GPUs in parallel. We discuss several options to solve this issue.","","978-1-4244-6830-0","10.1109/HPCS.2010.5547152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5547152","GPGPU;Large Scale Scientific Computing;Parallelization of Simulation;CUDA;Numerical Weather Prediction model;Dynamics","Graphics processing unit;Kernel;Weather forecasting;Predictive models","computer graphic equipment;coprocessors;geophysics computing;parallel programming;weather forecasting","GPU acceleration;dynamics routine;programmable graphics processing unit;high performance computing power;HIRLAM weather forecast model;NVIDIA GeForce 9800 GX2 GPU card;Compute Unified Device Architecture;CUDA;parallel programming;Intel quad core","","6","","18","","12 Aug 2010","","","IEEE","IEEE Conferences"
"CUDA-Based Computation for Visual Odometry","S. -H. Liu; C. -C. Hsu; W. -Y. Wang; C. -H. Lin","Department of Electrical Engineering, National Taiwan Normal University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan Normal University, Taipei, Taiwan; Department of Mechatronic Engineering, National Taiwan Normal University, Taipei, Taiwan; Department of Mechatronic Engineering, National Taiwan Normal University, Taipei, Taiwan","2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)","13 Dec 2018","2018","","","64","65","An enhanced visual odometry (VO) system is proposed to improve the accuracy of pose estimation based on a corrected model, and the matching algorithm is implemented on graphical processing units (GPUs) so that the computation can be accelerated in parallel and in real-time using the compute unified device architecture (CUDA) programming model. To evaluate the performance of the proposed approach, an ASUS Xtion 3D camera, laptop, and NVIDIA TX2 are employed to conduct extensive experiments. The experimental results show that compared with the traditional VO algorithm, the proposed approach gives better results over the traditional VO algorithm.","2378-8143","978-1-5386-6309-7","10.1109/GCCE.2018.8574869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574869","Visual odometry;GPU;CUDA","Graphics processing units;Cameras;Feature extraction;Pose estimation;Computational modeling;Visual odometry;Computational efficiency","cameras;computerised instrumentation;distance measurement;graphics processing units;parallel architectures;pose estimation;robot vision","graphical processing units;compute unified device architecture programming model;ASUS Xtion 3D camera;NVIDIA TX2;CUDA-based computation;matching algorithm;visual odometry system;VO algorithm;GPU","","","","6","","13 Dec 2018","","","IEEE","IEEE Conferences"
"Empowering Visual Categorization With the GPU","K. E. A. van de Sande; T. Gevers; C. G. M. Snoek","Intelligent Systems Lab Amsterdam, Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands; Intelligent Systems Lab Amsterdam, Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands; Intelligent Systems Lab Amsterdam, Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands","IEEE Transactions on Multimedia","17 Jan 2011","2011","13","1","60","70","Visual categorization is important to manage large collections of digital images and video, where textual metadata is often incomplete or simply unavailable. The bag-of-words model has become the most powerful method for visual categorization of images and video. Despite its high accuracy, a severe drawback of this model is its high computational cost. As the trend to increase computational power in newer CPU and GPU architectures is to increase their level of parallelism, exploiting this parallelism becomes an important direction to handle the computational cost of the bag-of-words approach. When optimizing a system based on the bag-of-words approach, the goal is to minimize the time it takes to process batches of images. this paper, we analyze the bag-of-words model for visual categorization in terms of computational cost and identify two major bottlenecks: the quantization step and the classification step. We address these two bottlenecks by proposing two efficient algorithms for quantization and classification by exploiting the GPU hardware and the CUDA parallel programming model. The algorithms are designed to (1) keep categorization accuracy intact, (2) decompose the problem, and (3) give the same numerical results. In the experiments on large scale datasets, it is shown that, by using a parallel implementation on the Geforce GTX260 GPU, classifying unseen images is 4.8 times faster than a quad-core CPU version on the Core i7 920, while giving the exact same numerical results. In addition, we show how the algorithms can be generalized to other applications, such as text retrieval and video retrieval. Moreover, when the obtained speedup is used to process extra video frames in a video retrieval benchmark, the accuracy of visual categorization is improved by 29%.","1941-0077","","10.1109/TMM.2010.2091400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625914","Bag-of-words;computational efficiency;General-Purpose computation on Graphics Processing Units (GPGPU);image classification;image/video retrieval;multicore processing;parallel processing;support vector machines","Visualization;Graphics processing unit;Kernel;Computational modeling;Feature extraction;Vector quantization;Acceleration","computer graphic equipment;coprocessors;image classification;image segmentation;meta data;parallel programming;video signal processing","visual categorization;GPU;digital image;digital video;textual metadata;parallel architecture;quantization step;classification step;CUDA;parallel programming model;Geforce GTX260;image classification;Core i7 920;computational efficiency;graphics processing unit;general purpose computation","","51","5","51","IEEE","11 Nov 2010","","","IEEE","IEEE Journals"
"Parallelization and Optimization of SIFT on GPU Using CUDA","Z. Yonglong; M. Kuizhi; J. Xiang; D. Peixiang","NA; NA; Xian Jiaotong Univ., Xian, China; NA","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","12 Jun 2014","2013","","","1351","1358","Scale-invariant feature transform (SIFT) based feature extraction algorithm is widely applied to extract features from images, and it is very attractive to accelerate these SIFT based algorithms on GPU. In this paper, we present several parallel computing strategies, implement and optimize the SIFT algorithm using CUDA programming model on GPU. Each stage of SIFT is analyzed in detail to choose the parallel strategy. On the basis of the elementary CUDA-SIFT and CUDA architecture, we optimize the implementation from several aspects to speedup the CUDA-SIFT. Experimental results demonstrate that our implementation after optimization is 2.5 times faster than previous optimization, and our CUDA based SIFT can run at the speed of 20 frames per second on most images with 1280x 960 resolution in the test. Using 1920 x1440 image to test, we have obtained a speed of 11 frames per second on average, which is about 60 times faster than the CPU implementation of SIFT. In short, our implementation obtains appropriate accuracy and higher efficiency compared to CPU implementations and other GPU implementations, which is attributed to our dedicated optimization strategies.","","978-0-7695-5088-6","10.1109/HPCC.and.EUC.2013.192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832074","","Graphics processing units;Instruction sets;Feature extraction;Convolution;Histograms;Optimization;Vectors","graphics processing units;parallel architectures;parallel processing;transforms","GPU;SIFT optimization;SIFT parallelization;scale invariant feature transform;feature extraction algorithm;parallel computing strategies;CUDA programming model;parallel strategy;CUDA architecture;CUDA-SIFT;CPU implementation","","5","","13","","12 Jun 2014","","","IEEE","IEEE Conferences"
"Simplifying the multi-GPU programming of a hyperspectral image registration algorithm","J. Fernàndez-Fabeiro; A. Gonzalez-Escribano; D. R. Llanos","Universidad de Valladolid,Departamento de Informaticá,Valladolid,Spain; Universidad de Valladolid,Departamento de Informaticá,Valladolid,Spain; Universidad de Valladolid,Departamento de Informaticá,Valladolid,Spain","2019 International Conference on High Performance Computing & Simulation (HPCS)","9 Sep 2020","2019","","","11","18","Hyperspectral image registration is a relevant task for real-time applications like environmental disasters management or search and rescue scenarios. Traditional algorithms for this problem were not really devoted to real-time performance. The HYFMGPU algorithm arose as a high-performance GPU-based solution to solve such a lack. Nevertheless, a single-GPU solution is not enough, as sensors are evolving and then generating images with finer resolutions and wider wavelength ranges. An MPI+CUDA multi-GPU implementation of HYFMGPU was previously presented. However, this solution shows the programming complexity of combining MPI with an accelerator programming model. In this paper we present a new and more abstract programming approach for this type of applications, which provides a high efficiency while simplifying the programming of the multi-device parts of the code. The solution uses Hitmap, a library to ease the programming of parallel applications based on distributed arrays. It uses a more algorithm-oriented approach than MPI, including abstractions for the automatic partition and mapping of arrays at runtime with arbitrary granularity, as well as techniques to build flexible communication patterns that transparently adapt to the data partitions. We show how these abstractions apply to this application class. We present a comparison of development effort metrics between the original MPI implementation and the one based on Hitmap, with reductions of up to 95% for the Halstead score in specific work redistribution steps. We finally present experimental results showing that these abstractions are internally implemented in a high efficient way that can reduce the overall performance time in up to 37% comparing with the original MPI implementation.","","978-1-7281-4484-9","10.1109/HPCS48598.2019.9188064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188064","Hyperspectral imaging;image registration;parallel libraries;distributed arrays;MPI;CUDA;GPGPU","Programming;Graphics processing units;Hyperspectral imaging;Real-time systems;Libraries;Principal component analysis","application program interfaces;emergency management;graphics processing units;hyperspectral imaging;image registration;message passing;parallel architectures","multiGPU programming;hyperspectral image registration algorithm;real-time applications;environmental disasters management;search and rescue scenario;real-time performance;HYFMGPU algorithm;high-performance GPU-based solution;programming complexity;accelerator programming model;abstract programming approach;multidevice parts;Hitmap;parallel applications;distributed arrays;algorithm-oriented approach;application class;performance time","","","","29","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Two-stage Asynchronous Iterative Solvers for multi-GPU Clusters","P. Nayak; T. Cojean; H. Anzt","Karlsruhe Institute of Technology,Germany; Karlsruhe Institute of Technology,Germany; Karlsruhe Institute of Technology,Germany","2020 IEEE/ACM 11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA)","31 Dec 2020","2020","","","9","18","Given the trend of supercomputers accumulating much of their compute power in GPU accelerators composed of thousands of cores and operating in streaming mode, global synchronization points become a bottleneck, severely confining the performance of applications. In consequence, asynchronous methods breaking up the bulk-synchronous programming model are becoming increasingly attractive. In this paper, we study a GPU-focused asynchronous version of the Restricted Additive Schwarz (RAS) method that employs preconditioned Krylov subspace methods as subdomain solvers. We analyze the method for various parameters such as local solver tolerance and iteration counts. Leveraging the multi-GPU architecture on Summit, we show that these two-stage methods are more memory and time efficient than asynchronous RAS using direct solvers. We also demonstrate the superiority over synchronous counterparts, and present results using one-sided CUDA-aware MPI on up to 36 NVIDIA V100 GPUs.","","978-1-6654-2270-3","10.1109/ScalA51936.2020.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308758","Asynchronous iterative methods;Schwarz methods;GPUs;Krylov subspace solvers","Iterative methods;Convergence;Synchronization;Additives;Symmetric matrices;Sparse matrices;Programming","graphics processing units;hardware accelerators;iterative methods;multiprocessing systems;parallel architectures;parallel machines","multiGPU clusters;GPU accelerators;global synchronization points;GPU-focused asynchronous version;restricted additive Schwarz method;preconditioned Krylov subspace methods;subdomain solvers;local solver tolerance;iteration counts;multiGPU architecture;two-stage asynchronous iterative solvers;supercomputers;Summit;asynchronous RAS;one-sided CUDA-aware MPI;NVIDIA V100 GPUs","","","","14","","31 Dec 2020","","","IEEE","IEEE Conferences"
"Parallel Fast Pencil Drawing Generation Algorithm Based on GPU","J. Qiu; B. Liu; J. He; C. Liu; Y. Li","College of Information Engineering, Northwest A&F University, Yangling, China; College of Information Engineering, Northwest A&F University, Yangling, China; College of Mathematics and Computer Science, Yan’an University, Yan’an, China; College of Information Engineering, Northwest A&F University, Yangling, China; School of Computer Science and Technology, Xi’an University of Science and Technology, Xi’an, China","IEEE Access","5 Jul 2019","2019","7","","83543","83555","With the development of image processing technology, pencil drawing has been widely used in video games and mobile phone applications. However, the existing pencil drawing algorithms require a large amount of time to convert a real picture into a pencil drawing; hence, it is difficult to apply them to real-time systems. This paper proposes a parallel fast pencil drawing generation algorithm based on the graphics processing unit (GPU) to accelerate the real-time rendering process of sketch painting. The parallelism of the pencil drawing generation algorithm is identified via a theoretical analysis at first. Then, sub-algorithms of the sequential algorithm are designed in parallel using the compute unified device architecture (CUDA) programming model and executed via thread-level parallel techniques. Furthermore, an optimal cache pattern of data that reduce the access time of the most frequently used data is structured using shared memory and constant memory. Finally, task-level parallelism is achieved by the CUDA stream technology, which overlaps independent sub-tasks for further acceleration. On the CUDA platform, the experimental results demonstrate that the proposed parallel algorithm can achieve a significant increase in speedup. The proposed algorithm achieves a performance improvement of 448.59 times compared with the sequential algorithm, on 2560×1920-resolution images, and maintains a high degree of similarity with the real pencil paintings. Hence, the proposed algorithm is suitable for real-time pencil drawing rendering and has promising application prospects in non-photorealistic rendering.","2169-3536","","10.1109/ACCESS.2019.2924658","National Natural Science Foundation of China(grant numbers:61602388); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2017JM6059); Fundamental Research Funds for the Central Universities(grant numbers:2452019064); Postdoctoral Science Foundation of Shaanxi Province of China(grant numbers:2016BSHEDZZ121); China Postdoctoral Science Foundation(grant numbers:2017M613216,2018M633585); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2018JQ6060); Key Program of the National Natural Science Foundation of China(grant numbers:61834005); Fundamental Research Funds for the Central Universities(grant numbers:2452016081); Doctoral Starting up Foundation of Yan’an University(grant numbers:YDBK2019-06); Northwest A and F University(grant numbers:2201810712307); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744506","Non-photorealistic rendering;pencil drawing;parallel algorithm;GPU platform;convolution operation;CUDA","Graphics processing units;Rendering (computer graphics);Instruction sets;Histograms;Parallel processing;STEM;Real-time systems","graphics processing units;image resolution;parallel architectures;real-time systems;rendering (computer graphics)","parallel algorithm;sequential algorithm;parallel fast pencil drawing generation algorithm;image processing technology;real-time rendering process;thread-level parallel techniques;task-level parallelism;CUDA programming model;compute unified device architecture;real-time pencil drawing rendering;pencil drawing algorithms","","3","","37","CCBY","24 Jun 2019","","","IEEE","IEEE Journals"
"An Asynchronous Parallel Implementation of Multilevel Fast Multipole Algorithm on GPU Cluster for 3D Electromagnetic Scattering Problems","R. -P. Xi; W. -J. He; M. -L. Yang; X. -Q. Sheng","Center for Electromagnetic Simulation, School of Information and Electronics, Beijing Institute of Technology,Beijing,China,100081; Center for Electromagnetic Simulation, School of Information and Electronics, Beijing Institute of Technology,Beijing,China,100081; Center for Electromagnetic Simulation, School of Information and Electronics, Beijing Institute of Technology,Beijing,China,100081; Center for Electromagnetic Simulation, School of Information and Electronics, Beijing Institute of Technology,Beijing,China,100081","2021 International Applied Computational Electromagnetics Society (ACES-China) Symposium","8 Nov 2021","2021","","","1","2","This paper presents a CPU/GPU asynchronous computing pattern based improved parallel multilevel fast multipole algorithm (MLFMA) for 3D electromagnetic scattering problems on GPU Cluster. In the presented parallel implementation, the matrix assembly process of the MLFMA is decomposed into CPU execution and GPU execution two parts. The former is performed on CPU using OpenMP multi-threading programming model, while the latter is performed on GPU with CUDA programming model. The execution time between the two parts is overlapped by using the feature of asynchronous execution between CPU and GPU. The performance of the proposed parallel implementation is investigated in terms of accuracy and efficiency. Numerical results show that, with the proposed parallel approach, over 10% speed-up can be attained, compared with the original parallel implementation.","","978-1-7335096-1-9","10.23919/ACES-China52398.2021.9581392","National Key R&D Program of China(grant numbers:2017YFB0202500); NSFC(grant numbers:61971034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581392","Multilevel fast multipole algorithm;OpenMP;CUDA;Asynchronous Computing;scattering","Three-dimensional displays;Runtime;Computational modeling;Electromagnetic scattering;Graphics processing units;Programming;Computational electromagnetics","","","","","","4","","8 Nov 2021","","","IEEE","IEEE Conferences"
"Accelerating DNN Inference with GraphBLAS and the GPU","X. Wang; Z. Lin; C. Yang; J. D. Owens","University of California, Davis,Department of Computer Science,Davis,California,95616; University of California, Davis,Department of Electrical & Computer Engineering,Davis,California,95616; University of California, Davis,Department of Electrical & Computer Engineering,Davis,California,95616; University of California, Davis,Department of Electrical & Computer Engineering,Davis,California,95616","2019 IEEE High Performance Extreme Computing Conference (HPEC)","28 Nov 2019","2019","","","1","6","This work addresses the 2019 Sparse Deep Neural Network Graph Challenge with an implementation of this challenge using the GraphBLAS programming model. We demonstrate our solution to this challenge with GraphBLAST, a GraphBLAS implementation on the GPU, and compare it to SuiteSparse, a GraphBLAS implementation on the CPU. The GraphBLAST implementation is 1.94 × faster than Suite-Sparse; the primary opportunity to increase performance on the GPU is a higher-performance sparse-matrix-times-sparse-matrix (SpGEMM) kernel.","2643-1971","978-1-7281-5020-8","10.1109/HPEC.2019.8916498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8916498","","Sparse matrices;Graphics processing units;Neurons;Matlab;Runtime;Neural networks;Memory management","graph theory;graphics processing units;inference mechanisms;multiprocessing systems;neural nets;sparse matrices","GraphBLAS implementation;GPU;GraphBLAST implementation;Suite-Sparse;DNN Inference;2019 Sparse Deep Neural Network Graph Challenge;GraphBLAS programming model;sparse-matrix-times-sparse-matrix kernel","","4","","14","","28 Nov 2019","","","IEEE","IEEE Conferences"
"CNN based high performance computing for real time image processing on GPU","S. Potluri; A. Fasih; L. K. Vutukuru; F. A. Machot; K. Kyamakya","Transportation Informatics Group, Alpen-Adria University of Klagenfurt, Klagenfurt, Austria; Transportation Informatics Group, Alpen-Adria University of Klagenfurt, Klagenfurt, Austria; Transportation Informatics Group, Alpen-Adria University of Klagenfurt, Klagenfurt, Austria; Transportation Informatics Group, Alpen-Adria University of Klagenfurt, Klagenfurt, Austria; Transportation Informatics Group, Alpen-Adria University of Klagenfurt, Klagenfurt, Austria","Proceedings of the Joint INDS'11 & ISTET'11","22 Sep 2011","2011","","","1","7","Many of the basic image processing tasks suffer from processing overhead to operate over the whole image. In real time applications the processing time is considered as a big obstacle for its implementations. A High Performance Computing (HPC) platform is necessary in order to solve this problem. The usage of hardware accelerator make the processing time low. In recent developments, the Graphics Processing Unit (GPU) is being used in many applications. Along with the hardware accelerator a proper choice of the computing algorithm makes it an added advantage for fast processing of images. The Cellular Neural Network (CNN) is a large-scale nonlinear analog circuit able to process signals in real time [1]. In this paper, we develop a new design in evaluation of image processing algorithms on the massively parallel GPUs with CNN implementation using Open Computing Language (OpenCL) programming model. This implementation uses the Discrete Time CNN (DT-CNN) model which is derived from originally proposed CNN model. The inherent massive parallelism of CNN along with GPUs makes it an advantage for high performance computing platform [2]. The advantage of OpenCL makes the design to be portable on all the available graphics processing devices and multi core processors. Performance evaluation is done in terms of execution time with both device (i.e. GPU) and host (i.e. CPU).","2324-8335","978-1-4577-0762-9","10.1109/INDS.2011.6024781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024781","Image processing;Hardware accelerators;Cellular Neural Networks;GPUs;High Performance Computing;OpenCL","Graphics processing unit;Mathematical model;Kernel;Image processing;Equations;Computer architecture","analogue circuits;cellular neural nets;computer graphic equipment;coprocessors;image processing;multiprocessing systems;parallel programming","CNN based high performance computing;real time image processing;hardware accelerator;graphical processing unit;cellular neural network;large-scale nonlinear analog circuit;signal processing;parallel GPU;open computing language programming model;discrete time CNN model;DT-CNN model;OpenCL;multicore processor;performance evaluation","","23","","17","","22 Sep 2011","","","IEEE","IEEE Conferences"
"Accelerating scientific applications using GPU's","M. Taher","Ain Shams University, Cairo, Egypt","2009 4th International Design and Test Workshop (IDT)","2 Feb 2010","2009","","","1","6","Graphics processing units (GPUs) have emerged as a powerful platform for high-performance computation. They have been successfully used to accelerate many scientific workloads. Typically, the computationally intensive parts of the application are offloaded to the GPU, which serves as the CPU's parallel coprocessor. The key to effective utilization of GPUs for scientific computing is the design and implementation of efficient data-parallel algorithms that can scale to hundreds of tightly coupled processing units. Many compute intensive scientific applications are well suited to GPUs, due to their extensive computational requirements, and because they lend themselves to parallel processing implementations. The use of multiple GPUs can bring even more computational power to bear on highly parallelizable computational problems. This paper discusses performance results for some fundamental cores of scientific applications such as fft, smith-waterman sequence alignment algorithm, and data encryption standard (DES) on the Nvidia GPUs using the CUDA programming model. Results have demonstrated acceleration up to 25 times speedup using a single G80 Nvidia GPU.","2162-061X","978-1-4244-5750-2","10.1109/IDT.2009.5404114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404114","","Acceleration;Yarn;Graphics;Concurrent computing;Hardware;Parallel processing;Computer architecture;Coprocessors;Computer applications;Cryptography","computer graphic equipment;coprocessors","graphics processing units;CPU parallel coprocessor;scientific computing;efficient data parallel algorithms;parallel processing;smith-waterman sequence alignment algorithm;data encryption standard;CUDA programming model;Nvidia GPU","","2","","11","","2 Feb 2010","","","IEEE","IEEE Conferences"
"A large-scale cross-architecture evaluation of thread-coarsening","A. Magni; C. Dubach; M. F. P. O'Boyle","University of Edinburgh, UK; University of Edinburgh, UK; University of Edinburgh, UK","SC '13: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","14 Aug 2014","2013","","","1","11","OpenCL has become the de-facto data parallel programming model for parallel devices in today's high-performance supercomputers. OpenCL was designed with the goal of guaranteeing program portability across hardware from different vendors. However, achieving good performance is hard, requiring manual tuning of the program and expert knowledge of each target device. In this paper we consider a data parallel compiler transformation - thread-coarsening - and evaluate its effects across a range of devices by developing a source-to-source OpenCL compiler based on LLVM. We thoroughly evaluate this transformation on 17 benchmarks and five platforms with different coarsening parameters giving over 43,000 different experiments. We achieve speedups over 9x on individual applications and average speedups ranging from 1.15x on the Nvidia Kepler GPU to 1.50x on the AMD Cypress GPU. Finally, we use statistical regression to analyse and explain program performance in terms of hardware-based performance counters.","2167-4337","978-1-4503-2378-9","10.1145/2503210.2503268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877444","GPU;OpenCL;Thread coarsening;Regression trees","Instruction sets;Performance evaluation;Kernel;Hardware;Benchmark testing;Graphics processing units;Multicore processing","graphics processing units;multi-threading;program compilers;regression analysis;software architecture;software performance evaluation;software portability","large-scale cross-architecture evaluation;thread-coarsening parameters;de-facto data parallel programming model;high-performance supercomputers;program portability;data parallel compiler trans- formation;source-to-source OpenCL compiler;LLVM;Nvidia Kepler GPU;AMD Cypress GPU;statistical regression;program performance;hardware-based performance counters","","24","","28","","14 Aug 2014","","","IEEE","IEEE Conferences"
"StreamMR: An Optimized MapReduce Framework for AMD GPUs","M. Elteir; H. Lin; W. Feng; T. Scogland","City of Sci. Researches & Technol. Applic., Egypt; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA","2011 IEEE 17th International Conference on Parallel and Distributed Systems","2 Jan 2012","2011","","","364","371","MapReduce is a programming model from Google that facilitates parallel processing on a cluster of thousands of commodity computers. The success of MapReduce in cluster environments has motivated several studies of implementing MapReduce on a graphics processing unit (GPU), but generally focusing on the NVIDIA GPU. Our investigation reveals that the design and mapping of the MapReduce framework needs to be revisited for AMD GPUs due to their notable architectural differences from NVIDIA GPUs. For instance, current state-of-the-art MapReduce implementations employ atomic operations to coordinate the execution of different threads. However, atomic operations can implicitly cause inefficient memory access, and in turn, severely impact performance. In this paper, we propose Streamer, an OpenCL MapReduce framework optimized for AMD GPUs. With efficient atomic-free algorithms for output handling and intermediate result shuffling, Stream MR is superior to atomic-based MapReduce designs and can outperform existing atomic-free MapReduce implementations by nearly five-fold on an AMD Radeon HD 5870.","1521-9097","978-0-7695-4576-9","10.1109/ICPADS.2011.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121299","atomics;parallel computing;AMD GPU;GPGPU;MapReduce;Mars;MapCG;OpenCL","Instruction sets;Kernel;Mars;Graphics processing unit;Programming;High definition video;Optimization","graphics processing units;parallel processing;workstation clusters","StreamMR;optimized MapReduce framework;AMD GPU;programming model;parallel processing;commodity computers;cluster environments;graphics processing unit;NVIDIA GPU;OpenCL MapReduce framework;atomic-free algorithm","","20","1","19","","2 Jan 2012","","","IEEE","IEEE Conferences"
"Evaluating OpenMP 4.0's Effectiveness as a Heterogeneous Parallel Programming Model","M. Martineau; S. McIntosh-Smith; W. Gaudin","HPC Group, Univ. of Bristol, Bristol, UK; HPC Group, Univ. of Bristol, Bristol, UK; Atomic Weapons Establ., Aldermaston, UK","2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","4 Aug 2016","2016","","","338","347","Although the OpenMP 4.0 standard has been available since 2013, support for GPUs has been absent up until very recently, with only a handful of experimental compilers available. In this work we evaluate the performance of Cray's new NVIDIA GPU targeting implementation of OpenMP 4.0, with the mini-apps TeaLeaf, CloverLeaf and BUDE. We successfully port each of the applications, using a simple and consistent design throughout, and achieve performance on an NVIDIA K20X that is comparable to Cray's OpenACC in all cases. BUDE, a compute bound code, required 2.2x the runtime of an equivalently optimised CUDA code, which we believe is caused by an inflated frequency of control flow operations and less efficient arithmetic optimisation. Impressively, both TeaLeaf and CloverLeaf, memory bandwidth bound codes, only required 1.3x the runtime of hand-optimised CUDA implementations. Overall, we find that OpenMP 4.0 is a highly usable open standard capable of performant heterogeneous execution, making it a promising option for scientific application developers.","","978-1-5090-3682-0","10.1109/IPDPSW.2016.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529889","high performance computing;parallel computing;application programming interfaces;OpenMP;performance portability","Standards;Graphics processing units;Performance evaluation;Complexity theory;Parallel processing;Parallel programming;Runtime","application program interfaces;graphics processing units;optimising compilers;parallel architectures;parallel programming;software performance evaluation","OpenMP 4.0 effectiveness evaluation;heterogeneous parallel programming model;OpenMP 4.0 standard;compilers;NVIDIA GPU targeting implementation;mini-apps;TeaLeaf;CloverLeaf;BUDE;NVIDIA K20X;OpenACC;BUDE;compute bound code;equivalently optimised CUDA code;control flow operations;arithmetic optimisation;memory bandwidth bound codes;hand-optimised CUDA implementations;heterogeneous execution;scientific application developers","","18","","19","","4 Aug 2016","","","IEEE","IEEE Conferences"
"The Research on Parallel Optimization of SAR Imaging R-D Algorithm Based on CUDA","P. Wei; J. Du; S. Sui; Y. Chen","Luoyang Electronic Equipment Test Center,LEETC, Luoyang, China; Luoyang Electronic Equipment Test Center,LEETC, Luoyang, China; Luoyang Electronic Equipment Test Center,LEETC, Luoyang, China; Luoyang Electronic Equipment Test Center,LEETC, Luoyang, China","2018 10th International Conference on Communication Software and Networks (ICCSN)","11 Oct 2018","2018","","","526","530","Synthetic Aperture Radar (SAR) imaging technology is widely used in the field of remote sensing observation, navigation positioning and so on, SAR imaging is large in data scale and long in operating time. Based on the Compute Unified Device Architecture (CUDA) programming model, the SAR imaging R-D algorithm is designed and implemented for parallel optimization on the CPU-GPU heterogeneous platform and tested on the GPU Tesla K20. The test shows that the efficiency of the core steps of R-D algorithm has been greatly improved.","2472-8489","978-1-5386-7223-5","10.1109/ICCSN.2018.8488250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488250","SAR;parallel optimization;CUDA programming model","Graphics processing units;Azimuth;Imaging;Instruction sets;Radar polarimetry;Radar imaging;Programming","graphics processing units;microprocessor chips;parallel architectures;radar imaging;synthetic aperture radar","parallel optimization;R-D algorithm;CUDA;Synthetic Aperture Radar imaging technology;remote sensing observation;navigation positioning;SAR imaging;data scale;Compute Unified Device Architecture;CPU-GPU heterogeneous platform","","","","13","","11 Oct 2018","","","IEEE","IEEE Conferences"
"Exploiting Task-Parallelism on GPU Clusters via OmpSs and rCUDA Virtualization","A. Castelló; R. Mayo; J. Planas; E. S. Quintana-Ortí","Dept. de Ing. y Cienc. de Comput., Univ. Jaume I, Castellon, Spain; Dept. de Ing. y Cienc. de Comput., Univ. Jaume I, Castellon, Spain; Barcelona Supercomput. Center, Barcelona, Spain; Dept. de Ing. y Cienc. de Comput., Univ. Jaume I, Castellon, Spain","2015 IEEE Trustcom/BigDataSE/ISPA","3 Dec 2015","2015","3","","160","165","OmpSs is a task-parallel programming model consisting of a reduced collection of OpenMP-like directives, a front-end compiler, and a runtime system. This directive-based programming interface helps developers accelerate their application's execution, e.g. in a cluster equipped with graphics processing units (GPUs), with a low programming effort. On the other hand, the virtualization package rCUDA provides seamless and transparent remote access to any CUDA GPU in a cluster, via the CUDA Driver and Runtime programming interfaces. In this paper we investigate the hurdles and practical advantages of combining these two technologies. Our experimental study targets two cluster configurations: a system where all the GPUs are located into a single cluster node, and a cluster with the GPUs distributed among the nodes. Two applications, the N-body particle simulation and the Cholesky factorization of a dense matrix, are employed to expose the bottlenecks and performance of a remote virtualization solution applied to these two OmpSs task-parallel codes.","","978-1-4673-7952-6","10.1109/Trustcom.2015.626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345642","Task parallelism;graphics processing units (GPUs);OmpSs;CUDA;remote virtualization","Graphics processing units;Servers;Virtualization;Runtime;Message systems;Programming;Instruction sets","device drivers;graphics processing units;matrix decomposition;parallel architectures;parallel programming;pattern clustering;program compilers;task analysis;virtualisation","OmpSs task parallel code;task parallel programming model;OpenMP-like directive;frontend compiler;runtime system;directive-based programming interface;application execution;graphics processing unit;GPU;rCUDA;CUDA driver;runtime programming interface;cluster configuration;single cluster node;N-body particle simulation;Cholesky factorization;dense matrix;remote virtualization package","","1","","20","","3 Dec 2015","","","IEEE","IEEE Conferences"
"A Translation Framework for Virtual Execution Environment on CPU/GPU Architecture","G. Dong; K. Chen; E. Zhu; Y. Zhang; Z. Qi; H. Guan","Sch. of Software, Shanghai Jiaotong Univ., Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiaotong Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Shanghai Jiaotong Univ., Shanghai, China; Sch. of Software, Shanghai Jiaotong Univ., Shanghai, China; Sch. of Software, Shanghai Jiaotong Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Shanghai Jiaotong Univ., Shanghai, China","2010 3rd International Symposium on Parallel Architectures, Algorithms and Programming","17 Feb 2011","2010","","","130","137","GPUs are many-core processors with tremendous computational power. However, as automatic parallelization has not been realized yet, developing high-performance parallel code for GPUs is still very challenging. The paper presents a novel translation framework designed for virtual execution environment based on CPU/GPU architecture. It addresses two major challenges of taking advantage of general purpose computation on graphics processing units (GPGPU) to improve performance: no rewriting the existing source code and resolving binary compatibility issues between different GPUs. The translation framework uses semi-automatic parallelization technology to port existing code to explicitly parallel programming models. It not only offers a mapping strategy from X86 platform to CUDA programming model, but also synchronizes the execution between the CPU and the GPUs. The input to our translation framework is parallelizable part of the program within binary code. With an additional information related to the parallelizable part, the translation framework transforms the sequential code into PTX code and execute it on GPUs. Experimental results on several programs from CUDA SDK Code Samples and Parboil Benchmark Suite show that our translation framework could achieve very high performance, even up to several tens of times speedup over the X86 native version.","2168-3042","978-1-4244-9482-8","10.1109/PAAP.2010.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715074","GPGPU;Parallelization;Translator;CUDA","Graphics processing unit;Registers;Kernel;Instruction sets;Driver circuits;Programming;Computer architecture","computer graphic equipment;coprocessors;multiprocessing systems;parallel architectures","virtual execution environment;CPU-GPU Architecture;many core processors;X86 platform;CUDA programming model;CUDA SDK code samples;Parboil benchmark suite","","","1","25","","17 Feb 2011","","","IEEE","IEEE Conferences"
"Parallel quantum computer simulation on the GPU","A. Amariutei; S. Caraiman","Faculty of Automatic Control and Computer Engineering, Technical University of Iasi, Romania; Faculty of Automatic Control and Computer Engineering, Technical University of Iasi, Romania","15th International Conference on System Theory, Control and Computing","28 Nov 2011","2011","","","1","6","Simulation of quantum computers using classical computers is a hard problem with high memory and computational requirements. Parallelization can alleviate this problem, allowing the simulation of more qubits at the same time or the same number of qubits to be simulated in less time. A promising approach is to exploit the high performance computing capabilities provided by the latest graphical processing units. In this paper we present a parallel implementation of the QC-lib quantum computer simulator on the GPU using the CUDA programming model. The proposed scheme for partitioning the terms that describe the state of a quantum register takes advantage of the specific characteristics of the CUDA memory spaces and allows for an efficient parallelization of the general singe qubit operator. Experimental results indicate that very good speed-ups can be obtained in contrast with the sequential implementation.","","978-973-621-321-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6085728","","Quantum computing;Graphics processing unit;Computational modeling;Registers;Computers;Instruction sets;Quantum cascade lasers","computer graphic equipment;coprocessors;parallel programming;quantum computing","parallel quantum computer simulation;GPU;high performance computing capability;graphical processing unit;parallel implementation;QC-lib quantum computer simulator;CUDA programming model;quantum register","","","","33","","28 Nov 2011","","","IEEE","IEEE Conferences"
"Solving N-Queens problem on GPU architecture using OpenCL with special reference to synchronization issues","K. Thouti; S. R. Sathe","Dept. of Computer Science & Engg., Visvesvaraya National Institute of Technology, Nagpur, India; Dept. of Computer Science & Engg., Visvesvaraya National Institute of Technology, Nagpur, India","2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing","7 Feb 2013","2012","","","806","810","The N-Queens problem is to place N queens on an N × N chessboard such that no two queens attack each other. General purpose computing on graphics processing units (GPGPU) is fast becoming a common feature of high performance computing. This paper investigates cost of finding the solutions to N-Queens problem on GPGPU architecture using OpenCL programming model. We extensively analyze the N-Queen problem with respect to local, global memory parameters and atomicity and synchronization issues in OpenCL and its effects on performance. Experimental results are shown on NVidia Quadro FX 3800 GPU. Using Queens between 16 and 21, we observed average speedup of 20x.","","978-1-4673-2925-5","10.1109/PDGC.2012.6449926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449926","GPGPU;OpenCL;N-Queen problem;Parallel copmuting;Graphical processors","Performance evaluation;Graphics processing units;Artificial neural networks;Synchronization","application program interfaces;game theory;graphics processing units;multiprocessing systems;parallel architectures","many-core processor;NVidia Quadro FX 3800 GPU;synchronization issues;atomicity issues;global memory parameters;local memory parameters;OpenCL programming model;general purpose computing-on-graphics processing units;GPGPU architecture;N-queens problem","","6","","26","","7 Feb 2013","","","IEEE","IEEE Conferences"
"Efficient Parallel Preconditioned Conjugate Gradient Solver on GPU for FE Modeling of Electromagnetic Fields in Highly Dissipative Media","A. F. P. de Camargos; V. C. Silva; J. Guichon; G. Munier","Escola Politécnica da Universidade de São Paulo, São Paulo, Brazil; V. C. Silva is with the Escola Politécnica da Universidade de São Paulo, São Paulo, SP, 05508-010, Brazil; Laboratoire de Génie Electrique de Grenoble, CNRS, Saint Martin d'Hères, France; Laboratoire de Génie Electrique de Grenoble, CNRS, Saint Martin d'Hères, France","IEEE Transactions on Magnetics","26 Feb 2014","2014","50","2","569","572","We present a performance analysis of a parallel implementation of preconditioned conjugate gradient solvers using graphic processing units with compute unified device architecture programming model. The solvers were optimized for the solution of sparse systems of equations arising from finite-element analysis of electromagnetic phenomena involved in the diffusion of underground currents in both steady state and under time-harmonic current excitation. We used both shifted incomplete Cholesky factorization and incomplete LU factorization as preconditioners. The results show a significant speedup using the graphics processing unit compared with a serial CPU implementation.","1941-0069","","10.1109/TMAG.2013.2285091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6749203","FEMs;graphic processing unit (GPU);linear systems;performance analysis","Graphics processing units;Linear systems;Mathematical model;Convergence;Silicon carbide;Computer architecture;Sparse matrices","absorbing media;electromagnetic fields;finite element analysis;graphics processing units;matrix decomposition","parallel preconditioned conjugate gradient solver;graphic processing units;GPU;electromagnetic fields;dissipative media;unified device architecture programming;sparse systems;finite element analysis;electromagnetic phenomena;underground currents;time-harmonic current excitation;Cholesky factorization;graphics processing unit","","12","","15","IEEE","26 Feb 2014","","","IEEE","IEEE Journals"
"Unlocking bandwidth for GPUs in CC-NUMA systems","N. Agarwal; D. Nellans; M. O'Connor; S. W. Keckler; T. F. Wenisch",University of Michigan; NVIDIA; NVIDIA; NVIDIA; University of Michigan,"2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)","9 Mar 2015","2015","","","354","365","Historically, GPU-based HPC applications have had a substantial memory bandwidth advantage over CPU-based workloads due to using GDDR rather than DDR memory. However, past GPUs required a restricted programming model where application data was allocated up front and explicitly copied into GPU memory before launching a GPU kernel by the programmer. Recently, GPUs have eased this requirement and now can employ on-demand software page migration between CPU and GPU memory to obviate explicit copying. In the near future, CC-NUMA GPU-CPU systems will appear where software page migration is an optional choice and hardware cache-coherence can also support the GPU accessing CPU memory directly. In this work, we describe the trade-offs and considerations in relying on hardware cache-coherence mechanisms versus using software page migration to optimize the performance of memory-intensive GPU workloads. We show that page migration decisions based on page access frequency alone are a poor solution and that a broader solution using virtual address-based program locality to enable aggressive memory prefetching combined with bandwidth balancing is required to maximize performance. We present a software runtime system requiring minimal hardware support that, on average, outperforms CC-NUMA-based accesses by 1.95 ×, performs 6% better than the legacy CPU to GPU memcpy regime by intelligently using both CPU and GPU memory bandwidth, and comes within 28% of oracular page placement, all while maintaining the relaxed memory semantics of modern GPUs.","2378-203X","978-1-4799-8930-0","10.1109/HPCA.2015.7056046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056046","","Graphics processing units;Bandwidth;Memory management;Hardware;Runtime;Random access memory","cache storage;graphics processing units;parallel processing;storage management","GPU-based HPC applications;GDDR memory;GPU kernel;on-demand software page migration;CC-NUMA GPU-CPU systems;hardware cache-coherence;memory-intensive GPU workloads;virtual address-based program locality;aggressive memory prefetching;bandwidth balancing;software runtime system;minimal hardware support;CPU memory bandwidth;GPU memory bandwidth;oracular page placement;GPU relaxed memory semantics","","54","1","44","","9 Mar 2015","","","IEEE","IEEE Conferences"
"Design evaluation of OpenCL compiler framework for Coarse-Grained Reconfigurable Arrays","H. Kim; M. Ahn; J. A. Stratton; W. W. Hwu","Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, USA; Samsung Advanced Institute of Technology, San 14-1, Nongseo-dong, Giheung-gu, Yongin-si, Geyonggi-do, Korea; Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, USA; Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, USA","2012 International Conference on Field-Programmable Technology","17 Jan 2013","2012","","","313","320","OpenCL is undoubtedly becoming one of the most popular parallel programming languages as it provides a standardized and portable programming model. However, adopting OpenCL for Coarse-Grained Reconfigurable Arrays (CGRA) is challenging due to divergent architecture capability compared to GPUs. In particular, CGRAs are designed to accelerate loop execution by software pipelining on a grid of functional units exploiting instruction-level parallelism. This is vastly different from a GPU in that it executes data parallel kernels using a large number of parallel threads. Therefore, an OpenCL compiler and runtime for CGRAs must map the threaded parallel programming model to a loop-parallel execution model so that the architecture can best utilize its resources. In this paper, we propose and evaluate a design for an OpenCL compiler framework for CGRAs. The proposed design is composed of a serializer and post optimizer. The serializer transforms parallel execution of work-items to an equivalent loop-based iterative execution in order to avoid expensive multithreading on CGRAs. The resulting code is further optimized by the post optimizer to maximize the coverage of software-pipelinable innermost loops. In order to achieve the goal, various loop-level optimizations can take place in the post optimizer using the loops introduced by the serializer for iterative execution of OpenCL kernels. We provide an analysis of the propose framework from a set of well-studied standard OpenCL kernels by comparing performance of various implementations of benchmarks.","","978-1-4673-2845-6","10.1109/FPT.2012.6412155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412155","OpenCL;GPU;Coarse-Grained Reconfigurable Arrays;CGRA;Samsung Reconfigurable Processor;SRP;RP","Kernel;Computer architecture;Graphics processing units;Optimization;Programming;Hardware","multi-threading;optimising compilers;parallel languages;pipeline processing;reconfigurable architectures;software performance evaluation","design evaluation;OpenCL compiler framework;coarse-grained reconfigurable arrays;parallel programming languages;standardized programming model;portable programming model;CGRA;software pipelining;functional unit grid;instruction-level parallelism;parallel kernels;data execution;parallel threads;threaded parallel programming model;loop-parallel execution model;serializer;post optimizer;loop-based iterative execution;multithreading;innermost loop coverage maximization;loop-level optimizations;iterative OpenCL kernel execution","","6","","19","","17 Jan 2013","","","IEEE","IEEE Conferences"
"A programming model and runtime system for approximation-aware heterogeneous computing","I. Parnassos; N. Bellas; N. Katsaros; N. Patsiatzis; A. Gkaras; K. Kanellis; C. D. Antonopoulos; M. Spyrou; M. Maroudas","Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece; Electrical and Computer Engineering Department, University of Thessaly, Volos, Greece","2017 27th International Conference on Field Programmable Logic and Applications (FPL)","5 Oct 2017","2017","","","1","4","Heterogeneous platforms that include diverse architectures such as multicore CPUs, FPGAs and GPUs are becoming very popular due to their superior performance and energy efficiency. Besides heterogeneity, a promising approach for minimizing energy consumption is through approximate computing which relaxes the requirement that all parts of a program are considered equally important to the output quality, thus, all should be executed at full accuracy. Our work extends a traditional OpenMP-like programming model and runtime system to support seamless execution on hybrid architectures with approximation semantics. Starting from a common application code, annotated with our programming model, the programmer can not only target heterogeneous architectures comprising CPU, FPGA and GPU components, but can also regulate the amount of approximation. We evaluate our framework on a number of large-scale applications and demonstrate that the combination of heterogeneous and approximate computing can provide a powerful dynamic interplay between performance and output quality.","1946-1488","978-9-0903-0428-1","10.23919/FPL.2017.8056774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8056774","","Field programmable gate arrays;Kernel;Runtime;Graphics processing units;Programming;Histograms","multiprocessing systems;parallel architectures;parallel programming;power aware computing","energy consumption;approximate computing;runtime system;hybrid architectures;approximation semantics;heterogeneous architectures;approximation-aware heterogeneous computing;energy efficiency;OpenMP-like programming model","","1","","8","","5 Oct 2017","","","IEEE","IEEE Conferences"
"Performance Models for Hybrid Programs Accelerated by GPUs","A. Sasidharan","Ansys, Inc,Lebanon,NH,USA","2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","24 Jun 2021","2021","","","641","651","This paper describes the use of statistical tools to model the performance of mixed device (hosts and devices) programs where hosts are CPUs and devices are GPUs. The purpose of GPUs is to accelerate compute-intensive sections of a program, thereby reducing total execution time, with side-effects including reduced machine usage and energy consumption. To model major and minor factors that affect the execution time of offloaded programs, we used a compute-intensive program with several GPU kernels. We have abstracted the hybrid program as a sequence of computations that access various types of memories (device caches, device shared memory, memory of other devices and host memory). In the programming model discussed, the role of a host is reduced to scheduling and coordinating execution of kernels across devices and communicating with other hosts. It can be extended to models where hosts perform computations or are obliterated. Experiments were designed to include a range of memory sizes and types. The performance models were trained, and their predictions were verified using test data.","","978-1-6654-3577-2","10.1109/IPDPSW52791.2021.00098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460650","Hybrid Programming Models;Multi-GPU architectures;Unified Memory;Performance Models;Statistics","Performance evaluation;Processor scheduling;Multiprocessor interconnection;Computational modeling;Graphics processing units;Programming;Tools","cache storage;graphics processing units;mathematics computing;message passing;multiprocessing systems;scheduling","offloaded programs;compute-intensive program;GPU kernels;hybrid program;device caches;host memory;programming model;scheduling;memory sizes;performance models;hybrid programs;GPU;statistical tools;mixed device;compute-intensive sections;total execution time;reduced machine usage;energy consumption","","","","37","","24 Jun 2021","","","IEEE","IEEE Conferences"
"A GPU-inspired soft processor for high-throughput acceleration","J. Kingyens; J. Gregory Steffan","Department of Electrical and Computer Engineering, University of Toronto, Canada; Department of Electrical and Computer Engineering, University of Toronto, Canada","2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)","24 May 2010","2010","","","1","8","There is building interest in using FPGAs as accelerators for high-performance computing, but existing systems for programming them are so far inadequate. In this paper we propose a soft processor programming model and architecture inspired by graphics processing units (GPUs) that are well-matched to the strengths of FPGAs, namely highly-parallel and pipelinable computation. In particular, our soft processor architecture exploits multithreading and vector operations to supply a floating-point pipeline of 64 stages via hardware support for up to 256 concurrent thread contexts. The key new contributions of our architecture are mechanisms for managing threads and register files that maximize data-level and instruction-level parallelism while overcoming the challenges of port limitations of FPGA block memories, as well as memory and pipeline latency. Through simulation of a system that (i) supports AMD's CTM r5xx GPU ISA [1], and (ii) is realizable on an XtremeData XD1000 FPGA-based accelerator system, we demonstrate that our soft processor can achieve 100% utilization of the deeply-pipelined floating-point datapath.","","978-1-4244-6534-7","10.1109/IPDPSW.2010.5470679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470679","","Acceleration;Field programmable gate arrays;Computer architecture;Memory management;Graphics processing unit;Multithreading;Hardware","computer graphic equipment;coprocessors;field programmable gate arrays;multi-threading;parallel processing;pipeline processing","high-throughput acceleration;soft processor programming;graphics processing units;pipelinable computation;highly-parallel computation;floating-point pipeline;multithreading process;instruction-level parallelism;AMD CTM r5xx GPU ISA;XtremeData XD1000 FPGA-based accelerator system;GPU","","7","","15","","24 May 2010","","","IEEE","IEEE Conferences"
"DistributedCL: A Framework for Transparent Distributed GPU Processing Using the OpenCL API","A. Tupinambá; A. Sztajnberg","Programa de Eng. Eletron. - PEL, Univ. do Estado do Rio de Janeiro - UERJ, Rio de Janeiro, Brazil; Dept. de Inf. e Cienc. da Comput., Univ. do Estado do Rio de Janeiro - UERJ, Rio de Janeiro, Brazil","2012 13th Symposium on Computer Systems","24 Dec 2012","2012","","","187","193","This paper presents the DistributedCL, a framework that provides the applications developed using the OpenCL interface location-transparent GPU processing. The application can explore distributed processing with no modification. The architecture of the framework and the programming model are presented, and the possible performance bottlenecks are discussed.","","978-1-4673-4468-5","10.1109/WSCAD-SSC.2012.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391781","OpenCL;GPGPU;framework;distributed systems","Graphics processing units;Kernel;Computational modeling;Electronic mail;Linux;Distributed processing;Computer architecture","application program interfaces;graphics processing units","transparent distributed GPU processing;OpenCL API;DistributedCL;OpenCL interface location-transparent GPU processing;performance bottlenecks","","1","","20","","24 Dec 2012","","","IEEE","IEEE Conferences"
"Performance Analysis of Sequential and Parallel Programming Paradigms on CPU-GPUs Cluster","B. N. Chandrashekhar; H. A. Sanjay","Nitte Meenakshi Institute of Technology,Department of ISE,Bangalore,India,560064; Nitte Meenakshi Institute of Technology,Department of ISE,Bangalore,India,560064","2021 Third International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)","31 Mar 2021","2021","","","1205","1213","The entire world of parallel computing endured a change when accelerators are gradually embraced in today's high-performance computing cluster. A hybrid CPU-GPU cluster is required to speed up the complex computations by using parallel programming paradigms. This paper deals with performance evaluation of sequential, parallel and hybrid programming paradigms on the hybrid CPU-GPU cluster using the sorting strategies such as quick sort, heap sort and merge sort. In this research work performance comparison of C, MPI, and hybrid [MPI+CUDA] on CPU-GPUs hybrid systems are performed by using the sorting strategies. From the analysis it is observed that, the performance of parallel programming paradigm MPI is better when compared against sequential programming model. Also, research work evaluates the performance of CUDA on GPUs and hybrid programming model [MPI+CUDA] on CPU+GPU cluster using merge sort strategies and noticed that hybrid programming model [MPI+CUDA] has better performance against traditional approach and parallel programming paradigms MPI and CUDA When the overall performance of all three programming paradigms are compared, MPI+CUDA based on CPU+GPU environment gives the best speedup.","","978-1-6654-1960-4","10.1109/ICICV50876.2021.9388469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9388469","Parallel programming pardigms;Performance model;hybrid parallel computing models;Central Processing Unit;compute unified device architecture;Graphics Processing Unit;message passing interface","Performance evaluation;Parallel programming;Computational modeling;Graphics processing units;Programming;Parallel processing;Sorting","application program interfaces;graphics processing units;merging;message passing;parallel architectures;parallel programming;pattern clustering;software performance evaluation;sorting","sorting strategies;quick sort;heap sort;parallel programming paradigm MPI;sequential programming model;hybrid programming model;performance analysis;parallel computing;high performance computing cluster;hybrid CPU GPU cluster;CUDA performance evaluation;merge sort strategies","","","","21","","31 Mar 2021","","","IEEE","IEEE Conferences"
"An MPI/GPU parallelization of an interior penalty discontinuous Galerkin time domain method for Maxwell's equations","S. Dosopoulos; J. D. Gardiner; J. Lee","Electrical and Computer Engineering Department, Ohio State University, Columbus, Ohio, USA.; Ohio Supercomputer Center, Columbus, Ohio, USA.; Electrical and Computer Engineering Department, Ohio State University, Columbus, Ohio, USA.","Radio Science","9 Dec 2016","2011","46","03","1","10","In this paper we discuss our approach to the MPI/GPU implementation of an Interior Penalty Discontinuous Galerkin Time domain (IPDGTD) method to solve the time dependent Maxwell's equations. In our approach, we exploit the inherent DGTD parallelism and describe a combined MPI/GPU and local time stepping implementation. This combination is aimed at increasing efficiency and reducing computational time, especially for multiscale applications. The CUDA programming model was used, together with non-blocking MPI calls to overlap communications across the network. A 10X speedup compared to CPU clusters is observed for double precision arithmetic. Finally, for p = 1 basis functions, a good scalability with parallelization efficiency of 85% for up to 40 GPUs and 80% for up to 160 CPU cores was achieved on the Ohio Supercomputer Center's Glenn cluster.","1944-799X","","10.1029/2011RS004689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776116","","Graphics processing units;Finite element analysis;Time-domain analysis;Scalability;Method of moments;Mathematical model;Hardware","","","","4","","","","9 Dec 2016","","","AGU","AGU Journals"
"Parallel implementation of Multi-dimensional Ensemble Empirical Mode Decomposition","L. Chang; M. Lo; N. Anssari; K. Hsu; N. E. Huang; W. W. Hwu","University of Illinois at Urbana-Champaign, USA 61801; National Central University, Chungli, Taiwan 32001; University of Illinois at Urbana-Champaign, USA 61801; National Central University, Chungli, Taiwan 32001; National Central University, Chungli, Taiwan 32001; University of Illinois at Urbana-Champaign, USA 61801","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","1621","1624","In this paper, we propose and evaluate two parallel implementations of Multi-dimensional Ensemble Empirical Mode Decomposition (MEEMD) for multi-core (CPU) and many-core (GPU) architectures. Relative to a sequential C implementation, our double precision GPU implementation, using the CUDA programming model, achieves up to 48.6x speedup on NVIDIA Tesla C2050. Our multi-core CPU implementation, using the OpenMP programming model, achieves up to 11.3x speedup on two octal-core Intel Xeon x7550 CPUs.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5946808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946808","Multi-dimensional Ensemble Empirical Mode Decomposition;GPGPU;OpenMP;CUDA","Graphics processing unit;Parallel processing;Instruction sets;Spline;Interpolation;Programming","computer graphic equipment;coprocessors;multiprocessing systems;parallel architectures;parallel programming","parallel implementation;multidimensional ensemble empirical mode decomposition;multicore architecture;many-core architecture;sequential C implementation;double precision GPU implementation;CUDA programming model;NVIDIA Tesla C2050;multicore CPU implementation;OpenMP programming model;octal-core Intel Xeon CPU","","15","","17","","11 Jul 2011","","","IEEE","IEEE Conferences"
"OpenCL - An effective programming model for data parallel computations at the Cell Broadband Engine","J. Breitbart; C. Fohry","Research Group Programming Languages / Methodologies, Universität Kassel, Germany; Research Group Programming Languages / Methodologies, Universität Kassel, Germany","2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)","24 May 2010","2010","","","1","8","Current processor architectures are diverse and heterogeneous. Examples include multicore chips, CPUs and the Cell Broadband Engine (CBE). The recent Open Compute Language (OpenCL) standard aims at efficiency and portability. This paper explores its efficiency when implemented on the CBE, without using CBE-specific features such as explicit asynchronous memory transfers. We based our experiments on two applications: matrix multiplication, and the client side of the Einstein@Home distributed computing project. Both were programmed in OpenCL, and then translated to the CBE. For matrix multiplication, we deployed different levels of OpenCL performance optimization, and observed that they pay off on the CBE. For the Einstein@Home application, our translated OpenCL version achieves almost the same speed as a native CBE version. Another main contribution of the paper is a proposal for an additional memory level in OpenCL, called static local memory. With little programming expense, it can lead to significant speedups such as factor seven for reduction. Finally, we studied two versions of the OpenCL to CBE mapping, in which the PPE component of the CBE does or does not take the role of a compute unit.","","978-1-4244-6534-7","10.1109/IPDPSW.2010.5470823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470823","","Parallel programming;Concurrent computing;Engines;Computer architecture;Multicore processing;Hardware;Computer languages;Distributed computing;Optimization;Proposals","computer graphics;coprocessors;matrix multiplication;parallel programming","programming model;data parallel computations;cell broadband engine;multicore chips;GPU;open compute language;explicit asynchronous memory transfers;matrix multiplication;Einstein@Home distributed computing project;static local memory","","7","","12","","24 May 2010","","","IEEE","IEEE Conferences"
"GraphReduce: processing large-scale graphs on accelerator-based systems","D. Sengupta; S. L. Song; K. Agarwal; K. Schwan",NA; NA; NA; NA,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","26 Jan 2017","2015","","","1","12","Recent work on real-world graph analytics has sought to leverage the massive amount of parallelism offered by GPU devices, but challenges remain due to the inherent irregularity of graph algorithms and limitations in GPU-resident memory for storing large graphs. We present GraphReduce, a highly efficient and scalable GPU-based framework that operates on graphs that exceed the device's internal memory capacity. GraphReduce adopts a combination of edge- and vertex-centric implementations of the Gather-Apply-Scatter programming model and operates on multiple asynchronous GPU streams to fully exploit the high degrees of parallelism in GPUs with efficient graph data movement between the host and device. GraphReduce-based programming is performed via device functions that include gatherMap, gatherReduce, apply, and scatter, implemented by programmers for the graph algorithms they wish to realize. Extensive experimental evaluations for a wide variety of graph inputs and algorithms demonstrate that GraphReduce significantly outperforms other competing out-of-memory approaches.","2167-4337","978-1-4503-3723-6","10.1145/2807591.2807655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832802","","Graphics processing units;Parallel processing;Programming;Memory management;Computational modeling;Acceleration;Partitioning algorithms","data handling;graph theory;parallel processing","GraphReduce;large-scale graphs processing;accelerator-based systems;graph analytics;parallelism;GPU devices;GPU-based framework;internal memory capacity;edge-centric implementations;vertex-centric implementations;gather-apply-scatter programming model;multiple asynchronous GPU streams;graph data movement;device functions;gatherMap;gatherReduce","","12","1","43","","26 Jan 2017","","","IEEE","IEEE Conferences"
"Student Session: Practical Insights on Acceleration for 3D Lidar Data Processing","I. Baek; K. Fuseini; R. R. Rajkumar",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,"2020 IEEE 26th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)","22 Sep 2020","2020","","","1","2","3D Lidar has become a widely used sensor technology in autonomous vehicles by providing accurate distance information. However, lidar pointcloud processing often involves sophisticated algorithms, and takes a lot of computational power. Many prior approaches relied on a GPU-based parallel programming model, such as CUDA, to accelerate these computations. However, little attention has been given to comparing different methods for selecting the most-suited programming and parallelization approaches for a given computing system. We present our findings and insights identified by implementing various parallel approaches considering both CPUs and GPUs. We also demonstrate significant acceleration results using a real-world perception algorithm developed to detect road boundaries. Finally, we compare the pros and cons of each method in terms of system architecture, programming model, and resource utilization to yield a better understanding of choosing the best parallelization approach for a given optimization objective.","2325-1301","978-1-7281-4403-0","10.1109/RTCSA50079.2020.9203651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203651","","Graphics processing units;Data structures;Acceleration;Laser radar;Runtime;Sorting;Kernel","data structures;graphics processing units;optical information processing;optical radar;parallel programming","sensor technology;autonomous vehicles;accurate distance information;computational power;GPU-based parallel programming model;system architecture;parallelization approach;optimization objective;lidar point cloud processing;real-world perception algorithm;CUDA;CPUs;road boundaries detection;programming model;resource utilization;3D Lidar data processing acceleration","","","","2","","22 Sep 2020","","","IEEE","IEEE Conferences"
"Connecting the dots: Triangle completion and related problems on large data sets using GPUs","A. Chatterjee; S. Radhakrishnan; C. N. Sekharan","School of Computer Science, University of Oklahoma, Norman, USA; School of Computer Science, University of Oklahoma, Norman, USA; Department of Computer Science, Loyola University Chicago, Chicago, IL, USA","2014 IEEE International Conference on Big Data (Big Data)","8 Jan 2015","2014","","","1","8","Studying the properties of Online Social Networks (OSNs) and other real world graphs have gained importance due to the large amount of information available from them. These large graphs contain data that can be analyzed and effectively used in advertising, security and improving the overall experience of the users of these networks. However, the analysis of these graphs for studying specific properties requires combinatorially explosive number of computations. Compute Unified Device Architecture (CUDA) is a programming model available from Nvidia for solving general-purpose problems using the massively parallel and highly multi-threaded Graphics Processing Units (GPUs). Therefore, using GPUs to solve these types of problems is appropriate. In addition, due to the properties of real-world data, the graphs being considered are sparse and have irregular data dependencies. Hence, using efficient techniques to store the graph data for initial preprocessing and final computation by taking advantage of heterogeneous CPU-GPU systems can address these issues. In this paper, we are interested in studying different properties of these real-world entities that transform into the following graph problems: a) identifying a missing edge, which when added would result in maximum increase in the number of triangles, b) identifying an existing edge whose removal would result in the maximum decrease in the number of triangles, c) identifying an existing edge whose removal would increase the number of connected components in the graph. In this paper, we develop and implement algorithms to solve the above problems using both CPU and GPU. Specifically, given a graph G = (V, E), we provide algorithms for the following: a) find (v<sub>i</sub>, V<sub>j</sub>) ∉ E, such that Δ<sub>f</sub> - Δ<sub>c</sub> is maximized, where Δ<sub>f</sub> and Δ<sub>c</sub> are the number of triangles in G<sub>m</sub> = (V, E ∪(v<sub>i</sub>, V<sub>j</sub>)) and G, respectively, b) find a (v<sub>i</sub>, V<sub>j</sub>) ϵ E, such that Δ<sub>c</sub> - Δ<sub>f</sub> is maximized, where Δ<sub>f</sub> and Δ<sub>c</sub> are the number of triangles in G<sub>m</sub> = (V, E \ (v<sub>i</sub>, V<sub>j</sub>)) and G = (V, E), respectively, c) find a (v<sub>i</sub>, V<sub>j</sub>) ϵ E, such that Φ<sub>c</sub> > Φ<sub>c</sub>, where Φ<sub>c</sub> and Φ<sub>c</sub> are the number of connected components in G<sub>m</sub> = (V, E \ (v<sub>i</sub>, V<sub>j</sub>)) and G = (V, E), respectively. We implement the algorithms using a GPU and achieve a 10 × speedup as compared to a sequential implementation. Thereafter, we design a heuristic for finding an edge whose existence would result in the maximum increase in the number of triangles. The heuristic is implemented and the results are reported and compared to those of the regular algorithm on the GPU.","","978-1-4799-5666-1","10.1109/BigData.2014.7004365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004365","Graph problems;Online Social Networks;Triangle completion;CUDA;GPU","Joining processes;Graphics processing units;Algorithm design and analysis;Social network services;Testing;Educational institutions;Advertising","computer graphics;graphics processing units;Internet","triangle completion;large data sets;online social networks;OSN;compute unified device architecture;CUDA;programming model;Nvidia;general-purpose problems;multithreaded graphics processing units;irregular data dependencies;graph data;heterogeneous CPU-GPU systems","","3","","18","","8 Jan 2015","","","IEEE","IEEE Conferences"
"Speculative execution on multi-GPU systems","G. Diamos; S. Yalamanchili","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332-0250; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332-0250","2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)","24 May 2010","2010","","","1","12","The lag of parallel programming models and languages behind the advance of heterogeneous many-core processors has left a gap between the computational capability of modern systems and the ability of applications to exploit them. Emerging programming models, such as CUDA and OpenCL, force developers to explicitly partition applications into components (kernels) and assign them to accelerators in order to utilize them effectively. An accelerator is a processor with a different ISA and micro-architecture than the main CPU. These static partitioning schemes are effective when targeting a system with only a single accelerator. However, they are not robust to changes in the number of accelerators or the performance characteristics of future generations of accelerators. In previous work, we presented the Harmony execution model for computing on heterogeneous systems with several CPUs and accelerators. In this paper, we extend Harmony to target systems with multiple accelerators using control speculation to expose parallelism. We refer to this technique as Kernel Level Speculation (KLS). We argue that dynamic parallelization techniques such as KLS are sufficient to scale applications across several accelerators based on the intuition that there will be fewer distinct accelerators than cores within each accelerator. In this paper, we use a complete prototype of the Harmony runtime that we developed to explore the design decisions and trade-offs in the implementation of KLS. We show that KLS improves parallelism to a sufficient degree while retaining a sequential programming model. We accomplish this by demonstrating good scaling of KLS on a highly heterogeneous system with three distinct accelerator types and ten processors.","1530-2075","978-1-4244-6443-2","10.1109/IPDPS.2010.5470427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470427","","Parallel programming;Kernel;Parallel processing;Concurrent computing;Instruction sets;Robustness;Character generation;Control systems;Prototypes;Runtime","coprocessors;multiprocessing systems;parallel programming","speculative execution;multi-GPU systems;parallel programming models;parallel programming languages;heterogeneous many-core processors;computational capability;CUDA;OpenCL;application partitioning;components;accelerator;ISA;micro-architecture;Harmony execution model;kernel level speculation;dynamic parallelization techniques;Harmony runtime;sequential programming model;heterogeneous system","","12","","28","","24 May 2010","","","IEEE","IEEE Conferences"
"A GPU-Accelerated SVD Algorithm, Based on QR Factorization and Givens Rotations, for DWI Denoising","L. Marcellino; G. Navarra","Dept. of Sci. & Technol., Univ. of Naples Parthenope, Naples, Italy; Dept. of Sci. & Technol., Univ. of Naples Parthenope, Naples, Italy","2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","24 Apr 2017","2016","","","699","704","In this work, we present a parallel implementation of the Singular Value Decomposition (SVD) method on Graphics Processing Units (GPUs) using CUDA programming model. Our approach is based on an iterative parallel version of the QR factorization by means Givens plane rotations using the Sameh and Kuck scheme. The parallel algorithm is driven by an outer loop executed on the CPU. Therefore, threads and blocks configuration is organized in order to use the shared memory and avoid multiple accesses to global memory. However, the main kernel provides coalesced accesses to global memory using contiguous indices. As case study, we consider the application of the SVD in the Overcomplete Local Principal Component Analysis (OLPCA) algorithm for the Diffusion Weighted Imaging (DWI) denoising process. Our results show significant improvements in terms of performances with respect to the CPU version that encourage its usability for this expensive application.","","978-1-5090-5698-9","10.1109/SITIS.2016.117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907544","SVD;QR factorization;Givens Rotations;GPGPU;PCA;DWI denoising","Matrix decomposition;Graphics processing units;Central Processing Unit;Noise reduction;Principal component analysis;Parallel algorithms;Image processing","biomedical MRI;graphics processing units;image denoising;matrix decomposition;medical image processing;parallel architectures;parallel programming;principal component analysis;singular value decomposition","GPU-accelerated SVD algorithm;DWI denoising;singular value decomposition;SVD method;graphics processing units;CUDA programming model;iterative parallel QR factorization;Givens plane rotations;Sameh-and-Kuck scheme;parallel algorithm;outer loop;thread configuration;block configuration;shared memory;global memory;contiguous indices;SVD;overcomplete local principal component analysis;OLPCA algorithm;diffusion weighted imaging denoising process;DWI denoising process;performance improvement;diffusion tensor imaging","","1","","24","","24 Apr 2017","","","IEEE","IEEE Conferences"
"Game developer's perspective on OpenCL","E. Schenk","EA, USA","2009 IEEE Hot Chips 21 Symposium (HCS)","26 May 2016","2009","","","1","44","Presents a collection of slides covering the following topics: game development; OpenCL; concurrent programming model; Amdahl's law; game experiences; memory objects; data flow; command queue; vectorization; scalar integrator; vector integrator; parallelism; GPU memory; and GPU device performance.","","978-1-4673-8873-3","10.1109/HOTCHIPS.2009.7478347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478347","","Art;Open systems;Data models;Games;Computer platforms;Concurrent computing;Standards;Computer graphics;Videos","computer games;concurrency control;graphics processing units;parallel programming;specification languages;storage management","game development;OpenCL;concurrent programming model;Amdahl law;game experience;memory objects;data flow;command queue;vectorization;scalar integrator;vector integrator;parallelism;GPU memory;GPU device performance;graphics processing unit","","","","","","26 May 2016","","","IEEE","IEEE Conferences"
"Efficient Data Communication between CPU and GPU through Transparent Partial-Page Migration","S. Zhang; Y. Yang; L. Shen; Z. Wang","Dept. of Comput. Sci. & Technol., Nat. Univ. of Defense Technol., Changsha, China; Dept. of Comput. Sci. & Technol., Nat. Univ. of Defense Technol., Changsha, China; Dept. of Comput. Sci. & Technol., Nat. Univ. of Defense Technol., Changsha, China; Dept. of Comput. Sci. & Technol., Nat. Univ. of Defense Technol., Changsha, China","2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","24 Jan 2019","2018","","","618","625","Despite the increasing investment in integrated GPUs and next-generation interconnect research, discrete GPUs connected by PCI Express still account for the dominant position of the market, the management of data communication between CPU and GPU continues to evolve. Initially, the programmer controls the data transfer between CPU and GPU explicitly. To simplify programming and enable system-wide atomic memory operations, GPU vendors have developed a programming model that provides a single virtual address space. The page migration engine in this model migrates pages between CPU and GPU on demand automatically. To meet the needs of high-performance workloads, the page size tends to be larger. Limited by low bandwidth and high latency interconnects, larger page migration has longer delay, which may reduce the overlap of computation and transmission and cause serious performance decline. In this paper, we propose partial-page migration that only migrates the requested part of a page to shorten the migration latency and avoid the performance degradation of the whole-page migration when the page becomes larger. Experiments show that partial-page migration is possible to significantly hide the performance overheads of whole-page migration when the page size is 2MB and the PCI Express bandwidth is 16GB/sec, converting an average 72.72× slowdown to a 1.29× speedup when compared with programmers controlled data transmission. Additionally, we examine the impact of page size on TLB miss rate and the performance impact of migration unit size on execution time, enabling designers to make informed decisions.","","978-1-5386-6614-2","10.1109/HPCC/SmartCity/DSS.2018.00112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622850","Unified Memory, Data Communication, Partial Page Migration","Graphics processing units;Data communication;Memory management;Bandwidth;Programming;Delays;Central Processing Unit","data communication;graphics processing units;paged storage","whole-page migration;PCI Express bandwidth;data transmission;transparent partial-page migration;next-generation interconnect research;system-wide atomic memory operations;GPU vendors;high-performance workloads;data communication","","1","","23","","24 Jan 2019","","","IEEE","IEEE Conferences"
"Acceleration of regular grid traversals using extended chessboard distance transformation on GPU","A. Es; V. Isler","Tubitak-Bilten, METU, Turkey; NA","Ninth International Conference on Computer Aided Design and Computer Graphics (CAD-CG'05)","13 Mar 2006","2005","","","8 pp.","","In the recent years graphics processing units (GPU) have evolved into general purpose programmable streaming parallel processors. This evolution makes it possible to implement high quality photo realistic rendering techniques on graphics processors. There have been a few studies to show how to map ray tracing to the GPU. Since graphics processors are not designed to process complex data structures, it is crucial to explore data structures and algorithms for efficient stream processing. In particular ray traversal is one of the most time consuming parts of ray tracing methods. In this work we focus on the efficient ray traversals on GPU. Several known techniques have been redesigned and adapted to the GPU programming model. Also a new traversal method based on extended anisotropic chessboard distance metric has been introduced.","","0-7695-2473-7","10.1109/CAD-CG.2005.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1604672","","Acceleration;Data structures;Ray tracing;Computer graphics;Anisotropic magnetoresistance;Concurrent computing;Rendering (computer graphics);Process design;Algorithm design and analysis;Parallel processing","coprocessors;parallel processing;rendering (computer graphics);ray tracing","regular grid traversals;chessboard distance transformation;GPU;graphics processing units;general purpose programmable streaming parallel processors;photo realistic rendering;ray tracing;ray traversals;anisotropic chessboard distance metric","","2","","18","","13 Mar 2006","","","IEEE","IEEE Conferences"
"Achieving High Performance on Supercomputers with a Sequential Task-based Programming Model","E. Agullo; O. Aumage; M. Faverge; N. Furmento; F. Pruvost; M. Sergent; S. P. Thibault","HiePACS, Inria Centre de recherche Bordeaux Sud-Ouest, 113923 Talence, Aquitaine France (e-mail: emmanuel.agullo@inria.fr); STORM, Inria Centre de recherche Bordeaux Sud-Ouest, 113923 Talence, Aquitaine France (e-mail: olivier.aumage@inria.fr); HiePACS, Bordeaux INP, Talence, Aquitaine France (e-mail: mathieu.faverge@inria.fr); STORM, LaBRI, TALENCE, Aquitaine France (e-mail: nathalie.furmento@labri.fr); HiePACS, Inria Centre de recherche Bordeaux Sud-Ouest, 113923 Talence, Aquitaine France (e-mail: florent.pruvost@inria.fr); STORM, Inria Centre de recherche Bordeaux Sud-Ouest, 113923 Talence, Aquitaine France (e-mail: marc.sergent@inria.fr); Computer science, LaBRI, TALENCE, - France 33405 (e-mail: samuel.thibault@u-bordeaux.fr)","IEEE Transactions on Parallel and Distributed Systems","","2017","PP","99","1","1","The emergence of accelerators as standard computing resources on supercomputers and the subsequent architectural complexity increase revived the need for high-level parallel programming paradigms. Sequential task-based programming model has been shown to efficiently meet this challenge on a single multicore node possibly enhanced with accelerators, which motivated its support in the OpenMP 4.0 standard. In this paper, we show that this paradigm can also be employed to achieve high performance on modern supercomputers composed of multiple such nodes, with extremely limited changes in the user code. To prove this claim, we have extended the StarPU runtime system with an advanced inter-node data management layer that supports this model by posting communications automatically. We illustrate our discussion with the task-based tile Cholesky algorithm that we implemented on top of this new runtime system layer. We show that it allows for very high productivity while achieving a performance competitive with both the pure Message Passing Interface (MPI)-based ScaLAPACK Cholesky reference implementation and the DPLASMA Cholesky code, which implements another (non sequential) task-based programming paradigm.","1558-2183","","10.1109/TPDS.2017.2766064","ANR SOLHAR(grant numbers:ANR-13-MONU-0007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8226789","runtime system;sequential task flow;task-based programming;heterogeneous computing;distributed computing;multicore;GPU;Cholesky factorization","Runtime;Programming;Supercomputers;Libraries;Algorithm design and analysis;Productivity","","","","12","","","IEEE","19 Dec 2017","","","IEEE","IEEE Early Access Articles"
"Asynchronous Task-Based Execution of the Reverse Time Migration for the Oil and Gas Industry","A. AlOnazi; H. Ltaief; D. Keyes; I. Said; S. Thibault","King Abdullah University of Science and Technology,Extreme Computing Research Center,Thuwal,Jeddah 23955,Saudi Arabia; King Abdullah University of Science and Technology,Extreme Computing Research Center,Thuwal,Jeddah 23955,Saudi Arabia; King Abdullah University of Science and Technology,Extreme Computing Research Center,Thuwal,Jeddah 23955,Saudi Arabia; NVIDIA,Oil and Gas Department,Paris,France; Univ. Bordeaux,Talence,33400 France","2019 IEEE International Conference on Cluster Computing (CLUSTER)","7 Nov 2019","2019","","","1","11","We propose a new framework for deploying Reverse Time Migration (RTM) simulations on distributed-memory systems equipped with multiple GPUs. Our software, TB-RTM, infrastructure engine relies on the StarPU dynamic runtime system to orchestrate the asynchronous scheduling of RTM computational tasks on the underlying resources. Besides dealing with the challenging hardware heterogeneity, TB-RTM supports tasks with different workload characteristics, which stress disparate components of the hardware system. RTM is challenging in that it operates intensively at both ends of the memory hierarchy, with compute kernels running at the highest level of the memory system, possibly in GPU main memory, while I/O kernels are saving solution data to fast storage. We consider how to span the wide performance gap between the two extreme ends of the memory system, i.e., GPU memory and fast storage, on which large-scale RTM simulations routinely execute. To maximize hardware occupancy while maintaining high memory bandwidth throughout the memory subsystem, our framework presents the new-of-core (OOC) feature from StarPU to prefetch data solutions in and out not only from/to the GPU/CPU main memory but also from/to the fast storage system. The OOC technique may trigger opportunities for overlapping expensive data movement with computations. TB-RTM framework addresses this challenging problem of heterogeneity with a systematic approach that is oblivious to the targeted hardware architectures. Our resulting RTM framework can effectively be deployed on massively parallel GPU-based systems, while delivering performance scalability up to 500 GPUs.","2168-9253","978-1-7281-4734-5","10.1109/CLUSTER.2019.8891054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8891054","Reverse Time Migration;Task-Based Programming Model;Out-Of-Core Algorithms;Asynchronous Executions;Overlapping I/O with Computation;STARPU OOC","Task analysis;Kernel;Computational modeling;Hardware;Runtime;Graphics processing units","gas industry;graphics processing units;multiprocessing systems;parallel processing;scheduling;shared memory systems;storage management","asynchronous task-based execution;oil and gas industry;distributed-memory systems;multiple GPUs;StarPU dynamic runtime system;asynchronous scheduling;RTM computational tasks;workload characteristics;hardware system;memory hierarchy;compute kernels;memory system;GPU main memory;large-scale RTM simulations;hardware occupancy;memory bandwidth;memory subsystem;data solutions;fast storage system;targeted hardware architectures;massively parallel GPU-based systems;RTM framework;hardware heterogeneity;reverse time migration simulation","","2","","49","","7 Nov 2019","","","IEEE","IEEE Conferences"
"Efficient integral image computation on the GPU","B. Bilgic; B. K. P. Horn; I. Masaki","Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA 02139, USA; Department of Electrical Engineering and Computer Science and CSAIL, MIT, Cambridge, MA 02139, USA; Department of Electrical Engineering and Computer Science and MTL, MIT, Cambridge, MA 02139, USA","2010 IEEE Intelligent Vehicles Symposium","16 Aug 2010","2010","","","528","533","We present an integral image algorithm that can run in real-time on a Graphics Processing Unit (GPU). Our system exploits the parallelisms in computation via the NIVIDA CUDA programming model, which is a software platform for solving non-graphics problems in a massively parallel high-performance fashion. This implementation makes use of the work-efficient scan algorithm that is explicated in. Treating the rows and the columns of the target image as independent input arrays for the scan algorithm, our method manages to expose a second level of parallelism in the problem. We compare the performance of the parallel approach running on the GPU with the sequential CPU implementation across a range of image sizes and report a speed up by a factor of 8 for a 4 megapixel input. We further investigate the impact of using packed vector type data on the performance, as well as the effect of double precision arithmetic on the GPU.","1931-0587","978-1-4244-7868-2","10.1109/IVS.2010.5548142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5548142","","Signal processing algorithms;Graphics processing unit;Parallel processing;Concurrent computing;Parallel programming;Central Processing Unit;Detectors","computer graphic equipment;coprocessors;image processing","integral image computation;graphics processing unit;NIVIDA CUDA programming model;nongraphics problems;scan algorithm;feature evaluation","","51","2","12","","16 Aug 2010","","","IEEE","IEEE Conferences"
"Database processing by Linear Regression on GPU using CUDA","J. B. Kulkarni; A. A. Sawant; V. S. Inamdar","Department of Computer Engineering and Information Technology, College of Engineering, Pune - 411001, India; Department of Computer Engineering and Information Technology, College of Engineering, Pune - 411001, India; Department of Computer Engineering and Information Technology, College of Engineering, Pune - 411001, India","2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies","22 Sep 2011","2011","","","20","23","In today's era, there is a great importance to parallel programming to gain high performance in terms of time required for data computation. There are some constraints to achieve parallelism on CPU (Central Processing Unit). It is possible to achieve data parallelism by SIMD (Single Instruction Multiple Data) on General Purpose Graphics Processing Unit (GPGPU) integrated with Central Processing Unit (CPU). In Database processing, most of the research is going on. In this implementation, Linear Regression Algorithm is used to achieve parallelism in database processing on images using a programming model, Compute Unified Device Architecture (CUDA) which uses multithreading technique. Most of the time is required to perform various operations on huge content-based database e.g. to read big images, datasets, etc. Linear Regression is one of the algorithm to predict, forecast, mine huge amount of data. Linear Regression using CUDA can achieve high performance. Here, Linear Regression is implemented on Graphics Processing Unit (GPU) and on CPU to process image database for prediction of data by finding Covariance matrix, Eigen values and Eigen vectors. The strongest Eigen vector is the best fit line. The time spent for computation is compared in both the implementations.","","978-1-61284-653-8","10.1109/ICSCCN.2011.6024507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024507","Central Processing Unit;Graphics Processing Unit;CUDA;LTI;DirectX;Residual error;Sum of square;etc","Graphics processing unit;Linear regression;Central Processing Unit;Databases;Computer architecture;Covariance matrix;Programming","computer graphic equipment;coprocessors;covariance matrices;eigenvalues and eigenfunctions;multi-threading;regression analysis;visual databases","database processing;linear regression;CUDA;Compute Unified Device Architecture;parallel programming;central processing unit;CPU;data parallelism;SIMD;single instruction multiple data;GPGPU;general purpose graphics processing unit;image database;covariance matrix;eigenvalues;eigenvectors;multithreading technique","","3","","12","","22 Sep 2011","","","IEEE","IEEE Conferences"
