{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr9ik55TAtDB",
        "outputId": "6e3cd7fb-39ca-4755-f14b-157eb7ccd9a7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community langchain_openai pypdf faiss-cpu gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cM8Fd1ZApIC"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = userdata.get('openai-api')\n",
        "TAVILY_API_KEY = userdata.get('travily-api')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install langchain gradio openai tavily-python pypdf faiss-cpu\n",
        "# coding QA Expert Chatbot using langchain and gradio as web UI. use PDF RAG with faiss vector DB to save, retrieve the chunk documents from the PDF. if run this chatbot, read the PDF files from ./files folder, splite them into chunks, save them to faiss as vector database. after that, create LLM using openai and create langchain prompt template, tools with web search using Tavily. create agents with them including the previous dialog memory. this UI using gradio is simliar to ChatBot.\n",
        "import os, re\n",
        "import glob\n",
        "import gradio as gr\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.tools.tavily_search import TavilySearchResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 설정\n",
        "VECTOR_DB_PATH = './faiss_index'\n",
        "FILES_DIRECTORY = './files'\n",
        "CHUNK_SIZE = 2000\n",
        "CHUNK_OVERLAP = 300\n",
        "\n",
        "# OpenAI 설정\n",
        "llm_model = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. PDF 파일 로드 및 벡터화\n",
        "def load_and_split_pdfs(files_directory):\n",
        "\tpdf_files = glob.glob(os.path.join(files_directory, '*.pdf'))\n",
        "\tdocuments = []\n",
        "\tfor file in pdf_files:\n",
        "\t\tloader = PyPDFLoader(file)\n",
        "\t\tdocuments.extend(loader.load())\n",
        "\tsplitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\tsplit_documents = splitter.split_documents(documents)\n",
        "\tfor i, doc in enumerate(split_documents):\n",
        "\t\tprint(f\"Document {i}: {doc.page_content[:100]}...\")  # Print the first 100 characters of each split document\n",
        "\treturn splitter.split_documents(documents)\n",
        "\n",
        "# 3. FAISS 벡터DB 저장\n",
        "def save_to_faiss(documents):\n",
        "\tvectordb = FAISS.from_documents(documents, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))\n",
        "\tvectordb.save_local(VECTOR_DB_PATH)\n",
        "\tprint(f\"FAISS vector database saved to {VECTOR_DB_PATH}\")\n",
        "\treturn vectordb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. RAG Retrieval QA 체인 생성\n",
        "def create_retrieval_qa(vectordb):\n",
        "\tretriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'lambda_mult': 0.25})\n",
        "\tqa_chain = RetrievalQA.from_chain_type(\n",
        "\t\tllm=llm_model,\n",
        "\t\tretriever=retriever,\n",
        "\t\tchain_type=\"stuff\"\n",
        "\t)\n",
        "\treturn qa_chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Common agent Types:\n",
        "\n",
        "zero-shot-react-description:\n",
        "\tUses the ReAct (Reasoning + Acting) framework.\n",
        "\tSelects tools and generates responses based on tool descriptions.\n",
        "\tBest for scenarios where the agent needs to reason and act without prior context.\n",
        "\n",
        "chat-zero-shot-react-description:\n",
        "\tSimilar to zero-shot-react-description, but optimized for chat-based interactions.\n",
        "\tUseful for conversational agents.\n",
        "\n",
        "chat-conversational-react-description:\n",
        "\tDesigned for conversational agents with memory.\n",
        "\tKeeps track of the conversation history to provide context-aware responses.\n",
        "\tThis is the agent type used in your code.\n",
        "\n",
        "self-ask-with-search:\n",
        "\tDesigned for agents that need to ask clarifying questions before answering.\n",
        "\tOften used with search tools.\n",
        "\n",
        "react-docstore:\n",
        "\tOptimized for retrieving and reasoning over documents in a docstore.\n",
        "\tUseful for document-based question answering.\n",
        "\n",
        "conversational-react-description:\n",
        "\tSimilar to chat-conversational-react-description, but without explicit chat optimizations.\n",
        "\tIncludes memory for context-aware responses.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. LangChain Agent 생성\n",
        "def create_agent(qa_chain):\n",
        "\ttools = [\n",
        "\t\tTool(\n",
        "\t\t\tname=\"Expert PDF File QA\",\n",
        "\t\t\tfunc=qa_chain.run,\n",
        "\t\t\tdescription=\"질문에 대해 PDF 문서에서 답을 찾습니다.\"\n",
        "\t\t),\n",
        "\t\tTavilySearchResults(max_results=5, tavily_api_key=TAVILY_API_KEY)\n",
        "\t]\n",
        "\n",
        "\t'''\n",
        "\tprompt = PromptTemplate(\n",
        "\t\tinput_variables=[\"input\", \"chat_history\"],\n",
        "\t\ttemplate=\"\"\"\n",
        "너는 친절한 코딩 Q&A 봇입니다. 지금까지의 대화는 다음과 같습니다:\n",
        "{chat_history}\n",
        "\n",
        "사용자의 질문:\n",
        "{input}\n",
        "\n",
        "적절한 도구를 사용해서 답하세요.\n",
        "\t\t\"\"\"\n",
        "\t)\n",
        "\t'''\n",
        "\n",
        "\tprompt = PromptTemplate(\n",
        "\t\tinput_variables=[\"input\", \"chat_history\"],\n",
        "\t\ttemplate=\"\"\"너는 친절하고 전문적인 코딩 Q&A 어시스턴트이다.\n",
        "\n",
        "주어진 대화 내용을 참고하여 사용자의 질문에 대해 간결하고 명확한 답변을 작성하라.\n",
        "\n",
        "- 오직 답변 내용만 작성하라.\n",
        "- 서론, 결론, 불필요한 인삿말 없이, 질문에 대한 정확한 설명이나 해결책만 제공하라.\n",
        "\n",
        "지금까지의 대화:\n",
        "{chat_history}\n",
        "\n",
        "사용자의 질문:\n",
        "{input}\n",
        "\n",
        "답변:\n",
        "\"\"\"\n",
        "\t)\n",
        "\n",
        "\n",
        "\tmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\tagent = initialize_agent(\n",
        "\t\ttools=tools,\n",
        "\t\tllm=llm_model,\n",
        "\t\tagent=\"chat-conversational-react-description\",\n",
        "\t\tmemory=memory,\n",
        "\t\tverbose=True,\n",
        "\t\tagent_kwargs={\"prompt\": prompt},\n",
        "\t\thandle_parsing_errors=False # not working. \n",
        "\t)\n",
        "\treturn agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_action_input(text):\n",
        "\t# \"action_input\": \"...\" 패턴을 정규식으로 추출\n",
        "\tpattern = r'\"action_input\"\\s*:\\s*\"([^\"]+)\"'\n",
        "\tmatch = re.search(pattern, text, re.DOTALL)\n",
        "\tif match:\n",
        "\t\treturn match.group(1)  # 캡처한 \"...\" 안의 내용 리턴\n",
        "\treturn None  \n",
        "\n",
        "def clean_action_input_with_llm(text):\n",
        "\tprompt = f\"\"\"\n",
        "다음 텍스트에서 \"action_input\" 값에 해당하는 부분만 정확히 추출해줘.\n",
        "그 외의 모든 내용은 제거해. \n",
        "\n",
        "텍스트:\n",
        "{text}\n",
        "\n",
        "오직 \"action_input\" 안의 내용만 깔끔히 리턴해줘. 추가 설명 없이.\n",
        "\t\"\"\"\n",
        "\n",
        "\tresponse = llm_model.invoke(prompt)\n",
        "\treturn response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 초기화 과정\n",
        "if not os.path.exists(VECTOR_DB_PATH):\n",
        "    os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
        "    docs = load_and_split_pdfs(FILES_DIRECTORY)\n",
        "    save_to_faiss(docs)\n",
        "\n",
        "vectordb = FAISS.load_local(VECTOR_DB_PATH, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), allow_dangerous_deserialization=True)\n",
        "qa_chain = create_retrieval_qa(vectordb)\n",
        "agent = create_agent(qa_chain)\n",
        "\n",
        "def chatbot_interface(user_input, history):\n",
        "    try:\n",
        "        response = agent.run(user_input)\n",
        "        history = history + [(user_input, response)]\n",
        "    except Exception as e:\n",
        "        msg = f\"Error: {str(e)}\"\n",
        "        print(msg)\n",
        "\n",
        "        response = extract_action_input(str(e))\n",
        "        if response == None:\n",
        "            response = clean_action_input_with_llm(str(e))\n",
        "        history = history + [(user_input, response)]\n",
        "\n",
        "    return history, history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"Coding QA Expert Chatbot (PDF + Web Search)\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"질문을 입력하세요...\") # 1) in pdf, what is b2gm? 2) i'm tom, developer. 3) who is tom\n",
        "\n",
        "    clear = gr.Button(\"초기화\")\n",
        "\n",
        "    state = gr.State([])\n",
        "    msg.submit(chatbot_interface, [msg, state], [chatbot, state])\n",
        "    clear.click(lambda: ([], []), None, [chatbot, state])\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_lmm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
