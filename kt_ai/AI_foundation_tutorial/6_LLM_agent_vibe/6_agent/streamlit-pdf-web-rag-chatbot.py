import os, re, glob
import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.tools.tavily_search import TavilySearchResults
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

VECTOR_DB_PATH = './faiss_index'
FILES_DIRECTORY = './files'
CHUNK_SIZE = 2000
CHUNK_OVERLAP = 300

# OpenAI ì„¤ì •
llm_model = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)

def load_and_split_pdfs(files_directory):
    pdf_files = glob.glob(os.path.join(files_directory, '*.pdf'))
    documents = []
    for file in pdf_files:
        loader = PyPDFLoader(file)
        documents.extend(loader.load())
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    split_documents = splitter.split_documents(documents)
    for i, doc in enumerate(split_documents):
        print(f"Document {i}: {doc.page_content[:100]}...")
    return split_documents

def save_to_faiss(documents):
    vectordb = FAISS.from_documents(documents, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))
    vectordb.save_local(VECTOR_DB_PATH)
    print(f"FAISS vector database saved to {VECTOR_DB_PATH}")
    return vectordb

def create_retrieval_qa(vectordb):
    retriever = vectordb.as_retriever(search_type="mmr", search_kwargs={'k': 3, 'lambda_mult': 0.25})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm_model,
        retriever=retriever,
        chain_type="stuff"
    )
    return qa_chain

def create_agent(qa_chain):
    tools = [
        Tool(
            name="Expert PDF File QA",
            func=qa_chain.run,
            description="ì§ˆë¬¸ì— ëŒ€í•´ PDF ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ìŠµë‹ˆë‹¤."
        ),
        TavilySearchResults(max_results=5, tavily_api_key=TAVILY_API_KEY)
    ]

    prompt = PromptTemplate(
        input_variables=["input", "chat_history"],
        template="""ë„ˆëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì½”ë”© Q&A ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤.

ì£¼ì–´ì§„ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ë¼.

- ì˜¤ì§ ë‹µë³€ ë‚´ìš©ë§Œ ì‘ì„±í•˜ë¼.
- ì„œë¡ , ê²°ë¡ , ë¶ˆí•„ìš”í•œ ì¸ì‚¿ë§ ì—†ì´, ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì„¤ëª…ì´ë‚˜ í•´ê²°ì±…ë§Œ ì œê³µí•˜ë¼.

ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:
{chat_history}

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
{input}

ë‹µë³€:
"""
    )

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    agent = initialize_agent(
        tools=tools,
        llm=llm_model,
        agent="chat-conversational-react-description",
        memory=memory,
        verbose=True,
        agent_kwargs={"prompt": prompt},
        handle_parsing_errors=False
    )
    return agent

def extract_action_input(text):
    pattern = r'"action_input"\s*:\s*"([^"]+)"'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1)
    return None

def clean_action_input_with_llm(text):
    prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì§„ì§œ ëŒ€í™” ë‹µë³€ë§Œ ë‚¨ê¸°ê³ , ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì—ëŸ¬ ë©”ì‹œì§€ ë“±ì€ ëª¨ë‘ ì‚­ì œí•´ì¤˜.

í…ìŠ¤íŠ¸:
{text}

ì§€ì‹œì‚¬í•­:
- ëŒ€í™”í˜• ë‹µë³€ ë‚´ìš©ë§Œ ë‚¨ê¸°ê³ , ì—ëŸ¬ ì•ˆë‚´ë¬¸ì€ ì‚­ì œí•´ë¼.
- ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ë‹µë³€ ë¬¸ì¥ë§Œ ë‚¨ê²¨ë¼.
- ì¶”ê°€ ì„¤ëª… ì—†ì´ ë°”ë¡œ ì •ë¦¬ëœ ë‹µë³€ë§Œ ì¶œë ¥í•´ë¼.
- í¬ë§·ì€ ìì—°ìŠ¤ëŸ¬ìš´ ì¼ë°˜ ë¬¸ì¥ìœ¼ë¡œ í•´ë¼.

ë¦¬í„´í•  ê²°ê³¼:
"""
    response = llm_model.invoke(prompt)
    return response

def main():
    st.set_page_config(page_title="QA Expert Chatbot", page_icon="ğŸ¤–", layout="wide")
    st.title("QA Expert Chatbot")

    if 'history' not in st.session_state:
        st.session_state.history = []

    if 'agent' not in st.session_state:
        if not os.path.exists(VECTOR_DB_PATH):
            os.makedirs(VECTOR_DB_PATH, exist_ok=True)
            docs = load_and_split_pdfs(FILES_DIRECTORY)
            save_to_faiss(docs)

        vectordb = FAISS.load_local(VECTOR_DB_PATH, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), allow_dangerous_deserialization=True)
        qa_chain = create_retrieval_qa(vectordb)
        agent = create_agent(qa_chain)
        st.session_state.agent = agent

    # 1. ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for user_msg, bot_msg in st.session_state.history:
        st.chat_message("user").markdown(user_msg)
        st.chat_message("assistant").markdown(bot_msg)

    st.markdown("---")

    # 2. ì…ë ¥ì°½
    user_input = st.text_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="input")

    # 3. ë²„íŠ¼ ì˜ì—­ (2ê°œ ë²„íŠ¼ ë‚˜ë€íˆ)
    col1, col2 = st.columns([3, 1])
    with col1:
        if st.button("ì§ˆë¬¸í•˜ê¸°"):
            if user_input:
                try:
                    response = st.session_state.agent.run(user_input)
                    st.session_state.history.append((user_input, response))
                except Exception as e:
                    msg = f"Error: {str(e)}"
                    print(msg)

                    response = extract_action_input(str(e))
                    if not response:
                        response = clean_action_input_with_llm(str(e))

                    st.session_state.history.append((user_input, response))
                st.rerun()

    with col2:
        if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.history = []
            st.rerun()

if __name__ == "__main__":
    main()
