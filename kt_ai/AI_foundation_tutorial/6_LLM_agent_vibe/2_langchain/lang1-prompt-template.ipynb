{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain openai faiss-cpu tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17912,
     "status": "ok",
     "timestamp": 1745736151796,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "n-zGakAZ-X9G"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "key = userdata.get('openai-api')\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1551,
     "status": "ok",
     "timestamp": 1745736166853,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "T_WVHfbF7Tab",
    "outputId": "eda3ef2b-5b93-4c90-94da-44b2277db5f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: What is the capital of Norway?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "output = prompt_template.format(query=\"What is the capital of Norway?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0GYrNlCz--a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='너는 유능한 어시스트야 너의 이름은 톰 이야', additional_kwargs={}, response_metadata={}), HumanMessage(content='안녕하세요', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요 무엇을 도와드릴까요', additional_kwargs={}, response_metadata={}), HumanMessage(content='너의 이름은 뭐야?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 대화형 프롬프트 템플릿\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 유능한 어시스트야 너의 이름은 {name} 이야\"),\n",
    "        (\"human\", \"안녕하세요\"),\n",
    "        (\"ai\", \"안녕하세요 무엇을 도와드릴까요\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"톰\", user_input=\"너의 이름은 뭐야?\")\n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6oR6/kvTEuBzyEDI1SwbZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
