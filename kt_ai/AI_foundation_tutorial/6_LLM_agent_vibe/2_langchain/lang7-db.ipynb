{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21030,
     "status": "ok",
     "timestamp": 1745736849435,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "wr9ik55TAtDB",
    "outputId": "6ef66e36-debb-4fae-86c9-cd5cb00b868a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (1.25.5)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain) (0.3.49)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_community) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_openai) (1.75.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (2.6.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mac\\.conda\\envs\\venv_lmm\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community langchain_openai pymupdf sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1745736827461,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "5cM8Fd1ZApIC"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "key = userdata.get('openai-api')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# LLM, RAG and Agent Tutorial\\nThis repository contains AI tool, lib installation link for LLM & Agent, focusing on creative LLM coding, modeling, and computing as the viewpoint of media project. \\nPlease download and read the below docuemnt to understand this development environment. \\n- [LLM development environment document(word file)](https://github.com/mac999/LLM-RAG-Agent-Tutorial/blob/main/1-1.prepare/dev-env.docx)\\n\\n## Overview\\n- **Huggingface**: For uisng LLM, Stable Diffusion-based model, You need to sign up Huggingface. In example, [Single Image-to-3D model](https://huggingface.co/spaces/stabilityai/stable-point-aware-3d)\\n- **Ollama**: For using AI tools in interactive art projects. You need to install NVIDIA cuda for run it.\\n\\nThe repository includes examples to experiment with generative media art.</br>\\n- [Gen AI for Media Art](https://github.com/mac999/llm-media-art-demo)\\nIn addition, you can find Text-to-3D model tool the below link. \\n- [Text-to-3D model code](https://github.com/mac999/blender-llm-addin): Using Open-Source Models with Blender for AI-Assisted 3D Modeling: Comparative Study with OpenAI GPT\\n\\n---\\n\\n## Preparation and Installation\\n\\nBefore running the examples, ensure you have Python 3.8 or higher installed. Some tool or library use NVIDIA GPU, so if you want to use it, prepare notebook computer with NVIDIA GPU(recommend 8GB. minimum 4GB)\\nFollow the instructions below to set up your environment:\\n\\n### NVIDIA Drivers (for Ollama. optional)\\nFor GPU-accelerated tasks, you need to install the correct NVIDIA drivers for your GPU.\\n\\n- **Download NVIDIA Drivers**: [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)\\n- **Steps**:\\n  1. Identify your GPU model using the NVIDIA Control Panel or the `nvidia-smi` command in the terminal.\\n  2. Download the latest driver for your GPU model.\\n  3. Install the driver and reboot your system.\\n\\nTo confirm installation:\\n```bash\\nnvidia-smi\\n```\\n\\n### CUDA Toolkit (for NVIDIA. optional)\\nCUDA is required for running GPU-accelerated operations.\\n\\n- **Download CUDA Toolkit**: [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)\\n- During installation:\\n  - Match the CUDA version with your NVIDIA driver and GPU model. Refer to the compatibility chart on the download page.\\n  - Install the CUDA Toolkit with default options.\\n  - Add the CUDA binary paths to your environment variables.\\n\\n### Python\\nEnsure that Python (safe version 3.10 or 3.12) is installed on your system. About macbook, please refer to [how to install python on mac](https://www.youtube.com/watch?v=u4xUUBTER4I).\\n\\n- **Download Python**: [python.org](https://www.python.org/)\\n- During installation:\\n  - Check the box for **\"Add Python to PATH\"**.\\n  - Install Python along with **pip**.\\n\\nTo confirm installation in terminal(DOS command in windows. Shell terminal in linux):\\n```bash\\npython --version\\n```\\n\\n### Anaconda\\nEnsure that Anaconda (version 24.0 or later) is installed on your system.\\n\\n- **Download Anaconda**: [Anaconda](https://docs.anaconda.com/anaconda/install/)\\n\\n### Huggingface, OpenAI (Optinnal) Account \\nMake Accounts for OpenAI, Huggingface\\n- Sign up [Huggingface](https://huggingface.co/) and make [API token](https://huggingface.co/settings/tokens)to develop Open source LLM-base application\\n- Sign up [OpenAI API](https://platform.openai.com/) to develop ChatGPT-based application (*Note: don\\'t check auto-subscription option)\\n\\n### PyTorch library \\n\\n- If AI-related models or tools will be used (such as LLM model fine-tuning with Ollama), install stable [PyTorch](https://pytorch.org/get-started/locally/)(11.8 version) and additional packages:\\n   ```bash\\n   pip install openai\\n   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n   ```\\n\\n### Install Python Packages\\nRun the following command to install the required libraries:\\n```bash\\npip install pandas numpy\\npip install ollama openai transformers huggingface_hub langchain\\n```\\n\\n### Install Ollama \\nFor examples that utilize Ollama, follow the installation instructions from the [Ollama website](https://www.ollama.com/).\\n\\n### Blender (for AI-Assisted Modeling)\\nIf the script or application involves Blender for 3D modeling, ensure Blender is installed.\\n\\n- **Download Blender**: [blender.org](https://www.blender.org/download/)\\n- After installation:\\n  - Enable the **Python Console** within Blender to run scripts directly.\\n  - Ensure Blender uses the same Python environment where required libraries are installed.\\n\\n---\\n\\n### Install sublime and vscode (Optional)\\nInstall [Sublime](https://www.sublimetext.com/) for editing source code</br>\\nInstall [vscode](https://code.visualstudio.com/download) for debuging code. Please refer to how to [install vscode](https://www.youtube.com/watch?v=vesxpfOAOCw).</br>\\n\\n---\\n\\n## **System Environment Checks**\\n\\nAfter completing the installations, verify that the environment is set up correctly:\\n\\n1. **Check Python Version**:\\n   ```bash\\n   python --version\\n   ```\\n\\n2. **Verify NVIDIA Drivers**:\\n   ```bash\\n   nvidia-smi\\n   ```\\n\\n3. **Confirm CUDA Version**:\\n   ```bash\\n   nvcc --version\\n   ```\\n\\n4. **Test Python Libraries**:\\n   Create a test script and import the installed libraries:\\n   ```python\\n   import p5 # only, python 3.10, working\\n   import pandas as pd\\n   import numpy as np\\n   print(\"Libraries are installed successfully!\")\\n   ```\\n\\n## Reference\\n- [NVIDIA cuda programming, open source and AI](https://www.slideshare.net/slideshow/nvidia-cuda-programming-open-source-and-ai/270372806?from_search=6)\\n- [Docker 에서 Ollama, ComfyUI](https://www.youtube.com/watch?v=IxxOMLkcYNY)\\n- [Chat with ChatGPT through Arduino IoT Cloud](https://projecthub.arduino.cc/dbeamonte_arduino/chat-with-chatgpt-through-arduino-iot-cloud-6b4ef0)\\n- [chatGPT-Arduino-library](https://github.com/programming-electronics-academy/chatGPT-Arduino-library/tree/main)\\n- [ChatGPT_Client_For_Arduino](https://github.com/0015/ChatGPT_Client_For_Arduino)\\n- [I tried ChatGPT for Arduino - It’s Surprising](https://blog.wokwi.com/learn-arduino-using-ai-chatgpt/)\\n- [Using ChatGPT to Write Code for Arduino and ESP32](https://dronebotworkshop.com/chatgpt/)\\n\\n## License\\n\\nThis repository is licensed under the MIT License. You are free to use, modify, and distribute the code for personal or commercial projects.\\n\\n## Author\\nlaputa99999@gmail.com\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAC\\AppData\\Local\\Temp\\ipykernel_25880\\3823645941.py:11: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total = 1 in database\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./readme_file.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "texts = [doc.page_content for doc in documents]\n",
    "print(texts)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=key)\n",
    "document_embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "vector_db = FAISS.from_texts(texts, embedding_model)\n",
    "\n",
    "print(f\"total = {len(texts)} in database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAC\\AppData\\Local\\Temp\\ipykernel_25880\\4252827174.py:17: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hello, I am Tom\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to acknowledge the introduction.  \n",
      "Action: Echo  \n",
      "Action Input: Hello, I am Tom  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEchoing: Hello, I am Tom\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have acknowledged Tom's introduction.  \n",
      "Final Answer: Hello, I am Tom\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer: Hello, I am Tom\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, openai_api_key=key)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Echo\",\n",
    "        func=lambda x: f\"Echoing: {x}\",  # 단순한 에코 함수\n",
    "        description=\"Echo the input text\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True  \n",
    ")\n",
    "\n",
    "query = \"Hello, I am Tom\"\n",
    "print(\"Question:\", query)\n",
    "\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "\n",
    "print(\"Answer:\", result['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "# load pdf file and split into chunks\n",
    "loader = PyMuPDFLoader(\"./files/mama-mia.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_contents = [doc.page_content for doc in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# vedtorstore 생성\n",
    "vectorstore = FAISS.from_texts(text_contents, embeddings)\n",
    "\n",
    "# LLM \n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # 토크나이저\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline(\n",
    "        \"text-generation\", # 텍스트 생성\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256, # 성할 텍스트의 최대 토큰 수\n",
    "        do_sample=True, # 확률 기반 샘플링 생성\n",
    "        temperature=0.7,\n",
    "        top_p=0.95, # 확률 분포에서 상위 95%의 누적 확률에 해당하는 토큰만 고려하여 텍스트를 생성\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    ")\n",
    "\n",
    "# template 정의\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"당신은 AI 언어 모델 어시스턴트입니다. 사용자가 제공한 질문에 대해 벡터 데이터베이스에서 관련 문서를 검색할 수 있도록 질문을 3가지 다른 버전으로 생성하는 것이 당신의 임무입니다. 사용자의 질문을 다양한 관점에서 재구성하여 거리 기반 유사도 검색의 한계를 극복할 수 있도록 돕는 것이 목표입니다. 각 버전의 질문은 줄바꿈으로 구분하여 작성하세요. 한국어로 작성하세요. 원본 질문: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# OUtputParser 정의\n",
    "class LineListOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        return text.strip().split(\"\\n\")\n",
    "\n",
    "# LLM chain 생성\n",
    "output_parser = LineListOutputParser()\n",
    "llm_chain = custom_prompt | llm | output_parser\n",
    "\n",
    "# multi-query retriever 생성\n",
    "retriever_from_llm = MultiQueryRetriever(retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\") # 사용자의 질문을 여러 관점에서 재구성하여 다양한 쿼리를 생성\n",
    "retriever_from_llm.verbose = True\n",
    "\n",
    "# 쿼리 실행\n",
    "query = \"mama mia?\"\n",
    "results = retriever_from_llm.get_relevant_documents(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "1\n",
      "Archbishop Rummel Genesian Players Audition Packet \n",
      "Information, Audition Sides, and Music \n",
      " \n",
      "Mamma Mia! \n",
      "Based upon the hit songs of ABBA \n",
      "Music and Lyrics by BENNY ANDERSSON & BJÖRN ULVAEUS \n",
      "And some songs with STIG ANDERSON \n",
      "Book by CATHERINE JOHNSON  \n",
      "Stage & Music Direction by Brandt Blocker – Choreography by Karen Hebert  \n",
      "Performance Dates and Times  \n",
      "April 21, 22*, 23, 28, 29, 30, 2022 \n",
      "*Champagne Performance \n",
      "Rehearsal Schedule  \n",
      "Anyone cast in the show MUST be available March 2 – April 20 (Mondays through Fridays \n",
      "6PM-9PM and Saturdays 12PM-4PM. Exceptions may be made for school related activities.) \n",
      "You must also be available for all performances. \n",
      "Auditions \n",
      "By online submission at https://airtable.com/shrRx2KndqbqPhsuY    (Deadline to submit has \n",
      "been extended to Monday, January 31.) \n",
      "In-person callbacks will be held Saturday, February 19 at 12:00PM. \n",
      "Location for Callbacks \n",
      "Archbishop Rummel Genesian Theatre \n",
      "1901 Severn Avenue, Metairie  \n",
      "Audition Meeting (Optional)\n",
      "\n",
      "문서 2:\n",
      "help you decide on the roles you would like to audition for and craft your audition. Do \n",
      "some research on the musical if you would like. Familiarize yourself with the music \n",
      "(readily available online). There are also many YouTube videos of various productions. \n",
      "• \n",
      "Attend optional Audition ZOOM Meeting Monday, January 17 at 6:30PM to ask any \n",
      "remaining questions. https://us02web.zoom.us/j/89365862604 \n",
      "• \n",
      "Fill out the audition submission and upload your videos. Based upon your submission, \n",
      "you will be notified if you are invited to the callback auditions on February 19. \n",
      "Synopsis of Mamma Mia \n",
      "ABBA's hits tell the hilarious story of a young woman's search for her birth father. This sunny and funny tale \n",
      "unfolds on a Greek island paradise. On the eve of her wedding, a daughter's quest to discover the identity of her \n",
      "father brings three men back to the island they last visited 20 years ago.\n",
      "\n",
      "문서 3:\n",
      "• \n",
      "Dancing/Movement – If you wish, please submit a pre-existing video or record a video of \n",
      "you dancing/moving to music so we can see your ability. Have fun!  \n",
      "• \n",
      "Acting Auditions ‐ Choose any monologue, record it, and upload it on the submission \n",
      "form. You do not have to memorize the scene. Just read it with character from the page. \n",
      "Some existing sides (scenes chosen from the script) from Mamma Mia are copied below \n",
      "that you may use. \n",
      "• Resumes and Headshots will be gratefully accepted but are NOT required.  \n",
      " \n",
      "What To Do Next and Other Information  \n",
      "• \n",
      "Print this PDF File of the Audition Announcement, Audition Sides, Sheet Music, and \n",
      "download rehearsal tracks and backing tracks of your choice. \n",
      "Review the character summaries, play synopsis and other information provided. This will \n",
      "help you decide on the roles you would like to audition for and craft your audition. Do \n",
      "some research on the musical if you would like. Familiarize yourself with the music\n",
      "\n",
      "문서 4:\n",
      "2\n",
      "Audition Process and Instructions  \n",
      "Students are asked to submit a video of a prepared song (pop song, musical theatre song, or \n",
      "simply sing “Happy Birthday” so we can determine your level of vocal comfort), a dance to any \n",
      "music to show us you can move, OR, a brief monologue from a script.  \n",
      "• \n",
      "Singing Audition ‐ If you have a pre-existing piece that you’d like to audition with, please \n",
      "upload your video singing with piano accompaniment or a track. I have uploaded \n",
      "rehearsal tracks with guide vocals, backing tracks for your audition use, and sheet music. \n",
      "If you’d like to sing a song from “Mamma Mia” you can download sheet music and \n",
      "backing tracks here: \n",
      "https://drive.google.com/drive/folders/1RhyQd5EsNpqmr3GuxzFVvif9VH5dTgPU?usp=s\n",
      "haring \n",
      "Yes, you may upload more than one selection, if you wish. \n",
      "• \n",
      "Dancing/Movement – If you wish, please submit a pre-existing video or record a video of \n",
      "you dancing/moving to music so we can see your ability. Have fun!  \n",
      "•\n",
      "\n",
      "문서 5:\n",
      "21\n",
      "PEPPER, TANYA, & EDDIE - SIDE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "for i, doc in enumerate(results[:5]):  # 상위 5개 결과 출력\n",
    "    print(f\"문서 {i+1}:\")\n",
    "    print(doc.page_content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOXKu59KiZGTj+nqOFQJADw",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
